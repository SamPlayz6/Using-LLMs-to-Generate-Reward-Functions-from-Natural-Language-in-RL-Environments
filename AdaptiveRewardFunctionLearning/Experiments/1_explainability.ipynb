{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **This is a notebook to give proof of concept of Claude being able to explain the reward functions generated.**\n",
    "\n",
    "**H0: Claude can explain how its generated reward functions work and also explain the changes it makes to reward functions based of information in its context.**\n",
    "\n",
    "\n",
    "Answer: Yes\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Composite Reward Function with Focused Components:\n",
    "\n",
    "```python\n",
    "def stabilityReward(observation, action):\n",
    "    x, xDot, angle, angleDot = observation\n",
    "    # Focus purely on angle stability\n",
    "    angle_stability = 1.0 - abs(angle) / 0.209  # 0.209 radians is about 12 degrees\n",
    "    angular_velocity_component = -abs(angleDot) / 10.0  # Penalize fast angle changes\n",
    "    return float(angle_stability + angular_velocity_component)\n",
    "```\n",
    "\n",
    "```python\n",
    "def energyEfficiencyReward(observation, action):\n",
    "    x, xDot, angle, angleDot = observation\n",
    "    # Focus purely on minimizing movement and energy use\n",
    "    cart_movement_penalty = -abs(xDot) / 5.0  # Penalize cart velocity\n",
    "    angular_movement_penalty = -abs(angleDot) / 5.0  # Penalize pole angular velocity\n",
    "    return float(1.0 + cart_movement_penalty + angular_movement_penalty)\n",
    "```\n",
    "\n",
    "```python\n",
    "def timeBasedReward(observation, action):\n",
    "    x, xDot, angle, angleDot = observation\n",
    "    # Simple time-based reward that encourages survival\n",
    "    base_reward = 1.0\n",
    "    # Add small penalties for extreme positions/angles to prevent gaming\n",
    "    if abs(angle) > 0.209 or abs(x) > 2.4:  # If about to fail\n",
    "        base_reward = 0.0\n",
    "    return float(base_reward)\n",
    "```\n",
    "\n",
    "--\n",
    "\n",
    "```python\n",
    "def dynamicRewardFunction(observation, action):\n",
    "    x, xDot, angle, angleDot = observation\n",
    "    \n",
    "    # Get individual rewards\n",
    "    stability = stabilityReward(observation, action)\n",
    "    efficiency = energyEfficiencyReward(observation, action)\n",
    "    timeReward = timeBasedReward(observation, action)\n",
    "    \n",
    "    # Combine rewards with equal weights\n",
    "    return (stability + efficiency + timeReward) / 3.0\n",
    "```\n",
    "\n",
    "==================================================\n",
    "\n",
    "Stability Reward Function Explanation:\n",
    "Certainly! This stability reward function is designed to encourage the cart-pole system to maintain a stable, upright position. Let's break down the components and explain how they contribute to the goal of stability:\n",
    "\n",
    "1. Input:\n",
    "   - The function takes two parameters: `observation` and `action`.\n",
    "   - `observation` is unpacked into four variables: `x`, `xDot`, `angle`, and `angleDot`.\n",
    "   - `x`: Position of the cart\n",
    "   - `xDot`: Velocity of the cart\n",
    "   - `angle`: Angle of the pole from vertical\n",
    "   - `angleDot`: Angular velocity of the pole\n",
    "\n",
    "2. Angle Stability Component:\n",
    "   ```python\n",
    "   angle_stability = 1.0 - abs(angle) / 0.209\n",
    "   ```\n",
    "   - This component focuses on keeping the pole as close to vertical as possible.\n",
    "   - `abs(angle)` gives the absolute value of the angle, ensuring symmetrical treatment for both positive and negative angles.\n",
    "   - 0.209 radians is approximately 12 degrees, which is often used as a termination condition in cart-pole problems.\n",
    "   - The subtraction from 1.0 inverts the scale, so smaller angles result in higher rewards.\n",
    "   - This component will be 1.0 when the pole is perfectly vertical (angle = 0) and approach 0 as the angle nears Â±0.209 radians.\n",
    "\n",
    "3. Angular Velocity Component:\n",
    "   ```python\n",
    "   angular_velocity_component = -abs(angleDot) / 10.0\n",
    "   ```\n",
    "   - This component discourages rapid changes in the pole's angle.\n",
    "   - `abs(angleDot)` gives the absolute magnitude of the angular velocity.\n",
    "   - The negative sign ensures that faster angular velocities result in larger penalties (smaller rewards).\n",
    "   - Division by 10.0 scales this component to be in a similar range as the angle stability component.\n",
    "\n",
    "4. Combination:\n",
    "   ```python\n",
    "   return float(angle_stability + angular_velocity_component)\n",
    "   ```\n",
    "   - The function returns the sum of these two components.\n",
    "   - This combination encourages both a vertical pole position and minimal angular velocity.\n",
    "\n",
    "5. Focus on Stability:\n",
    "   - The function ignores the cart's position (`x`) and velocity (`xDot`), focusing solely on the pole's angle and angular velocity.\n",
    "   - This design prioritizes keeping the pole upright over the cart's position on the track.\n",
    "\n",
    "6. Why it works for stability:\n",
    "   - Highest rewards are given when the pole is vertical (angle near 0) and not moving quickly (low angular velocity).\n",
    "   - As the pole deviates from vertical or starts moving faster, the reward decreases.\n",
    "   - This incentivizes actions that keep the pole upright and still, which is the essence of stability in the cart-pole system.\n",
    "\n",
    "In summary, this reward function is tailored specifically for stability by rewarding a vertical pole position and penalizing both angle deviations and rapid angular movements. It disregards the cart's horizontal motion, emphasizing the pole's stability above all else.\n",
    "\n",
    "=================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Composite Reward Function with Focused Components:\n",
      "\n",
      "def stabilityReward(observation, action):\n",
      "    x, xDot, angle, angleDot = observation\n",
      "    # Focus purely on angle stability\n",
      "    angle_stability = 1.0 - abs(angle) / 0.209  # 0.209 radians is about 12 degrees\n",
      "    angular_velocity_component = -abs(angleDot) / 10.0  # Penalize fast angle changes\n",
      "    return float(angle_stability + angular_velocity_component)\n",
      "\n",
      "def energyEfficiencyReward(observation, action):\n",
      "    x, xDot, angle, angleDot = observation\n",
      "    # Focus purely on minimizing movement and energy use\n",
      "    cart_movement_penalty = -abs(xDot) / 5.0  # Penalize cart velocity\n",
      "    angular_movement_penalty = -abs(angleDot) / 5.0  # Penalize pole angular velocity\n",
      "    return float(1.0 + cart_movement_penalty + angular_movement_penalty)\n",
      "\n",
      "def timeBasedReward(observation, action):\n",
      "    x, xDot, angle, angleDot = observation\n",
      "    # Simple time-based reward that encourages survival\n",
      "    base_reward = 1.0\n",
      "    # Add small penalties for extreme positions/angles to prevent gaming\n",
      "    if abs(angle) > 0.209 or abs(x) > 2.4:  # If about to fail\n",
      "        base_reward = 0.0\n",
      "    return float(base_reward)\n",
      "\n",
      "def dynamicRewardFunction(observation, action):\n",
      "    x, xDot, angle, angleDot = observation\n",
      "    \n",
      "    # Get individual rewards\n",
      "    stability = stabilityReward(observation, action)\n",
      "    efficiency = energyEfficiencyReward(observation, action)\n",
      "    timeReward = timeBasedReward(observation, action)\n",
      "    \n",
      "    # Combine rewards with equal weights\n",
      "    return (stability + efficiency + timeReward) / 3.0\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Stability Reward Function Explanation:\n",
      "Certainly! This stability reward function is designed to encourage the cart-pole system to maintain a stable, upright position. Let's break down each component and explain how it contributes to the goal of stability:\n",
      "\n",
      "1. Input:\n",
      "   - The function takes two parameters: `observation` and `action`.\n",
      "   - The `observation` is unpacked into four variables: `x`, `xDot`, `angle`, and `angleDot`.\n",
      "   - `x`: Position of the cart\n",
      "   - `xDot`: Velocity of the cart\n",
      "   - `angle`: Angle of the pole from vertical\n",
      "   - `angleDot`: Angular velocity of the pole\n",
      "\n",
      "2. Angle Stability Component:\n",
      "   ```python\n",
      "   angle_stability = 1.0 - abs(angle) / 0.209\n",
      "   ```\n",
      "   - This component focuses on the angle of the pole.\n",
      "   - The absolute value of the angle is taken to treat deviations in either direction equally.\n",
      "   - The angle is divided by 0.209 (approximately 12 degrees), which is likely the maximum allowed angle before failure.\n",
      "   - Subtracting this value from 1.0 creates a reward that is highest (1.0) when the angle is 0 (perfectly vertical) and decreases as the angle increases.\n",
      "   - This encourages the pole to stay as close to vertical as possible.\n",
      "\n",
      "3. Angular Velocity Component:\n",
      "   ```python\n",
      "   angular_velocity_component = -abs(angleDot) / 10.0\n",
      "   ```\n",
      "   - This component focuses on the angular velocity of the pole.\n",
      "   - The absolute value of `angleDot` is taken to penalize movement in either direction.\n",
      "   - Dividing by 10.0 scales the penalty to be in a similar range as the angle stability component.\n",
      "   - The negative sign makes this a penalty: higher angular velocities result in lower (more negative) rewards.\n",
      "   - This discourages rapid changes in the pole's angle, promoting smoother, more stable behavior.\n",
      "\n",
      "4. Combined Reward:\n",
      "   ```python\n",
      "   return float(angle_stability + angular_velocity_component)\n",
      "   ```\n",
      "   - The final reward is the sum of the two components.\n",
      "   - This combination encourages both a vertical pole position and minimal angular velocity.\n",
      "\n",
      "Why this function focuses on stability:\n",
      "\n",
      "1. It only considers the angle and angular velocity of the pole, ignoring the cart's position and velocity. This means it's solely focused on keeping the pole upright, regardless of where the cart is on the track.\n",
      "\n",
      "2. The angle stability component is maximized when the pole is perfectly vertical, which is the most stable position.\n",
      "\n",
      "3. The angular velocity component penalizes any movement of the pole, encouraging it to remain as still as possible.\n",
      "\n",
      "4. By combining these components, the function rewards a state where the pole is both vertical and not moving, which is the definition of stability for this system.\n",
      "\n",
      "5. The function does not reward or penalize the cart's movement directly, allowing the learning algorithm to discover any cart movements necessary to keep the pole stable.\n",
      "\n",
      "This reward function guides the learning process towards finding a policy that prioritizes keeping the pole balanced and still, which is the primary challenge in the cart-pole problem.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Episode 1/100, Total Reward: 7.091875097536419\n",
      "Episode 2/100, Total Reward: 24.583341855872415\n",
      "Episode 3/100, Total Reward: 11.485423862534892\n",
      "Episode 4/100, Total Reward: 6.172910746889252\n",
      "Episode 5/100, Total Reward: 41.67835552007773\n",
      "Episode 6/100, Total Reward: 14.618665516176565\n",
      "Episode 7/100, Total Reward: 38.48786387884251\n",
      "Episode 8/100, Total Reward: 21.174661927469863\n",
      "Episode 9/100, Total Reward: 54.27209746298917\n",
      "Episode 10/100, Total Reward: 11.490137058741263\n",
      "Episode 11/100, Total Reward: 24.711637898475338\n",
      "Episode 12/100, Total Reward: 11.942126203324806\n",
      "Episode 13/100, Total Reward: 15.884070523329349\n",
      "Episode 14/100, Total Reward: 8.106231988459065\n",
      "Episode 15/100, Total Reward: 15.232348827355835\n",
      "Episode 16/100, Total Reward: 12.282849819824674\n",
      "Episode 17/100, Total Reward: 6.09421718809308\n",
      "Episode 18/100, Total Reward: 36.43052831841856\n",
      "Episode 19/100, Total Reward: 12.610903663358295\n",
      "Episode 20/100, Total Reward: 17.09597330160938\n",
      "Episode 21/100, Total Reward: 29.319146260262794\n",
      "Episode 22/100, Total Reward: 11.437075027120054\n",
      "Episode 23/100, Total Reward: 20.47764559109507\n",
      "Episode 24/100, Total Reward: 21.022141828348165\n",
      "Episode 25/100, Total Reward: 33.07493002124085\n",
      "Episode 26/100, Total Reward: 21.239857999183403\n",
      "Episode 27/100, Total Reward: 25.57542876211802\n",
      "Episode 28/100, Total Reward: 21.081870583687984\n",
      "Episode 29/100, Total Reward: 18.840192082216458\n",
      "Episode 30/100, Total Reward: 11.60082208808761\n",
      "Episode 31/100, Total Reward: 10.373920163652745\n",
      "Episode 32/100, Total Reward: 9.630844357544989\n",
      "Episode 33/100, Total Reward: 9.110697493279142\n",
      "Episode 34/100, Total Reward: 18.85356325360172\n",
      "Episode 35/100, Total Reward: 8.027302744576078\n",
      "Episode 36/100, Total Reward: 10.566633555307343\n",
      "Episode 37/100, Total Reward: 28.807100118107\n",
      "Episode 38/100, Total Reward: 17.86596114539751\n",
      "Episode 39/100, Total Reward: 34.396750378505246\n",
      "Episode 40/100, Total Reward: 7.189494460182445\n",
      "Episode 41/100, Total Reward: 18.33415852748556\n",
      "Episode 42/100, Total Reward: 6.128978110129753\n",
      "Episode 43/100, Total Reward: 29.19328632931146\n",
      "Episode 44/100, Total Reward: 10.126759188238417\n",
      "Episode 45/100, Total Reward: 7.409145278035049\n",
      "Episode 46/100, Total Reward: 13.441625659801128\n",
      "Episode 47/100, Total Reward: 21.0695612860086\n",
      "Episode 48/100, Total Reward: 17.932625645301354\n",
      "Episode 49/100, Total Reward: 11.877341823772404\n",
      "Episode 50/100, Total Reward: 18.378960264230553\n",
      "Episode 51/100, Total Reward: 16.723881801953347\n",
      "Episode 52/100, Total Reward: 12.741602046866168\n",
      "Episode 53/100, Total Reward: 14.242594443409581\n",
      "Episode 54/100, Total Reward: 6.358120575147953\n",
      "Episode 55/100, Total Reward: 26.90538442756749\n",
      "Episode 56/100, Total Reward: 7.327155033633279\n",
      "Episode 57/100, Total Reward: 9.826060796576588\n",
      "Episode 58/100, Total Reward: 8.94177658190324\n",
      "Episode 59/100, Total Reward: 8.779015558245971\n",
      "Episode 60/100, Total Reward: 16.078228856454885\n",
      "Episode 61/100, Total Reward: 9.205344781647787\n",
      "Episode 62/100, Total Reward: 17.6850669914304\n",
      "Episode 63/100, Total Reward: 13.455111546957037\n",
      "Episode 64/100, Total Reward: 16.347372280934046\n",
      "Episode 65/100, Total Reward: 25.70259002298339\n",
      "Episode 66/100, Total Reward: 9.057534380323377\n",
      "Episode 67/100, Total Reward: 12.844559287302426\n",
      "Episode 68/100, Total Reward: 10.647447372161361\n",
      "Episode 69/100, Total Reward: 6.854598570530684\n",
      "Episode 70/100, Total Reward: 38.89852715774039\n",
      "Episode 71/100, Total Reward: 14.595497830581676\n",
      "Episode 72/100, Total Reward: 4.755804208386838\n",
      "Episode 73/100, Total Reward: 10.211836030743452\n",
      "Episode 74/100, Total Reward: 8.270805540945874\n",
      "Episode 75/100, Total Reward: 14.348122891430645\n",
      "Episode 76/100, Total Reward: 7.344365344569682\n",
      "Episode 77/100, Total Reward: 10.80789460555325\n",
      "Episode 78/100, Total Reward: 16.040836763988427\n",
      "Episode 79/100, Total Reward: 9.861937180106072\n",
      "Episode 80/100, Total Reward: 20.61545189634037\n",
      "Episode 81/100, Total Reward: 23.26584941157145\n",
      "Episode 82/100, Total Reward: 11.458061816659496\n",
      "Episode 83/100, Total Reward: 12.721332301692579\n",
      "Episode 84/100, Total Reward: 16.932621264912076\n",
      "Episode 85/100, Total Reward: 11.43266321046722\n",
      "Episode 86/100, Total Reward: 9.203444004722868\n",
      "Episode 87/100, Total Reward: 42.525194557485456\n",
      "Episode 88/100, Total Reward: 82.35388157807323\n",
      "Episode 89/100, Total Reward: 41.679019820902084\n",
      "Episode 90/100, Total Reward: 26.350476425052495\n",
      "Episode 91/100, Total Reward: 84.28814899817013\n",
      "Episode 92/100, Total Reward: 24.84474724395798\n",
      "Episode 93/100, Total Reward: 15.213239642550636\n",
      "Episode 94/100, Total Reward: 21.447178074365954\n",
      "Episode 95/100, Total Reward: 15.562683428475777\n",
      "Episode 96/100, Total Reward: 13.717315322039514\n",
      "Episode 97/100, Total Reward: 40.96014285104852\n",
      "Episode 98/100, Total Reward: 34.1714596629982\n",
      "Episode 99/100, Total Reward: 16.120974437486936\n",
      "Episode 100/100, Total Reward: 28.27654581311755\n"
     ]
    }
   ],
   "source": [
    "# Generating a Dynmaically Updating Reward Function\n",
    "\n",
    "import anthropic\n",
    "import gymnasium as gym\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "current_dir = os.getcwd()  \n",
    "project_root = str(Path(current_dir).parent.parent)\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# from ExtraNotebooksCodeExamples.cartPoleShared import *\n",
    "\n",
    "# anthropicAPI\n",
    "from AdaptiveRewardFunctionLearning.Prompts.APIQuery import queryAnthropicApi\n",
    "\n",
    "\n",
    "# API configuration\n",
    "from AdaptiveRewardFunctionLearning.Prompts.prompts import apiKey, modelName, device\n",
    "\n",
    "# rewardCompositeGenerator\n",
    "from AdaptiveRewardFunctionLearning.RewardGeneration.rewardCodeGeneration import createCompositeCode\n",
    "\n",
    "def main():\n",
    "    # Get the composite code\n",
    "    compositeCode = createCompositeCode()\n",
    "    \n",
    "    print(\"Composite Reward Function with Focused Components:\")\n",
    "    print(compositeCode)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    \n",
    "\n",
    "    # criticPrompts\n",
    "    # Get explanation for stability reward\n",
    "    from AdaptiveRewardFunctionLearning.Prompts.criticPrompts import  stabilityExplanationMessage\n",
    "\n",
    "    \n",
    "    stabilityExplanation = queryAnthropicApi(apiKey, modelName, stabilityExplanationMessage)\n",
    "    print(\"Stability Reward Function Explanation:\")\n",
    "    print(stabilityExplanation)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "    # logClaudeCall\n",
    "    from AdaptiveRewardFunctionLearning.Prompts.APIQuery import logClaudeCall\n",
    "    logClaudeCall(\n",
    "        rewardPrompt=\"Composite Reward Function with Focused Components\",\n",
    "        rewardResponse=compositeCode,\n",
    "        explanationPrompt=\"Stability Reward Function Explanation\",\n",
    "        explanationResponse=stabilityExplanation,\n",
    "        logFile=\"../../logs/LLMExplainabilityPOC.jsonl\"\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Create namespace and execute all functions\n",
    "    namespace = {}\n",
    "    exec(compositeCode, globals(), namespace)\n",
    "    \n",
    "\n",
    "    # env\n",
    "    from RLEnvironment.env import CustomCartPoleEnv\n",
    "    # Set up environment\n",
    "    env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "    env = CustomCartPoleEnv(env)\n",
    "    \n",
    "    # Update reward function with the namespace\n",
    "    for name, func in namespace.items():\n",
    "        globals()[name] = func\n",
    "    \n",
    "    env.setRewardFunction(namespace['dynamicRewardFunction'])\n",
    "    \n",
    "    # Initialize and train agent\n",
    "    stateSize = env.observation_space.shape[0]\n",
    "    actionSize = env.action_space.n\n",
    "    \n",
    "    # env\n",
    "    from RLEnvironment.training import DQLearningAgent\n",
    "    agent = DQLearningAgent(env, stateSize, actionSize, device)\n",
    "    \n",
    "\n",
    "    # training\n",
    "    from RLEnvironment.training import trainDQLearning\n",
    "\n",
    "    # Training loop\n",
    "    numEpisodes = 100\n",
    "    agent, env, rewards = trainDQLearning(agent, env, numEpisodes=numEpisodes)\n",
    "\n",
    "    for episode in range(numEpisodes):\n",
    "        print(f\"Episode {episode + 1}/{numEpisodes}, Total Reward: {rewards[episode]}\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Is reward a good proxy for effectivness of the reward function?*\n",
    "\n",
    "### **Dynamically Updating Reward function**\n",
    "\n",
    "\n",
    "Attempting reward function update at episode 100\n",
    "\n",
    "Generating new reward function for component 1...\n",
    "\n",
    "Proposed Function:\n",
    "Here's a modified reward function with detailed inline comments explaining the changes:\n",
    "\n",
    "```python\n",
    "def reward_function_1(state):\n",
    "    \"\"\"\n",
    "    Custom reward function focusing on stability (pole angle and angular velocity).\n",
    "    \n",
    "    Args:\n",
    "    state (list): The current state [x, x_dot, theta, theta_dot]\n",
    "    \n",
    "    Returns:\n",
    "    float: The calculated reward\n",
    "    \"\"\"\n",
    "    # Unpack state variables\n",
    "    x, x_dot, theta, theta_dot = state\n",
    "    \n",
    "    # Define target values (center position and upright)\n",
    "    x_target, theta_target = 0.0, 0.0\n",
    "    \n",
    "    # Calculate errors\n",
    "    x_error = abs(x - x_target)\n",
    "    theta_error = abs(theta - theta_target)\n",
    "    \n",
    "    # Define weight factors (increased emphasis on angle and angular velocity)\n",
    "    w_x = 0.2  # Reduced from 1.0 to lower importance of cart position\n",
    "    w_x_dot = 0.1  # Reduced from 0.5 to lower importance of cart velocity\n",
    "    w_theta = 2.0  # Increased from 1.0 to emphasize pole angle\n",
    "    w_theta_dot = 1.5  # Increased from 0.5 to emphasize angular velocity\n",
    "\n",
    "    # Calculate component rewards (using gaussian-like functions for smoother transitions)\n",
    "    r_x = np.exp(-0.5 * (x_error / 0.5)**2)  # Widened gaussian (0.5 instead of 0.1) for more lenient position reward\n",
    "    r_x_dot = np.exp(-0.5 * (x_dot / 2.0)**2)  # Slightly widened gaussian for velocity\n",
    "    r_theta = np.exp(-0.5 * (theta_error / 0.2)**2)  # Narrowed gaussian (0.2 instead of 0.5) for stricter angle reward\n",
    "    r_theta_dot = np.exp(-0.5 * (theta_dot / 1.0)**2)  # Narrowed gaussian for stricter angular velocity reward\n",
    "\n",
    "    # Combine component rewards with weights\n",
    "    reward = w_x * r_x + w_x_dot * r_x_dot + w_theta * r_theta + w_theta_dot * r_theta_dot\n",
    "    \n",
    "    # Normalize reward to [0, 1] range\n",
    "    reward = reward / (w_x + w_x_dot + w_theta + w_theta_dot)\n",
    "    \n",
    "    return reward\n",
    "```\n",
    "\n",
    "Key changes and explanations:\n",
    "\n",
    "1. Increased emphasis on stability:\n",
    "   - Increased weight for theta (w_theta) from 1.0 to 2.0\n",
    "   - Increased weight for theta_dot (w_theta_dot) from 0.5 to 1.5\n",
    "   - Decreased weights for x and x_dot to shift focus to pole stability\n",
    "\n",
    "2. Modified gaussian functions for smoother rewards:\n",
    "   - Widened the gaussian for x (0.5 instead of 0.1) to be more lenient on cart position\n",
    "   - Narrowed the gaussian for theta (0.2 instead of 0.5) to be stricter on pole angle\n",
    "   - Adjusted gaussians for x_dot and theta_dot for better balance\n",
    "\n",
    "3. Normalized the final reward to ensure it stays in the [0, 1] range, making it easier to interpret and use in learning algorithms.\n",
    "\n",
    "These changes should result in a reward function that places higher importance on keeping the pole upright and stable, while being more forgiving of the cart's exact position. This should lead to improved stability in the pole balancing task.\n",
    "\n",
    "Waiting 10 seconds before critic evaluation...\n",
    "\n",
    "Getting critic's evaluation...\n",
    "\n",
    "Critic Response:\n",
    "1. The function is properly named 'reward_function_1', which is clear and distinct.\n",
    "\n",
    "2. The reward calculations appear mathematically sound. The use of Gaussian-like functions for smoother transitions is a good approach, and the normalization of the final reward to the [0, 1] range is appropriate.\n",
    "\n",
    "3. The function focuses appropriately on stability, particularly emphasizing the pole angle and angular velocity. This is achieved through increased weights for theta and theta_dot, and decreased weights for cart position and velocity.\n",
    "\n",
    "4. Potential issues:\n",
    "   - The function might be overly lenient on cart position, which could lead to the cart drifting too far from the center.\n",
    "   - The narrowed Gaussian for theta might make the task too difficult initially, potentially slowing down learning.\n",
    "   - The weights and Gaussian parameters are somewhat arbitrary and might need fine-tuning based on empirical results.\n",
    "   - There's no explicit penalty for failure states (e.g., pole falling past a certain angle), which might be beneficial.\n",
    "\n",
    "Overall, the function seems well-designed for its purpose, but may require some empirical testing and potential adjustments.\n",
    "\n",
    "Decision: Yes\n",
    "\n",
    "Critic Decision: Approved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempting reward function update at episode 100\n",
      "\n",
      "Generating new reward function for component 1...\n",
      "\n",
      "Proposed Function:\n",
      "Here is a modified version of the reward function with detailed inline comments:\n",
      "\n",
      "```python\n",
      "def rewardFunction1(observation, action):\n",
      "    _, _, angle, angleDot = observation\n",
      "    \n",
      "    # Calculate base reward based on angle (same as before)\n",
      "    angle_reward = np.cos(angle)\n",
      "    \n",
      "    # Penalize high angular velocity more heavily (increased from 0.1 to 0.2)\n",
      "    # This encourages more stable balancing\n",
      "    velocity_penalty = 0.2 * abs(angleDot)\n",
      "    \n",
      "    # Add small positive reward for taking action (new)\n",
      "    # This encourages the agent to make small adjustments rather than doing nothing\n",
      "    action_reward = 0.05 * abs(action)\n",
      "    \n",
      "    # Add time-based reward to encourage longer episodes (new)\n",
      "    # This starts small and grows over time to incentivize survival\n",
      "    time_reward = 0.001 * observation[0]  # observation[0] is time step\n",
      "    \n",
      "    # Combine all reward components\n",
      "    total_reward = angle_reward - velocity_penalty + action_reward + time_reward\n",
      "    \n",
      "    # Clip reward to reasonable range (new)\n",
      "    # This prevents extreme reward values that could destabilize learning\n",
      "    total_reward = np.clip(total_reward, -1.0, 1.0)\n",
      "    \n",
      "    return total_reward\n",
      "```\n",
      "\n",
      "Key changes and explanations:\n",
      "\n",
      "1. Increased velocity penalty from 0.1 to 0.2 to encourage more stable balancing.\n",
      "2. Added small positive reward for taking actions to encourage active control.\n",
      "3. Introduced time-based reward component to incentivize longer episodes.\n",
      "4. Combined all reward components into a total reward.\n",
      "5. Clipped final reward to prevent extreme values.\n",
      "\n",
      "These changes aim to optimize both the reward and episode duration by encouraging stable balancing, active control, and longer survival times. The function still focuses on the core objective of keeping the pole upright while adding nuanced incentives for improved performance.\n",
      "\n",
      "Waiting 10 seconds before critic evaluation...\n",
      "\n",
      "Getting critic's evaluation...\n",
      "\n",
      "Critic Response:\n",
      "1. The function is properly named 'rewardFunction1', which is clear and follows a common naming convention.\n",
      "\n",
      "2. The reward calculations appear mathematically sound. The use of cosine for the angle reward, absolute values for penalties, and linear scaling for time and action rewards are all reasonable approaches.\n",
      "\n",
      "3. The function does focus on both reward quality and episode duration. The angle reward and velocity penalty address the quality of balancing, while the time-based reward explicitly encourages longer episodes. The action reward also indirectly contributes to episode duration by encouraging active control.\n",
      "\n",
      "4. Potential issues to consider:\n",
      "   - The balance between different reward components might need tuning. For example, the action reward might encourage unnecessary movements if set too high.\n",
      "   - The time reward grows linearly and unbounded, which could potentially overshadow other components in very long episodes.\n",
      "   - The clipping of the total reward to [-1, 1] might limit the differentiation between good and excellent performances in some cases.\n",
      "   - The function assumes that observation[0] represents the time step, which may not be true for all environments and should be verified.\n",
      "\n",
      "Overall, this reward function appears well-designed and addresses multiple aspects of the pole balancing task. It incorporates improvements over a simpler version and provides clear explanations for each component. While there are some potential issues to be aware of, they are not severe and can be addressed through careful tuning and testing.\n",
      "\n",
      "Decision: Yes\n",
      "\n",
      "Critic Decision: Approved\n",
      "\n",
      "Updating reward component 1...\n",
      "\n",
      "Attempting reward function update at episode 200\n",
      "\n",
      "Generating new reward function for component 1...\n",
      "\n",
      "Proposed Function:\n",
      "Here's the modified reward function with detailed inline comments:\n",
      "\n",
      "```python\n",
      "def rewardFunction1(observation, action):\n",
      "    _, _, angle, angleDot = observation\n",
      "    \n",
      "    # Calculate base reward based on angle (same as before)\n",
      "    angle_reward = np.cos(angle)\n",
      "    \n",
      "    # Penalize high angular velocity more heavily (increased from 0.1 to 0.2)\n",
      "    # This encourages more stable balancing\n",
      "    velocity_penalty = 0.2 * abs(angleDot)\n",
      "    \n",
      "    # Add small positive reward for taking action (new)\n",
      "    # This encourages the agent to make small adjustments rather than doing nothing\n",
      "    action_reward = 0.05 * abs(action)\n",
      "    \n",
      "    # Add time-based reward to encourage longer episodes (new)\n",
      "    # This starts small and grows over time to incentivize survival\n",
      "    time_reward = 0.001 * observation[0]  # observation[0] is time step\n",
      "    \n",
      "    # Combine all reward components\n",
      "    total_reward = angle_reward - velocity_penalty + action_reward + time_reward\n",
      "    \n",
      "    # Clip reward to reasonable range (new)\n",
      "    # This prevents extreme reward values that could destabilize learning\n",
      "    total_reward = np.clip(total_reward, -1.0, 1.0)\n",
      "    \n",
      "    return total_reward\n",
      "```\n",
      "\n",
      "Key changes and explanations:\n",
      "\n",
      "1. Increased velocity penalty from 0.1 to 0.2 to encourage more stable balancing.\n",
      "2. Added small positive reward for taking actions to encourage active control.\n",
      "3. Introduced time-based reward component to incentivize longer episodes.\n",
      "4. Combined all reward components into a total reward.\n",
      "5. Clipped final reward to prevent extreme values.\n",
      "\n",
      "These changes aim to optimize both the reward and episode duration by encouraging stable balancing, active control, and longer survival times. The function still focuses on the core objective of keeping the pole upright while adding nuanced incentives for improved performance.\n",
      "\n",
      "Waiting 10 seconds before critic evaluation...\n",
      "\n",
      "Getting critic's evaluation...\n",
      "\n",
      "Critic Response:\n",
      "I'll analyze the proposed reward function based on the criteria you've outlined:\n",
      "\n",
      "1. Is the function properly named 'rewardFunction1'?\n",
      "Yes, the function is correctly named 'rewardFunction1'.\n",
      "\n",
      "2. Are reward calculations mathematically sound?\n",
      "The calculations appear to be mathematically sound. The function uses basic arithmetic operations and the numpy cosine function, which are appropriate for reward calculations.\n",
      "\n",
      "3. Does it focus on both reward quality and episode duration?\n",
      "Yes, the function addresses both reward quality and episode duration. The angle_reward and velocity_penalty components focus on the quality of balancing, while the time_reward component explicitly encourages longer episodes.\n",
      "\n",
      "4. Are there any potential issues?\n",
      "There are a few potential issues to consider:\n",
      "a) The action_reward might encourage unnecessary movements, potentially destabilizing the pole.\n",
      "b) The time_reward grows linearly with time, which could eventually outweigh other components for very long episodes.\n",
      "c) The clipping of the total reward to [-1, 1] might limit the agent's ability to distinguish between very good and very bad states.\n",
      "\n",
      "Overall, the function seems well-designed and addresses the key aspects of the task. The potential issues are minor and could be adjusted through experimentation.\n",
      "\n",
      "Decision: Yes\n",
      "\n",
      "Critic Decision: Approved\n",
      "\n",
      "Updating reward component 1...\n",
      "\n",
      "Attempting reward function update at episode 300\n",
      "\n",
      "Generating new reward function for component 1...\n",
      "\n",
      "Proposed Function:\n",
      "Here's the modified rewardFunction1 with detailed inline comments addressing your requirements:\n",
      "\n",
      "```python\n",
      "def rewardFunction1(observation, action):\n",
      "    _, _, angle, angleDot = observation\n",
      "    \n",
      "    # Base reward for pole angle - unchanged\n",
      "    angle_reward = np.cos(angle)\n",
      "    \n",
      "    # Increased velocity penalty from 0.1 to 0.3\n",
      "    # This more strongly discourages rapid movements, promoting stability\n",
      "    velocity_penalty = 0.3 * abs(angleDot)  # Old value: 0.2\n",
      "    \n",
      "    # Increased action reward from 0.05 to 0.1\n",
      "    # This further encourages the agent to take actions, avoiding inactivity\n",
      "    action_reward = 0.1 * abs(action)  # Old value: 0.05\n",
      "    \n",
      "    # Enhanced time-based reward to more strongly incentivize longer episodes\n",
      "    # Increased from 0.001 to 0.002 per time step\n",
      "    time_reward = 0.002 * observation[0]  # Old value: 0.001\n",
      "    \n",
      "    # New component: Penalty for being far from center\n",
      "    # This encourages the agent to keep the pole centered\n",
      "    position_penalty = 0.1 * abs(observation[0])  # New addition\n",
      "    \n",
      "    # Combine all reward components\n",
      "    total_reward = angle_reward - velocity_penalty + action_reward + time_reward - position_penalty\n",
      "    \n",
      "    # Adjusted reward clipping range\n",
      "    # Allows for slightly higher positive rewards to encourage better performance\n",
      "    total_reward = np.clip(total_reward, -1.0, 1.5)  # Old range: [-1.0, 1.0]\n",
      "    \n",
      "    return total_reward\n",
      "```\n",
      "\n",
      "This modified version aims to better balance reward optimization and episode duration:\n",
      "\n",
      "1. The velocity penalty is increased to more strongly promote stability.\n",
      "2. The action reward is increased to further encourage active control.\n",
      "3. The time-based reward is enhanced to put more emphasis on longer episodes.\n",
      "4. A new position penalty is added to keep the pole centered, adding complexity to the task.\n",
      "5. The reward clipping range is adjusted to allow for slightly higher positive rewards, potentially speeding up learning for good behaviors.\n",
      "\n",
      "These changes should result in an agent that balances the pole more steadily, keeps it more centered, and maintains balance for longer periods, thus optimizing both the reward and episode duration.\n",
      "\n",
      "Waiting 10 seconds before critic evaluation...\n",
      "\n",
      "Getting critic's evaluation...\n",
      "\n",
      "Critic Response:\n",
      "1. Yes, the function is properly named 'rewardFunction1'.\n",
      "\n",
      "2. The reward calculations appear mathematically sound. The function uses basic arithmetic operations and trigonometry (cosine) which are appropriate for this type of reward function.\n",
      "\n",
      "3. The function does focus on both reward quality and episode duration:\n",
      "   - Reward quality: It considers angle, velocity, action magnitude, and position.\n",
      "   - Episode duration: It includes a time-based reward component that increases with each time step.\n",
      "\n",
      "4. Potential issues:\n",
      "   - The balance between different components might need fine-tuning through experimentation.\n",
      "   - The position penalty might conflict with the goal of keeping the pole upright in some situations.\n",
      "   - The increased action reward might encourage unnecessary actions.\n",
      "   - The wider reward clipping range might lead to more volatile learning.\n",
      "\n",
      "Overall, the function seems well-designed and addresses the stated goals, but as with any reward function, it would need to be tested and potentially adjusted based on actual performance.\n",
      "\n",
      "Decision: Yes\n",
      "\n",
      "Critic Decision: Approved\n",
      "\n",
      "Updating reward component 1...\n",
      "\n",
      "Attempting reward function update at episode 400\n",
      "\n",
      "Generating new reward function for component 1...\n",
      "\n",
      "Proposed Function:\n",
      "Here's the modified rewardFunction1 with detailed inline comments addressing your requirements:\n",
      "\n",
      "```python\n",
      "def rewardFunction1(observation, action):\n",
      "    _, _, angle, angleDot = observation\n",
      "    \n",
      "    # Base reward for pole angle - unchanged\n",
      "    angle_reward = np.cos(angle)\n",
      "    \n",
      "    # Increased velocity penalty from 0.1 to 0.3\n",
      "    # This more strongly discourages rapid movements, promoting stability\n",
      "    velocity_penalty = 0.3 * abs(angleDot)  # Old value: 0.2\n",
      "    \n",
      "    # Increased action reward from 0.05 to 0.1\n",
      "    # This further encourages the agent to take actions, avoiding inactivity\n",
      "    action_reward = 0.1 * abs(action)  # Old value: 0.05\n",
      "    \n",
      "    # Enhanced time-based reward to more strongly incentivize longer episodes\n",
      "    # Increased from 0.001 to 0.002 per time step\n",
      "    time_reward = 0.002 * observation[0]  # Old value: 0.001\n",
      "    \n",
      "    # New component: Penalty for being far from center\n",
      "    # This encourages the agent to keep the pole centered\n",
      "    position_penalty = 0.1 * abs(observation[0])  # New addition\n",
      "    \n",
      "    # Combine all reward components\n",
      "    total_reward = angle_reward - velocity_penalty + action_reward + time_reward - position_penalty\n",
      "    \n",
      "    # Adjusted reward clipping range\n",
      "    # Allows for slightly higher positive rewards to encourage better performance\n",
      "    total_reward = np.clip(total_reward, -1.0, 1.5)  # Old range: [-1.0, 1.0]\n",
      "    \n",
      "    return total_reward\n",
      "```\n",
      "\n",
      "This modified version aims to better balance reward optimization and episode duration:\n",
      "\n",
      "1. The velocity penalty is increased to more strongly promote stability.\n",
      "2. The action reward is increased to further encourage active control.\n",
      "3. The time-based reward is enhanced to put more emphasis on longer episodes.\n",
      "4. A new position penalty is added to keep the pole centered, adding complexity to the task.\n",
      "5. The reward clipping range is adjusted to allow for slightly higher positive rewards, potentially speeding up learning for good behaviors.\n",
      "\n",
      "These changes should result in an agent that balances the pole more steadily, keeps it more centered, and maintains balance for longer periods, thus optimizing both the reward and episode duration.\n",
      "\n",
      "Waiting 10 seconds before critic evaluation...\n",
      "\n",
      "Getting critic's evaluation...\n",
      "\n",
      "Critic Response:\n",
      "Let's evaluate this reward function based on your criteria:\n",
      "\n",
      "1. Is the function properly named 'rewardFunction1'?\n",
      "Yes, the function is correctly named 'rewardFunction1'.\n",
      "\n",
      "2. Are reward calculations mathematically sound?\n",
      "The calculations appear to be mathematically sound. All operations are well-defined and use appropriate mathematical functions.\n",
      "\n",
      "3. Does it focus on both reward quality and episode duration?\n",
      "Yes, the function addresses both reward quality and episode duration:\n",
      "- Reward quality: It includes components for angle, velocity, action, and position.\n",
      "- Episode duration: The time_reward component (0.002 * observation[0]) directly incentivizes longer episodes.\n",
      "\n",
      "4. Are there any potential issues?\n",
      "While the function seems well-designed, there are a few potential considerations:\n",
      "- The balance between components might need fine-tuning through experimentation.\n",
      "- The position penalty might make the task too difficult if set too high.\n",
      "- The expanded reward range (now up to 1.5) might lead to more volatile learning, though it could also speed up learning for good behaviors.\n",
      "\n",
      "Overall, this reward function appears to be a thoughtful improvement over a previous version, addressing both reward optimization and episode duration while adding complexity to encourage more centered and stable pole-balancing behavior.\n",
      "\n",
      "Decision: Yes\n",
      "\n",
      "Critic Decision: Approved\n",
      "\n",
      "Updating reward component 1...\n"
     ]
    }
   ],
   "source": [
    "# Actively Changing the Composite Reward Function in the agent\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "def onEpisodeEnd(env, updateSystem, episode):\n",
    "    if updateSystem.waitingTime(episode):\n",
    "        print(f\"\\nAttempting reward function update at episode {episode}\")\n",
    "        funcName = f'rewardFunction{updateSystem.targetComponent}'\n",
    "        currentFunc = env.rewardComponents[funcName]\n",
    "        newFunction, updated = updateSystem.validateAndUpdate(currentFunc)\n",
    "        \n",
    "        if updated:\n",
    "            print(f\"\\nUpdating reward component {updateSystem.targetComponent}...\")\n",
    "            env.setComponentReward(updateSystem.targetComponent, newFunction)\n",
    "            updateSystem.lastUpdateEpisode = episode\n",
    "        else:\n",
    "            print(\"\\nKeeping current reward function.\")\n",
    "\n",
    "def main():\n",
    "    # Initialize with 3 reward components\n",
    "    env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "    # CustomCartPoleEnv\n",
    "    from RLEnvironment.env import CustomCartPoleEnv\n",
    "    env = CustomCartPoleEnv(env, numComponents=3)\n",
    "    \n",
    "    initialRewards = [\n",
    "        # Stability reward\n",
    "        \"\"\"def rewardFunction1(observation, action):\n",
    "            _, _, angle, angleDot = observation\n",
    "            return np.cos(angle) - 0.1 * abs(angleDot)\"\"\",\n",
    "        \n",
    "        # Position reward\n",
    "        \"\"\"def rewardFunction2(observation, action):\n",
    "            x, xDot, _, _ = observation\n",
    "            return -abs(x) - 0.1 * abs(xDot)\"\"\",\n",
    "        \n",
    "        # Energy efficiency reward\n",
    "        \"\"\"def rewardFunction3(observation, action):\n",
    "            _, xDot, _, angleDot = observation\n",
    "            return -0.1 * (abs(xDot) + abs(angleDot))\"\"\"\n",
    "    ]\n",
    "    \n",
    "    # Set initial reward functions\n",
    "    for i, rewardFunc in enumerate(initialRewards, 1):\n",
    "        env.setComponentReward(i, rewardFunc)\n",
    "    \n",
    "    # RewardUpdateSystem\n",
    "    from AdaptiveRewardFunctionLearning.RewardGeneration.rewardCritic import RewardUpdateSystem\n",
    "\n",
    "    updateSystem = RewardUpdateSystem(\n",
    "        apiKey=apiKey, \n",
    "        modelName=modelName, \n",
    "        targetComponent=1 \n",
    "    )\n",
    "    \n",
    "    # Initialize agent\n",
    "    stateSize = env.observation_space.shape[0]\n",
    "    actionSize = env.action_space.n\n",
    "\n",
    "    #DQLearningAgent\n",
    "    from RLEnvironment.training import DQLearningAgent\n",
    "\n",
    "    agent = DQLearningAgent(env, stateSize, actionSize, device)\n",
    "    \n",
    "    # Training loop with reward updates\n",
    "    numEpisodes = 500\n",
    "    \n",
    "    #trainDQLearning\n",
    "    from RLEnvironment.training import trainDQLearning\n",
    "    agent, env, rewards = trainDQLearning(\n",
    "        agent=agent,\n",
    "        env=env,\n",
    "        numEpisodes=numEpisodes,\n",
    "        updateSystem=updateSystem,\n",
    "        onEpisodeEnd=onEpisodeEnd\n",
    "    )\n",
    "\n",
    "    env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gymenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
