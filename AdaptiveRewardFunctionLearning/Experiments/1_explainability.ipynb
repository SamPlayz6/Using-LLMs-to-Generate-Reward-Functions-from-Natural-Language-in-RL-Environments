{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **This is a notebook to give proof of concept of Claude being able to explain the reward functions generated.**\n",
    "\n",
    "**H0: Claude can explain how its generated reward functions work and also explain the changes it makes to reward functions based of information in its context.**\n",
    "\n",
    "\n",
    "Answer: Yes\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Composite Reward Function with Focused Components:\n",
    "\n",
    "```python\n",
    "def stabilityReward(observation, action):\n",
    "    x, xDot, angle, angleDot = observation\n",
    "    # Focus purely on angle stability\n",
    "    angle_stability = 1.0 - abs(angle) / 0.209  # 0.209 radians is about 12 degrees\n",
    "    angular_velocity_component = -abs(angleDot) / 10.0  # Penalize fast angle changes\n",
    "    return float(angle_stability + angular_velocity_component)\n",
    "```\n",
    "\n",
    "```python\n",
    "def energyEfficiencyReward(observation, action):\n",
    "    x, xDot, angle, angleDot = observation\n",
    "    # Focus purely on minimizing movement and energy use\n",
    "    cart_movement_penalty = -abs(xDot) / 5.0  # Penalize cart velocity\n",
    "    angular_movement_penalty = -abs(angleDot) / 5.0  # Penalize pole angular velocity\n",
    "    return float(1.0 + cart_movement_penalty + angular_movement_penalty)\n",
    "```\n",
    "\n",
    "```python\n",
    "def timeBasedReward(observation, action):\n",
    "    x, xDot, angle, angleDot = observation\n",
    "    # Simple time-based reward that encourages survival\n",
    "    base_reward = 1.0\n",
    "    # Add small penalties for extreme positions/angles to prevent gaming\n",
    "    if abs(angle) > 0.209 or abs(x) > 2.4:  # If about to fail\n",
    "        base_reward = 0.0\n",
    "    return float(base_reward)\n",
    "```\n",
    "\n",
    "--\n",
    "\n",
    "```python\n",
    "def dynamicRewardFunction(observation, action):\n",
    "    x, xDot, angle, angleDot = observation\n",
    "    \n",
    "    # Get individual rewards\n",
    "    stability = stabilityReward(observation, action)\n",
    "    efficiency = energyEfficiencyReward(observation, action)\n",
    "    timeReward = timeBasedReward(observation, action)\n",
    "    \n",
    "    # Combine rewards with equal weights\n",
    "    return (stability + efficiency + timeReward) / 3.0\n",
    "```\n",
    "\n",
    "==================================================\n",
    "\n",
    "Stability Reward Function Explanation:\n",
    "Certainly! This stability reward function is designed to encourage the cart-pole system to maintain a stable, upright position. Let's break down the components and explain how they contribute to the goal of stability:\n",
    "\n",
    "1. Input:\n",
    "   - The function takes two parameters: `observation` and `action`.\n",
    "   - `observation` is unpacked into four variables: `x`, `xDot`, `angle`, and `angleDot`.\n",
    "   - `x`: Position of the cart\n",
    "   - `xDot`: Velocity of the cart\n",
    "   - `angle`: Angle of the pole from vertical\n",
    "   - `angleDot`: Angular velocity of the pole\n",
    "\n",
    "2. Angle Stability Component:\n",
    "   ```python\n",
    "   angle_stability = 1.0 - abs(angle) / 0.209\n",
    "   ```\n",
    "   - This component focuses on keeping the pole as close to vertical as possible.\n",
    "   - `abs(angle)` gives the absolute value of the angle, ensuring symmetrical treatment for both positive and negative angles.\n",
    "   - 0.209 radians is approximately 12 degrees, which is often used as a termination condition in cart-pole problems.\n",
    "   - The subtraction from 1.0 inverts the scale, so smaller angles result in higher rewards.\n",
    "   - This component will be 1.0 when the pole is perfectly vertical (angle = 0) and approach 0 as the angle nears ±0.209 radians.\n",
    "\n",
    "3. Angular Velocity Component:\n",
    "   ```python\n",
    "   angular_velocity_component = -abs(angleDot) / 10.0\n",
    "   ```\n",
    "   - This component discourages rapid changes in the pole's angle.\n",
    "   - `abs(angleDot)` gives the absolute magnitude of the angular velocity.\n",
    "   - The negative sign ensures that faster angular velocities result in larger penalties (smaller rewards).\n",
    "   - Division by 10.0 scales this component to be in a similar range as the angle stability component.\n",
    "\n",
    "4. Combination:\n",
    "   ```python\n",
    "   return float(angle_stability + angular_velocity_component)\n",
    "   ```\n",
    "   - The function returns the sum of these two components.\n",
    "   - This combination encourages both a vertical pole position and minimal angular velocity.\n",
    "\n",
    "5. Focus on Stability:\n",
    "   - The function ignores the cart's position (`x`) and velocity (`xDot`), focusing solely on the pole's angle and angular velocity.\n",
    "   - This design prioritizes keeping the pole upright over the cart's position on the track.\n",
    "\n",
    "6. Why it works for stability:\n",
    "   - Highest rewards are given when the pole is vertical (angle near 0) and not moving quickly (low angular velocity).\n",
    "   - As the pole deviates from vertical or starts moving faster, the reward decreases.\n",
    "   - This incentivizes actions that keep the pole upright and still, which is the essence of stability in the cart-pole system.\n",
    "\n",
    "In summary, this reward function is tailored specifically for stability by rewarding a vertical pole position and penalizing both angle deviations and rapid angular movements. It disregards the cart's horizontal motion, emphasizing the pole's stability above all else.\n",
    "\n",
    "=================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Composite Reward Function with Focused Components:\n",
      "\n",
      "def stabilityReward(observation, action):\n",
      "    x, xDot, angle, angleDot = observation\n",
      "    # Focus purely on angle stability\n",
      "    angle_stability = 1.0 - abs(angle) / 0.209  # 0.209 radians is about 12 degrees\n",
      "    angular_velocity_component = -abs(angleDot) / 10.0  # Penalize fast angle changes\n",
      "    return float(angle_stability + angular_velocity_component)\n",
      "\n",
      "def energyEfficiencyReward(observation, action):\n",
      "    x, xDot, angle, angleDot = observation\n",
      "    # Focus purely on minimizing movement and energy use\n",
      "    cart_movement_penalty = -abs(xDot) / 5.0  # Penalize cart velocity\n",
      "    angular_movement_penalty = -abs(angleDot) / 5.0  # Penalize pole angular velocity\n",
      "    return float(1.0 + cart_movement_penalty + angular_movement_penalty)\n",
      "\n",
      "def timeBasedReward(observation, action):\n",
      "    x, xDot, angle, angleDot = observation\n",
      "    # Simple time-based reward that encourages survival\n",
      "    base_reward = 1.0\n",
      "    # Add small penalties for extreme positions/angles to prevent gaming\n",
      "    if abs(angle) > 0.209 or abs(x) > 2.4:  # If about to fail\n",
      "        base_reward = 0.0\n",
      "    return float(base_reward)\n",
      "\n",
      "def dynamicRewardFunction(observation, action):\n",
      "    x, xDot, angle, angleDot = observation\n",
      "    \n",
      "    # Get individual rewards\n",
      "    stability = stabilityReward(observation, action)\n",
      "    efficiency = energyEfficiencyReward(observation, action)\n",
      "    timeReward = timeBasedReward(observation, action)\n",
      "    \n",
      "    # Combine rewards with equal weights\n",
      "    return (stability + efficiency + timeReward) / 3.0\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Stability Reward Function Explanation:\n",
      "Certainly! This stability reward function is designed to encourage the agent to keep the pole balanced and stable. Let's break down each component and explain how it contributes to the goal of stability:\n",
      "\n",
      "1. Input:\n",
      "   - The function takes two parameters: `observation` and `action`.\n",
      "   - The `observation` is unpacked into four variables: `x`, `xDot`, `angle`, and `angleDot`.\n",
      "   - `x`: Position of the cart\n",
      "   - `xDot`: Velocity of the cart\n",
      "   - `angle`: Angle of the pole (in radians)\n",
      "   - `angleDot`: Angular velocity of the pole\n",
      "\n",
      "2. Angle Stability Component:\n",
      "   ```python\n",
      "   angle_stability = 1.0 - abs(angle) / 0.209\n",
      "   ```\n",
      "   - This component focuses on keeping the pole upright.\n",
      "   - The absolute value of the angle is used to treat deviations in both directions equally.\n",
      "   - 0.209 radians is approximately 12 degrees, which is often used as a termination condition in the CartPole environment.\n",
      "   - When the angle is 0 (perfectly upright), this component will be 1.0 (maximum reward).\n",
      "   - As the angle approaches ±0.209 radians, the reward decreases linearly towards 0.\n",
      "\n",
      "3. Angular Velocity Component:\n",
      "   ```python\n",
      "   angular_velocity_component = -abs(angleDot) / 10.0\n",
      "   ```\n",
      "   - This component discourages rapid changes in the pole's angle.\n",
      "   - The absolute value of `angleDot` is used to penalize fast movement in either direction.\n",
      "   - The division by 10.0 scales this component to have less impact than the angle stability component.\n",
      "   - The negative sign means that faster angular velocities result in larger penalties (negative rewards).\n",
      "\n",
      "4. Total Reward:\n",
      "   ```python\n",
      "   return float(angle_stability + angular_velocity_component)\n",
      "   ```\n",
      "   - The total reward is the sum of these two components.\n",
      "   - Converting to float ensures a consistent return type.\n",
      "\n",
      "Why this function focuses on stability:\n",
      "\n",
      "1. Upright Position: The `angle_stability` component strongly encourages keeping the pole near vertical.\n",
      "\n",
      "2. Minimal Oscillation: The `angular_velocity_component` discourages rapid changes in angle, which helps prevent oscillations and promotes smooth, stable behavior.\n",
      "\n",
      "3. Ignoring Cart Position: By not including terms for `x` and `xDot`, the function focuses solely on the pole's stability, regardless of where the cart is on the track.\n",
      "\n",
      "4. Continuous Feedback: The reward provides continuous feedback about how stable the pole is, rather than just a binary success/failure signal.\n",
      "\n",
      "5. Balanced Priorities: The function balances the importance of the pole's angle with its angular velocity, promoting overall stability.\n",
      "\n",
      "This reward function encourages the agent to learn a policy that keeps the pole as upright as possible while minimizing rapid movements, which are the key aspects of stability in the CartPole problem.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Episode 1/100, Total Reward: 18.21075112145614\n",
      "Episode 2/100, Total Reward: 15.276917317420544\n",
      "Episode 3/100, Total Reward: 12.256325069732332\n",
      "Episode 4/100, Total Reward: 24.06217154204567\n",
      "Episode 5/100, Total Reward: 19.81179388872164\n",
      "Episode 6/100, Total Reward: 30.01672614945613\n",
      "Episode 7/100, Total Reward: 8.619601941258875\n",
      "Episode 8/100, Total Reward: 9.077529700177365\n",
      "Episode 9/100, Total Reward: 36.86015430516932\n",
      "Episode 10/100, Total Reward: 16.228496118248746\n",
      "Episode 11/100, Total Reward: 9.686798204570723\n",
      "Episode 12/100, Total Reward: 13.683319705142308\n",
      "Episode 13/100, Total Reward: 38.85479317300194\n",
      "Episode 14/100, Total Reward: 26.82094257580905\n",
      "Episode 15/100, Total Reward: 17.45853880903308\n",
      "Episode 16/100, Total Reward: 7.625032611578655\n",
      "Episode 17/100, Total Reward: 12.368881005724145\n",
      "Episode 18/100, Total Reward: 15.658546668404574\n",
      "Episode 19/100, Total Reward: 39.1328458587281\n",
      "Episode 20/100, Total Reward: 7.187638942924204\n",
      "Episode 21/100, Total Reward: 29.21640574832015\n",
      "Episode 22/100, Total Reward: 6.372686483410365\n",
      "Episode 23/100, Total Reward: 6.376939760892396\n",
      "Episode 24/100, Total Reward: 14.303066601463408\n",
      "Episode 25/100, Total Reward: 25.06979872683714\n",
      "Episode 26/100, Total Reward: 16.08099903112082\n",
      "Episode 27/100, Total Reward: 12.945255146520152\n",
      "Episode 28/100, Total Reward: 27.150658006712263\n",
      "Episode 29/100, Total Reward: 14.23491339016718\n",
      "Episode 30/100, Total Reward: 9.341199810574302\n",
      "Episode 31/100, Total Reward: 16.316649921955925\n",
      "Episode 32/100, Total Reward: 23.669481789572476\n",
      "Episode 33/100, Total Reward: 21.130840614053646\n",
      "Episode 34/100, Total Reward: 13.150200350268891\n",
      "Episode 35/100, Total Reward: 17.46649967863609\n",
      "Episode 36/100, Total Reward: 6.0773751360050845\n",
      "Episode 37/100, Total Reward: 10.80810244696503\n",
      "Episode 38/100, Total Reward: 20.44376199807603\n",
      "Episode 39/100, Total Reward: 19.19945982082478\n",
      "Episode 40/100, Total Reward: 16.138800632392027\n",
      "Episode 41/100, Total Reward: 9.776140375476944\n",
      "Episode 42/100, Total Reward: 11.725534354031318\n",
      "Episode 43/100, Total Reward: 16.868417994289917\n",
      "Episode 44/100, Total Reward: 7.32910453357765\n",
      "Episode 45/100, Total Reward: 6.8754165096288675\n",
      "Episode 46/100, Total Reward: 13.713467180333804\n",
      "Episode 47/100, Total Reward: 8.146118495162975\n",
      "Episode 48/100, Total Reward: 16.390003153183915\n",
      "Episode 49/100, Total Reward: 13.491795448706366\n",
      "Episode 50/100, Total Reward: 24.30348250512241\n",
      "Episode 51/100, Total Reward: 28.59912762880026\n",
      "Episode 52/100, Total Reward: 12.237512769851318\n",
      "Episode 53/100, Total Reward: 7.1850397925437735\n",
      "Episode 54/100, Total Reward: 7.232799142736946\n",
      "Episode 55/100, Total Reward: 7.063999652096101\n",
      "Episode 56/100, Total Reward: 8.811440073518732\n",
      "Episode 57/100, Total Reward: 6.4524189365727675\n",
      "Episode 58/100, Total Reward: 6.987336266513742\n",
      "Episode 59/100, Total Reward: 14.220991674270568\n",
      "Episode 60/100, Total Reward: 9.664604744513591\n",
      "Episode 61/100, Total Reward: 7.942344759380299\n",
      "Episode 62/100, Total Reward: 6.153352725821023\n",
      "Episode 63/100, Total Reward: 6.425172068870239\n",
      "Episode 64/100, Total Reward: 18.7095788557013\n",
      "Episode 65/100, Total Reward: 12.09325663158566\n",
      "Episode 66/100, Total Reward: 10.542726205660111\n",
      "Episode 67/100, Total Reward: 17.1984044866291\n",
      "Episode 68/100, Total Reward: 37.39636114020842\n",
      "Episode 69/100, Total Reward: 57.20712473304195\n",
      "Episode 70/100, Total Reward: 5.533899518438693\n",
      "Episode 71/100, Total Reward: 23.98889588344073\n",
      "Episode 72/100, Total Reward: 8.86131515388141\n",
      "Episode 73/100, Total Reward: 32.48205588337682\n",
      "Episode 74/100, Total Reward: 13.881338742516782\n",
      "Episode 75/100, Total Reward: 27.012938058949437\n",
      "Episode 76/100, Total Reward: 27.983963791222457\n",
      "Episode 77/100, Total Reward: 7.202315341451902\n",
      "Episode 78/100, Total Reward: 5.444936414584636\n",
      "Episode 79/100, Total Reward: 7.146191632590797\n",
      "Episode 80/100, Total Reward: 27.985484714984473\n",
      "Episode 81/100, Total Reward: 17.736819961249967\n",
      "Episode 82/100, Total Reward: 7.2142254190457065\n",
      "Episode 83/100, Total Reward: 11.192503848149657\n",
      "Episode 84/100, Total Reward: 39.533451757061954\n",
      "Episode 85/100, Total Reward: 12.235459356549596\n",
      "Episode 86/100, Total Reward: 13.515103420020717\n",
      "Episode 87/100, Total Reward: 15.54732877903662\n",
      "Episode 88/100, Total Reward: 44.51087325977852\n",
      "Episode 89/100, Total Reward: 12.606959706218984\n",
      "Episode 90/100, Total Reward: 20.014459696734036\n",
      "Episode 91/100, Total Reward: 41.35272612502874\n",
      "Episode 92/100, Total Reward: 45.292384379594644\n",
      "Episode 93/100, Total Reward: 19.299359121214614\n",
      "Episode 94/100, Total Reward: 17.77076954392559\n",
      "Episode 95/100, Total Reward: 32.461830658550284\n",
      "Episode 96/100, Total Reward: 18.51136124974601\n",
      "Episode 97/100, Total Reward: 51.629169987616045\n",
      "Episode 98/100, Total Reward: 12.102093870099038\n",
      "Episode 99/100, Total Reward: 14.168449783515953\n",
      "Episode 100/100, Total Reward: 14.277877622251314\n"
     ]
    }
   ],
   "source": [
    "import anthropic\n",
    "import gymnasium as gym\n",
    "# from ...ExtraNotebooksCodeExamples.cartPoleShared import *\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get current working directory and go up to project root\n",
    "current_dir = os.getcwd()  # Gets current notebook directory\n",
    "project_root = str(Path(current_dir).parent.parent)  # Go up two levels\n",
    "\n",
    "# Add to path\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# Then import\n",
    "from ExtraNotebooksCodeExamples.cartPoleShared import *\n",
    "\n",
    "\n",
    "# API configuration\n",
    "apiKey = \"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\"\n",
    "modelName = \"claude-3-5-sonnet-20240620\"\n",
    "\n",
    "def createCompositeCode():\n",
    "    stabilityFunc = \"\"\"\n",
    "def stabilityReward(observation, action):\n",
    "    x, xDot, angle, angleDot = observation\n",
    "    # Focus purely on angle stability\n",
    "    angle_stability = 1.0 - abs(angle) / 0.209  # 0.209 radians is about 12 degrees\n",
    "    angular_velocity_component = -abs(angleDot) / 10.0  # Penalize fast angle changes\n",
    "    return float(angle_stability + angular_velocity_component)\n",
    "\"\"\"\n",
    "\n",
    "    efficiencyFunc = \"\"\"\n",
    "def energyEfficiencyReward(observation, action):\n",
    "    x, xDot, angle, angleDot = observation\n",
    "    # Focus purely on minimizing movement and energy use\n",
    "    cart_movement_penalty = -abs(xDot) / 5.0  # Penalize cart velocity\n",
    "    angular_movement_penalty = -abs(angleDot) / 5.0  # Penalize pole angular velocity\n",
    "    return float(1.0 + cart_movement_penalty + angular_movement_penalty)\n",
    "\"\"\"\n",
    "\n",
    "    timeFunc = \"\"\"\n",
    "def timeBasedReward(observation, action):\n",
    "    x, xDot, angle, angleDot = observation\n",
    "    # Simple time-based reward that encourages survival\n",
    "    base_reward = 1.0\n",
    "    # Add small penalties for extreme positions/angles to prevent gaming\n",
    "    if abs(angle) > 0.209 or abs(x) > 2.4:  # If about to fail\n",
    "        base_reward = 0.0\n",
    "    return float(base_reward)\n",
    "\"\"\"\n",
    "\n",
    "    dynamicFunc = \"\"\"\n",
    "def dynamicRewardFunction(observation, action):\n",
    "    x, xDot, angle, angleDot = observation\n",
    "    \n",
    "    # Get individual rewards\n",
    "    stability = stabilityReward(observation, action)\n",
    "    efficiency = energyEfficiencyReward(observation, action)\n",
    "    timeReward = timeBasedReward(observation, action)\n",
    "    \n",
    "    # Combine rewards with equal weights\n",
    "    return (stability + efficiency + timeReward) / 3.0\n",
    "\"\"\"\n",
    "    return stabilityFunc + efficiencyFunc + timeFunc + dynamicFunc\n",
    "\n",
    "def main():\n",
    "    # Get the composite code\n",
    "    compositeCode = createCompositeCode()\n",
    "    \n",
    "    print(\"Composite Reward Function with Focused Components:\")\n",
    "    print(compositeCode)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    \n",
    "    # Get explanation for stability reward\n",
    "    stabilityExplanationMessage = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\"Please explain the logic behind this focused stability reward function:\n",
    "\n",
    "            def stabilityReward(observation, action):\n",
    "                x, xDot, angle, angleDot = observation\n",
    "                angle_stability = 1.0 - abs(angle) / 0.209\n",
    "                angular_velocity_component = -abs(angleDot) / 10.0\n",
    "                return float(angle_stability + angular_velocity_component)\n",
    "\n",
    "            Explain why this function focuses specifically on stability and how each component contributes to this goal.\"\"\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    stabilityExplanation = queryAnthropicApi(apiKey, modelName, stabilityExplanationMessage)\n",
    "    print(\"Stability Reward Function Explanation:\")\n",
    "    print(stabilityExplanation)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    \n",
    "    # Create namespace and execute all functions\n",
    "    namespace = {}\n",
    "    exec(compositeCode, globals(), namespace)\n",
    "    \n",
    "    # Set up environment\n",
    "    env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "    env = CustomCartPoleEnv(env)\n",
    "    \n",
    "    # Update reward function with the namespace\n",
    "    for name, func in namespace.items():\n",
    "        globals()[name] = func\n",
    "    \n",
    "    env.setRewardFunction(namespace['dynamicRewardFunction'])\n",
    "    \n",
    "    # Initialize and train agent\n",
    "    stateSize = env.observation_space.shape[0]\n",
    "    actionSize = env.action_space.n\n",
    "    agent = DQLearningAgent(env, stateSize, actionSize, device)\n",
    "    \n",
    "    # Training loop\n",
    "    numEpisodes = 100\n",
    "    for episode in range(numEpisodes):\n",
    "        observation = env.reset()[0]\n",
    "        totalReward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.chooseAction(observation)\n",
    "            nextObservation, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            totalReward += reward\n",
    "            \n",
    "            agent.remember(observation, action, reward, nextObservation, done)\n",
    "            observation = nextObservation\n",
    "            \n",
    "        agent.replay(32)\n",
    "        print(f\"Episode {episode + 1}/{numEpisodes}, Total Reward: {totalReward}\")\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Is reward a good proxy for effectivness of the reward function?*\n",
    "\n",
    "### **Dynamically Updating Reward function**\n",
    "\n",
    "\n",
    "Attempting reward function update at episode 100\n",
    "\n",
    "Generating new reward function for component 1...\n",
    "\n",
    "Proposed Function:\n",
    "Here's a modified reward function with detailed inline comments explaining the changes:\n",
    "\n",
    "```python\n",
    "def reward_function_1(state):\n",
    "    \"\"\"\n",
    "    Custom reward function focusing on stability (pole angle and angular velocity).\n",
    "    \n",
    "    Args:\n",
    "    state (list): The current state [x, x_dot, theta, theta_dot]\n",
    "    \n",
    "    Returns:\n",
    "    float: The calculated reward\n",
    "    \"\"\"\n",
    "    # Unpack state variables\n",
    "    x, x_dot, theta, theta_dot = state\n",
    "    \n",
    "    # Define target values (center position and upright)\n",
    "    x_target, theta_target = 0.0, 0.0\n",
    "    \n",
    "    # Calculate errors\n",
    "    x_error = abs(x - x_target)\n",
    "    theta_error = abs(theta - theta_target)\n",
    "    \n",
    "    # Define weight factors (increased emphasis on angle and angular velocity)\n",
    "    w_x = 0.2  # Reduced from 1.0 to lower importance of cart position\n",
    "    w_x_dot = 0.1  # Reduced from 0.5 to lower importance of cart velocity\n",
    "    w_theta = 2.0  # Increased from 1.0 to emphasize pole angle\n",
    "    w_theta_dot = 1.5  # Increased from 0.5 to emphasize angular velocity\n",
    "\n",
    "    # Calculate component rewards (using gaussian-like functions for smoother transitions)\n",
    "    r_x = np.exp(-0.5 * (x_error / 0.5)**2)  # Widened gaussian (0.5 instead of 0.1) for more lenient position reward\n",
    "    r_x_dot = np.exp(-0.5 * (x_dot / 2.0)**2)  # Slightly widened gaussian for velocity\n",
    "    r_theta = np.exp(-0.5 * (theta_error / 0.2)**2)  # Narrowed gaussian (0.2 instead of 0.5) for stricter angle reward\n",
    "    r_theta_dot = np.exp(-0.5 * (theta_dot / 1.0)**2)  # Narrowed gaussian for stricter angular velocity reward\n",
    "\n",
    "    # Combine component rewards with weights\n",
    "    reward = w_x * r_x + w_x_dot * r_x_dot + w_theta * r_theta + w_theta_dot * r_theta_dot\n",
    "    \n",
    "    # Normalize reward to [0, 1] range\n",
    "    reward = reward / (w_x + w_x_dot + w_theta + w_theta_dot)\n",
    "    \n",
    "    return reward\n",
    "```\n",
    "\n",
    "Key changes and explanations:\n",
    "\n",
    "1. Increased emphasis on stability:\n",
    "   - Increased weight for theta (w_theta) from 1.0 to 2.0\n",
    "   - Increased weight for theta_dot (w_theta_dot) from 0.5 to 1.5\n",
    "   - Decreased weights for x and x_dot to shift focus to pole stability\n",
    "\n",
    "2. Modified gaussian functions for smoother rewards:\n",
    "   - Widened the gaussian for x (0.5 instead of 0.1) to be more lenient on cart position\n",
    "   - Narrowed the gaussian for theta (0.2 instead of 0.5) to be stricter on pole angle\n",
    "   - Adjusted gaussians for x_dot and theta_dot for better balance\n",
    "\n",
    "3. Normalized the final reward to ensure it stays in the [0, 1] range, making it easier to interpret and use in learning algorithms.\n",
    "\n",
    "These changes should result in a reward function that places higher importance on keeping the pole upright and stable, while being more forgiving of the cart's exact position. This should lead to improved stability in the pole balancing task.\n",
    "\n",
    "Waiting 10 seconds before critic evaluation...\n",
    "\n",
    "Getting critic's evaluation...\n",
    "\n",
    "Critic Response:\n",
    "1. The function is properly named 'reward_function_1', which is clear and distinct.\n",
    "\n",
    "2. The reward calculations appear mathematically sound. The use of Gaussian-like functions for smoother transitions is a good approach, and the normalization of the final reward to the [0, 1] range is appropriate.\n",
    "\n",
    "3. The function focuses appropriately on stability, particularly emphasizing the pole angle and angular velocity. This is achieved through increased weights for theta and theta_dot, and decreased weights for cart position and velocity.\n",
    "\n",
    "4. Potential issues:\n",
    "   - The function might be overly lenient on cart position, which could lead to the cart drifting too far from the center.\n",
    "   - The narrowed Gaussian for theta might make the task too difficult initially, potentially slowing down learning.\n",
    "   - The weights and Gaussian parameters are somewhat arbitrary and might need fine-tuning based on empirical results.\n",
    "   - There's no explicit penalty for failure states (e.g., pole falling past a certain angle), which might be beneficial.\n",
    "\n",
    "Overall, the function seems well-designed for its purpose, but may require some empirical testing and potential adjustments.\n",
    "\n",
    "Decision: Yes\n",
    "\n",
    "Critic Decision: Approved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cartPoleShared'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01manthropic\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcartPoleShared\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cartPoleShared'"
     ]
    }
   ],
   "source": [
    "import anthropic\n",
    "import gymnasium as gym\n",
    "from cartPoleShared import *\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import base64\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "# API configuration\n",
    "apiKey = \"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\"\n",
    "modelName = \"claude-3-5-sonnet-20240620\"\n",
    "\n",
    "class RewardUpdateSystem:\n",
    "    def __init__(self, apiKey: str, modelName: str, maxHistoryLength: int = 100, target_component: int = 1):\n",
    "        self.apiKey = apiKey\n",
    "        self.modelName = modelName\n",
    "        self.rewardHistory = deque(maxlen=maxHistoryLength)\n",
    "        self.episodeCount = 0\n",
    "        self.lastUpdateEpisode = 0\n",
    "        self.target_component = target_component\n",
    "    \n",
    "    def generateRewardGraph(self):\n",
    "        \"\"\"Generate a base64 encoded string of the reward history plot.\"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(list(self.rewardHistory), label=f'Reward Component {self.target_component}')\n",
    "        plt.title(f'Component {self.target_component} Reward History')\n",
    "        plt.xlabel('Steps')\n",
    "        plt.ylabel('Reward Value')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        buffer = io.BytesIO()\n",
    "        plt.savefig(buffer, format='png')\n",
    "        buffer.seek(0)\n",
    "        image_png = buffer.getvalue()\n",
    "        buffer.close()\n",
    "        plt.close()\n",
    "        \n",
    "        return base64.b64encode(image_png).decode()\n",
    "\n",
    "    def createUpdatePrompt(self, currentFunction: str, graph: str):\n",
    "        return [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"Analyze this reward component function and its performance graph, then suggest specific modifications.\n",
    "                \n",
    "                Current Function:\n",
    "                {currentFunction}\n",
    "                \n",
    "                Performance Graph (base64 encoded):\n",
    "                {graph}\n",
    "                \n",
    "                Requirements:\n",
    "                1. The function MUST be named 'reward_function_{self.target_component}'\n",
    "                2. Focus on stability (pole angle and angular velocity)\n",
    "                3. Include detailed inline comments explaining each change\n",
    "                4. Compare old vs new values in comments\n",
    "                \n",
    "                Output only the modified function with detailed inline comments explaining changes.\"\"\"\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    def createCriticPrompt(self, proposedFunction: str):\n",
    "        return [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"Act as a reward function critic. Analyze this proposed reward component:\n",
    "\n",
    "                {proposedFunction}\n",
    "\n",
    "                Evaluate:\n",
    "                1. Is the function properly named 'reward_function_{self.target_component}'?\n",
    "                2. Are reward calculations mathematically sound?\n",
    "                3. Does it focus appropriately on its specific aspect?\n",
    "                4. Are there any potential issues?\n",
    "                \n",
    "                End your response with EXACTLY one line containing only \"Decision: Yes\" or \"Decision: No\".\"\"\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "    def recordReward(self, info: dict):\n",
    "        \"\"\"Record the specific component reward value.\"\"\"\n",
    "        if 'component_rewards' in info:\n",
    "            component_reward = info['component_rewards'].get(f'reward_function_{self.target_component}')\n",
    "            if component_reward is not None:\n",
    "                self.rewardHistory.append(component_reward)\n",
    "        \n",
    "    def validateAndUpdate(self, currentFunction: str):\n",
    "        \"\"\"Generate and validate updated reward function with delays between API calls.\"\"\"\n",
    "        try:\n",
    "            # First API call for function update\n",
    "            print(f\"\\nGenerating new reward function for component {self.target_component}...\")\n",
    "            graph = self.generateRewardGraph()\n",
    "            updatePrompt = self.createUpdatePrompt(currentFunction, graph)\n",
    "            proposedFunction = queryAnthropicApi(self.apiKey, self.modelName, updatePrompt)\n",
    "            \n",
    "            print(\"\\nProposed Function:\")\n",
    "            print(proposedFunction)\n",
    "            \n",
    "            print(\"\\nWaiting 10 seconds before critic evaluation...\")\n",
    "            time.sleep(10)\n",
    "            \n",
    "            print(\"\\nGetting critic's evaluation...\")\n",
    "            criticPrompt = self.createCriticPrompt(proposedFunction)\n",
    "            criticResponse = queryAnthropicApi(self.apiKey, self.modelName, criticPrompt)\n",
    "            \n",
    "            print(\"\\nCritic Response:\")\n",
    "            print(criticResponse)\n",
    "            \n",
    "            approved = criticResponse.strip().endswith(\"Decision: Yes\")\n",
    "            print(f\"\\nCritic Decision: {'Approved' if approved else 'Rejected'}\")\n",
    "            \n",
    "            if approved:\n",
    "                return proposedFunction, True\n",
    "            return currentFunction, False\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError during update: {e}\")\n",
    "            return currentFunction, False\n",
    "\n",
    "    def waitingTime(self, episode: int):\n",
    "        \"\"\"Determine if it's time to update the reward function.\"\"\"\n",
    "        UPDATE_INTERVAL = 100\n",
    "        return episode - self.lastUpdateEpisode >= UPDATE_INTERVAL\n",
    "\n",
    "def main():\n",
    "    # Initialize with 3 reward components\n",
    "    env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "    env = CustomCartPoleEnv(env, num_components=3)\n",
    "    \n",
    "    # Initialize the update system targeting first component\n",
    "    updateSystem = RewardUpdateSystem(apiKey=apiKey, modelName=modelName, target_component=1)\n",
    "    \n",
    "    # Initialize agent\n",
    "    stateSize = env.observation_space.shape[0]\n",
    "    actionSize = env.action_space.n\n",
    "    agent = DQLearningAgent(env, stateSize, actionSize, device)\n",
    "    \n",
    "    # Training loop\n",
    "    numEpisodes = 500\n",
    "    for episode in range(numEpisodes):\n",
    "        observation = env.reset()[0]\n",
    "        totalReward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.chooseAction(observation)\n",
    "            nextObservation, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            totalReward += reward\n",
    "            \n",
    "            # Record the specific component reward\n",
    "            updateSystem.recordReward(info)\n",
    "            \n",
    "            agent.remember(observation, action, reward, nextObservation, done)\n",
    "            observation = nextObservation\n",
    "        \n",
    "        if updateSystem.waitingTime(episode):\n",
    "            print(f\"\\nAttempting reward function update at episode {episode}\")\n",
    "            func_name = f'reward_function_{updateSystem.target_component}'\n",
    "            current_func = env.reward_components[func_name]\n",
    "            newFunction, updated = updateSystem.validateAndUpdate(current_func)\n",
    "            \n",
    "            if updated:\n",
    "                print(f\"\\nUpdating reward component {updateSystem.target_component}...\")\n",
    "                env.set_component_reward(updateSystem.target_component, newFunction)\n",
    "                updateSystem.lastUpdateEpisode = episode\n",
    "            else:\n",
    "                print(\"\\nKeeping current reward function.\")\n",
    "        \n",
    "        agent.replay(32)\n",
    "        print(f\"Episode {episode + 1}/{numEpisodes}, Total Reward: {totalReward}\")\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gymenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
