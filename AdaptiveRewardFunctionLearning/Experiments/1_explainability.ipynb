{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **This is a notebook to give proof of concept of Claude being able to explain the reward functions generated.**\n",
    "\n",
    "**H0: Claude can explain how its generated reward functions work and also explain the changes it makes to reward functions based of information in its context.**\n",
    "\n",
    "\n",
    "Answer: Yes\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Composite Reward Function with Focused Components:\n",
    "\n",
    "```python\n",
    "def stabilityReward(observation, action):\n",
    "    x, xDot, angle, angleDot = observation\n",
    "    # Focus purely on angle stability\n",
    "    angle_stability = 1.0 - abs(angle) / 0.209  # 0.209 radians is about 12 degrees\n",
    "    angular_velocity_component = -abs(angleDot) / 10.0  # Penalize fast angle changes\n",
    "    return float(angle_stability + angular_velocity_component)\n",
    "```\n",
    "\n",
    "```python\n",
    "def energyEfficiencyReward(observation, action):\n",
    "    x, xDot, angle, angleDot = observation\n",
    "    # Focus purely on minimizing movement and energy use\n",
    "    cart_movement_penalty = -abs(xDot) / 5.0  # Penalize cart velocity\n",
    "    angular_movement_penalty = -abs(angleDot) / 5.0  # Penalize pole angular velocity\n",
    "    return float(1.0 + cart_movement_penalty + angular_movement_penalty)\n",
    "```\n",
    "\n",
    "```python\n",
    "def timeBasedReward(observation, action):\n",
    "    x, xDot, angle, angleDot = observation\n",
    "    # Simple time-based reward that encourages survival\n",
    "    base_reward = 1.0\n",
    "    # Add small penalties for extreme positions/angles to prevent gaming\n",
    "    if abs(angle) > 0.209 or abs(x) > 2.4:  # If about to fail\n",
    "        base_reward = 0.0\n",
    "    return float(base_reward)\n",
    "```\n",
    "\n",
    "--\n",
    "\n",
    "```python\n",
    "def dynamicRewardFunction(observation, action):\n",
    "    x, xDot, angle, angleDot = observation\n",
    "    \n",
    "    # Get individual rewards\n",
    "    stability = stabilityReward(observation, action)\n",
    "    efficiency = energyEfficiencyReward(observation, action)\n",
    "    timeReward = timeBasedReward(observation, action)\n",
    "    \n",
    "    # Combine rewards with equal weights\n",
    "    return (stability + efficiency + timeReward) / 3.0\n",
    "```\n",
    "\n",
    "==================================================\n",
    "\n",
    "Stability Reward Function Explanation:\n",
    "Certainly! This stability reward function is designed to encourage the cart-pole system to maintain a stable, upright position. Let's break down the components and explain how they contribute to the goal of stability:\n",
    "\n",
    "1. Input:\n",
    "   - The function takes two parameters: `observation` and `action`.\n",
    "   - `observation` is unpacked into four variables: `x`, `xDot`, `angle`, and `angleDot`.\n",
    "   - `x`: Position of the cart\n",
    "   - `xDot`: Velocity of the cart\n",
    "   - `angle`: Angle of the pole from vertical\n",
    "   - `angleDot`: Angular velocity of the pole\n",
    "\n",
    "2. Angle Stability Component:\n",
    "   ```python\n",
    "   angle_stability = 1.0 - abs(angle) / 0.209\n",
    "   ```\n",
    "   - This component focuses on keeping the pole as close to vertical as possible.\n",
    "   - `abs(angle)` gives the absolute value of the angle, ensuring symmetrical treatment for both positive and negative angles.\n",
    "   - 0.209 radians is approximately 12 degrees, which is often used as a termination condition in cart-pole problems.\n",
    "   - The subtraction from 1.0 inverts the scale, so smaller angles result in higher rewards.\n",
    "   - This component will be 1.0 when the pole is perfectly vertical (angle = 0) and approach 0 as the angle nears ±0.209 radians.\n",
    "\n",
    "3. Angular Velocity Component:\n",
    "   ```python\n",
    "   angular_velocity_component = -abs(angleDot) / 10.0\n",
    "   ```\n",
    "   - This component discourages rapid changes in the pole's angle.\n",
    "   - `abs(angleDot)` gives the absolute magnitude of the angular velocity.\n",
    "   - The negative sign ensures that faster angular velocities result in larger penalties (smaller rewards).\n",
    "   - Division by 10.0 scales this component to be in a similar range as the angle stability component.\n",
    "\n",
    "4. Combination:\n",
    "   ```python\n",
    "   return float(angle_stability + angular_velocity_component)\n",
    "   ```\n",
    "   - The function returns the sum of these two components.\n",
    "   - This combination encourages both a vertical pole position and minimal angular velocity.\n",
    "\n",
    "5. Focus on Stability:\n",
    "   - The function ignores the cart's position (`x`) and velocity (`xDot`), focusing solely on the pole's angle and angular velocity.\n",
    "   - This design prioritizes keeping the pole upright over the cart's position on the track.\n",
    "\n",
    "6. Why it works for stability:\n",
    "   - Highest rewards are given when the pole is vertical (angle near 0) and not moving quickly (low angular velocity).\n",
    "   - As the pole deviates from vertical or starts moving faster, the reward decreases.\n",
    "   - This incentivizes actions that keep the pole upright and still, which is the essence of stability in the cart-pole system.\n",
    "\n",
    "In summary, this reward function is tailored specifically for stability by rewarding a vertical pole position and penalizing both angle deviations and rapid angular movements. It disregards the cart's horizontal motion, emphasizing the pole's stability above all else.\n",
    "\n",
    "=================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Composite Reward Function with Focused Components:\n",
      "\n",
      "def stabilityReward(observation, action):\n",
      "    x, xDot, angle, angleDot = observation\n",
      "    # Focus purely on angle stability\n",
      "    angle_stability = 1.0 - abs(angle) / 0.209  # 0.209 radians is about 12 degrees\n",
      "    angular_velocity_component = -abs(angleDot) / 10.0  # Penalize fast angle changes\n",
      "    return float(angle_stability + angular_velocity_component)\n",
      "\n",
      "def energyEfficiencyReward(observation, action):\n",
      "    x, xDot, angle, angleDot = observation\n",
      "    # Focus purely on minimizing movement and energy use\n",
      "    cart_movement_penalty = -abs(xDot) / 5.0  # Penalize cart velocity\n",
      "    angular_movement_penalty = -abs(angleDot) / 5.0  # Penalize pole angular velocity\n",
      "    return float(1.0 + cart_movement_penalty + angular_movement_penalty)\n",
      "\n",
      "def timeBasedReward(observation, action):\n",
      "    x, xDot, angle, angleDot = observation\n",
      "    # Simple time-based reward that encourages survival\n",
      "    base_reward = 1.0\n",
      "    # Add small penalties for extreme positions/angles to prevent gaming\n",
      "    if abs(angle) > 0.209 or abs(x) > 2.4:  # If about to fail\n",
      "        base_reward = 0.0\n",
      "    return float(base_reward)\n",
      "\n",
      "def dynamicRewardFunction(observation, action):\n",
      "    x, xDot, angle, angleDot = observation\n",
      "    \n",
      "    # Get individual rewards\n",
      "    stability = stabilityReward(observation, action)\n",
      "    efficiency = energyEfficiencyReward(observation, action)\n",
      "    timeReward = timeBasedReward(observation, action)\n",
      "    \n",
      "    # Combine rewards with equal weights\n",
      "    return (stability + efficiency + timeReward) / 3.0\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Stability Reward Function Explanation:\n",
      "Certainly! This stability reward function is designed to encourage the agent to keep the pole upright and stable. Let's break down the components and explain how they contribute to the goal of stability:\n",
      "\n",
      "1. Input:\n",
      "   - `observation`: A tuple containing four values (x, xDot, angle, angleDot)\n",
      "   - `action`: The action taken by the agent (not used in this specific function)\n",
      "\n",
      "2. Components:\n",
      "\n",
      "   a. Angle Stability Component:\n",
      "      ```python\n",
      "      angle_stability = 1.0 - abs(angle) / 0.209\n",
      "      ```\n",
      "      - This component focuses on keeping the pole vertical.\n",
      "      - The absolute value of the angle is used to treat deviations in both directions equally.\n",
      "      - 0.209 radians is approximately 12 degrees, which is likely the maximum allowed angle before failure.\n",
      "      - As the angle approaches 0 (vertical), this component approaches 1 (maximum reward).\n",
      "      - As the angle approaches ±0.209, this component approaches 0 (minimum reward).\n",
      "\n",
      "   b. Angular Velocity Component:\n",
      "      ```python\n",
      "      angular_velocity_component = -abs(angleDot) / 10.0\n",
      "      ```\n",
      "      - This component discourages rapid changes in the pole's angle.\n",
      "      - The absolute value of angleDot is used to penalize fast movements in both directions.\n",
      "      - The negative sign makes this a penalty (reduces the overall reward).\n",
      "      - Dividing by 10.0 scales this component to have less impact than the angle stability component.\n",
      "\n",
      "3. Final Reward:\n",
      "   ```python\n",
      "   return float(angle_stability + angular_velocity_component)\n",
      "   ```\n",
      "   - The final reward is the sum of both components.\n",
      "   - Converting to float ensures a numerical output.\n",
      "\n",
      "4. Focus on Stability:\n",
      "   - Angle Stability: By rewarding smaller angles, the function encourages keeping the pole upright.\n",
      "   - Angular Velocity: By penalizing high angular velocities, the function discourages rapid oscillations or movements that could lead to instability.\n",
      "\n",
      "5. Why it works:\n",
      "   - The function provides the highest reward when the pole is perfectly vertical (angle = 0) and not moving (angleDot = 0).\n",
      "   - It smoothly decreases the reward as the pole deviates from vertical or as it moves more quickly.\n",
      "   - This guides the agent towards actions that maintain the pole's upright position with minimal oscillation.\n",
      "\n",
      "6. Limitations:\n",
      "   - This function doesn't consider the cart's position or velocity, which might be important for long-term stability.\n",
      "   - The scaling factors (0.209 and 10.0) might need tuning for optimal performance.\n",
      "\n",
      "In summary, this reward function focuses on stability by directly rewarding a vertical pole position and penalizing rapid angular movements, guiding the agent towards stable control of the pole.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Episode 1/100, Total Reward: 15.55665699603798\n",
      "Episode 2/100, Total Reward: 9.019264520772479\n",
      "Episode 3/100, Total Reward: 27.049961635314727\n",
      "Episode 4/100, Total Reward: 10.518156444131616\n",
      "Episode 5/100, Total Reward: 11.309013794548514\n",
      "Episode 6/100, Total Reward: 14.719810052954722\n",
      "Episode 7/100, Total Reward: 37.79718589119288\n",
      "Episode 8/100, Total Reward: 40.85388378070919\n",
      "Episode 9/100, Total Reward: 11.615907236283713\n",
      "Episode 10/100, Total Reward: 29.5976979367582\n",
      "Episode 11/100, Total Reward: 7.440564688968887\n",
      "Episode 12/100, Total Reward: 13.334610008342082\n",
      "Episode 13/100, Total Reward: 8.863101954571867\n",
      "Episode 14/100, Total Reward: 14.913390376736846\n",
      "Episode 15/100, Total Reward: 13.351906231030574\n",
      "Episode 16/100, Total Reward: 20.74823559832387\n",
      "Episode 17/100, Total Reward: 7.980389084696674\n",
      "Episode 18/100, Total Reward: 8.183508467683762\n",
      "Episode 19/100, Total Reward: 9.010289244950293\n",
      "Episode 20/100, Total Reward: 18.028557291206873\n",
      "Episode 21/100, Total Reward: 11.896319885781267\n",
      "Episode 22/100, Total Reward: 8.119290694230171\n",
      "Episode 23/100, Total Reward: 12.541716830023345\n",
      "Episode 24/100, Total Reward: 7.420315971103107\n",
      "Episode 25/100, Total Reward: 8.821529350797551\n",
      "Episode 26/100, Total Reward: 43.5276175993262\n",
      "Episode 27/100, Total Reward: 11.070400422105665\n",
      "Episode 28/100, Total Reward: 11.833412698556504\n",
      "Episode 29/100, Total Reward: 9.12880599232839\n",
      "Episode 30/100, Total Reward: 14.321036490282278\n",
      "Episode 31/100, Total Reward: 35.28265435184661\n",
      "Episode 32/100, Total Reward: 10.045136307615437\n",
      "Episode 33/100, Total Reward: 13.701119964146033\n",
      "Episode 34/100, Total Reward: 21.053075293029558\n",
      "Episode 35/100, Total Reward: 16.774949873534343\n",
      "Episode 36/100, Total Reward: 12.957381380928092\n",
      "Episode 37/100, Total Reward: 22.759379338101375\n",
      "Episode 38/100, Total Reward: 8.301501886496109\n",
      "Episode 39/100, Total Reward: 6.286118737365931\n",
      "Episode 40/100, Total Reward: 17.93599538068626\n",
      "Episode 41/100, Total Reward: 15.29109975772522\n",
      "Episode 42/100, Total Reward: 6.536677215765252\n",
      "Episode 43/100, Total Reward: 18.87818720816245\n",
      "Episode 44/100, Total Reward: 11.329524398124011\n",
      "Episode 45/100, Total Reward: 7.019324094092732\n",
      "Episode 46/100, Total Reward: 6.314278835909027\n",
      "Episode 47/100, Total Reward: 5.660019482565269\n",
      "Episode 48/100, Total Reward: 25.241033398372196\n",
      "Episode 49/100, Total Reward: 13.389455032826291\n",
      "Episode 50/100, Total Reward: 9.234733010656788\n",
      "Episode 51/100, Total Reward: 11.168440357247361\n",
      "Episode 52/100, Total Reward: 23.39554119568799\n",
      "Episode 53/100, Total Reward: 16.910212730720605\n",
      "Episode 54/100, Total Reward: 8.325809015220171\n",
      "Episode 55/100, Total Reward: 21.946238190282585\n",
      "Episode 56/100, Total Reward: 15.971798745857962\n",
      "Episode 57/100, Total Reward: 14.034618597680131\n",
      "Episode 58/100, Total Reward: 16.80442082246787\n",
      "Episode 59/100, Total Reward: 15.758069005139188\n",
      "Episode 60/100, Total Reward: 15.733018879719125\n",
      "Episode 61/100, Total Reward: 32.67172801562896\n",
      "Episode 62/100, Total Reward: 20.356856945868806\n",
      "Episode 63/100, Total Reward: 9.482141474030609\n",
      "Episode 64/100, Total Reward: 7.340775685716699\n",
      "Episode 65/100, Total Reward: 8.876234728675426\n",
      "Episode 66/100, Total Reward: 16.275608888526328\n",
      "Episode 67/100, Total Reward: 14.100044399014921\n",
      "Episode 68/100, Total Reward: 9.056989942453622\n",
      "Episode 69/100, Total Reward: 12.211961798078448\n",
      "Episode 70/100, Total Reward: 10.875235831196141\n",
      "Episode 71/100, Total Reward: 6.437228716974054\n",
      "Episode 72/100, Total Reward: 8.434837504626628\n",
      "Episode 73/100, Total Reward: 11.649424340010901\n",
      "Episode 74/100, Total Reward: 9.84321136246933\n",
      "Episode 75/100, Total Reward: 8.105459249546037\n",
      "Episode 76/100, Total Reward: 5.22615110847083\n",
      "Episode 77/100, Total Reward: 17.030465592680564\n",
      "Episode 78/100, Total Reward: 12.900631400657764\n",
      "Episode 79/100, Total Reward: 15.221602573455828\n",
      "Episode 80/100, Total Reward: 7.435762460056864\n",
      "Episode 81/100, Total Reward: 12.848227631226418\n",
      "Episode 82/100, Total Reward: 26.444015433831474\n",
      "Episode 83/100, Total Reward: 11.006704714002929\n",
      "Episode 84/100, Total Reward: 8.370321498878978\n",
      "Episode 85/100, Total Reward: 7.345643692643068\n",
      "Episode 86/100, Total Reward: 14.610466237116958\n",
      "Episode 87/100, Total Reward: 9.803466470992605\n",
      "Episode 88/100, Total Reward: 7.666896613227408\n",
      "Episode 89/100, Total Reward: 6.537435557668288\n",
      "Episode 90/100, Total Reward: 11.659858637327474\n",
      "Episode 91/100, Total Reward: 7.102661978385666\n",
      "Episode 92/100, Total Reward: 7.656485945982132\n",
      "Episode 93/100, Total Reward: 15.810843754915052\n",
      "Episode 94/100, Total Reward: 26.258768279144054\n",
      "Episode 95/100, Total Reward: 12.0381760948152\n",
      "Episode 96/100, Total Reward: 9.85188230662926\n",
      "Episode 97/100, Total Reward: 12.352047394675338\n",
      "Episode 98/100, Total Reward: 9.860366263512342\n",
      "Episode 99/100, Total Reward: 7.999591718577338\n",
      "Episode 100/100, Total Reward: 9.679675354427127\n"
     ]
    }
   ],
   "source": [
    "import anthropic\n",
    "import gymnasium as gym\n",
    "# from ...ExtraNotebooksCodeExamples.cartPoleShared import *\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get current working directory and go up to project root\n",
    "current_dir = os.getcwd()  # Gets current notebook directory\n",
    "project_root = str(Path(current_dir).parent.parent)  # Go up two levels\n",
    "\n",
    "# Add to path\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# Then import\n",
    "from ExtraNotebooksCodeExamples.cartPoleShared import *\n",
    "\n",
    "\n",
    "# API configuration\n",
    "apiKey = \"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\"\n",
    "modelName = \"claude-3-5-sonnet-20240620\"\n",
    "\n",
    "def createCompositeCode():\n",
    "    stabilityFunc = \"\"\"\n",
    "def stabilityReward(observation, action):\n",
    "    x, xDot, angle, angleDot = observation\n",
    "    # Focus purely on angle stability\n",
    "    angle_stability = 1.0 - abs(angle) / 0.209  # 0.209 radians is about 12 degrees\n",
    "    angular_velocity_component = -abs(angleDot) / 10.0  # Penalize fast angle changes\n",
    "    return float(angle_stability + angular_velocity_component)\n",
    "\"\"\"\n",
    "\n",
    "    efficiencyFunc = \"\"\"\n",
    "def energyEfficiencyReward(observation, action):\n",
    "    x, xDot, angle, angleDot = observation\n",
    "    # Focus purely on minimizing movement and energy use\n",
    "    cart_movement_penalty = -abs(xDot) / 5.0  # Penalize cart velocity\n",
    "    angular_movement_penalty = -abs(angleDot) / 5.0  # Penalize pole angular velocity\n",
    "    return float(1.0 + cart_movement_penalty + angular_movement_penalty)\n",
    "\"\"\"\n",
    "\n",
    "    timeFunc = \"\"\"\n",
    "def timeBasedReward(observation, action):\n",
    "    x, xDot, angle, angleDot = observation\n",
    "    # Simple time-based reward that encourages survival\n",
    "    base_reward = 1.0\n",
    "    # Add small penalties for extreme positions/angles to prevent gaming\n",
    "    if abs(angle) > 0.209 or abs(x) > 2.4:  # If about to fail\n",
    "        base_reward = 0.0\n",
    "    return float(base_reward)\n",
    "\"\"\"\n",
    "\n",
    "    dynamicFunc = \"\"\"\n",
    "def dynamicRewardFunction(observation, action):\n",
    "    x, xDot, angle, angleDot = observation\n",
    "    \n",
    "    # Get individual rewards\n",
    "    stability = stabilityReward(observation, action)\n",
    "    efficiency = energyEfficiencyReward(observation, action)\n",
    "    timeReward = timeBasedReward(observation, action)\n",
    "    \n",
    "    # Combine rewards with equal weights\n",
    "    return (stability + efficiency + timeReward) / 3.0\n",
    "\"\"\"\n",
    "    return stabilityFunc + efficiencyFunc + timeFunc + dynamicFunc\n",
    "\n",
    "def main():\n",
    "    # Get the composite code\n",
    "    compositeCode = createCompositeCode()\n",
    "    \n",
    "    print(\"Composite Reward Function with Focused Components:\")\n",
    "    print(compositeCode)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    \n",
    "    # Get explanation for stability reward\n",
    "    stabilityExplanationMessage = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\"Please explain the logic behind this focused stability reward function:\n",
    "\n",
    "            def stabilityReward(observation, action):\n",
    "                x, xDot, angle, angleDot = observation\n",
    "                angle_stability = 1.0 - abs(angle) / 0.209\n",
    "                angular_velocity_component = -abs(angleDot) / 10.0\n",
    "                return float(angle_stability + angular_velocity_component)\n",
    "\n",
    "            Explain why this function focuses specifically on stability and how each component contributes to this goal.\"\"\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    stabilityExplanation = queryAnthropicApi(apiKey, modelName, stabilityExplanationMessage)\n",
    "    print(\"Stability Reward Function Explanation:\")\n",
    "    print(stabilityExplanation)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    \n",
    "    # Create namespace and execute all functions\n",
    "    namespace = {}\n",
    "    exec(compositeCode, globals(), namespace)\n",
    "    \n",
    "    # Set up environment\n",
    "    env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "    env = CustomCartPoleEnv(env)\n",
    "    \n",
    "    # Update reward function with the namespace\n",
    "    for name, func in namespace.items():\n",
    "        globals()[name] = func\n",
    "    \n",
    "    env.setRewardFunction(namespace['dynamicRewardFunction'])\n",
    "    \n",
    "    # Initialize and train agent\n",
    "    stateSize = env.observation_space.shape[0]\n",
    "    actionSize = env.action_space.n\n",
    "    agent = DQLearningAgent(env, stateSize, actionSize, device)\n",
    "    \n",
    "    # Training loop\n",
    "    numEpisodes = 100\n",
    "    for episode in range(numEpisodes):\n",
    "        observation = env.reset()[0]\n",
    "        totalReward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.chooseAction(observation)\n",
    "            nextObservation, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            totalReward += reward\n",
    "            \n",
    "            agent.remember(observation, action, reward, nextObservation, done)\n",
    "            observation = nextObservation\n",
    "            \n",
    "        agent.replay(32)\n",
    "        print(f\"Episode {episode + 1}/{numEpisodes}, Total Reward: {totalReward}\")\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Is reward a good proxy for effectivness of the reward function?*\n",
    "\n",
    "### **Dynamically Updating Reward function**\n",
    "\n",
    "\n",
    "Attempting reward function update at episode 100\n",
    "\n",
    "Generating new reward function for component 1...\n",
    "\n",
    "Proposed Function:\n",
    "Here's a modified reward function with detailed inline comments explaining the changes:\n",
    "\n",
    "```python\n",
    "def reward_function_1(state):\n",
    "    \"\"\"\n",
    "    Custom reward function focusing on stability (pole angle and angular velocity).\n",
    "    \n",
    "    Args:\n",
    "    state (list): The current state [x, x_dot, theta, theta_dot]\n",
    "    \n",
    "    Returns:\n",
    "    float: The calculated reward\n",
    "    \"\"\"\n",
    "    # Unpack state variables\n",
    "    x, x_dot, theta, theta_dot = state\n",
    "    \n",
    "    # Define target values (center position and upright)\n",
    "    x_target, theta_target = 0.0, 0.0\n",
    "    \n",
    "    # Calculate errors\n",
    "    x_error = abs(x - x_target)\n",
    "    theta_error = abs(theta - theta_target)\n",
    "    \n",
    "    # Define weight factors (increased emphasis on angle and angular velocity)\n",
    "    w_x = 0.2  # Reduced from 1.0 to lower importance of cart position\n",
    "    w_x_dot = 0.1  # Reduced from 0.5 to lower importance of cart velocity\n",
    "    w_theta = 2.0  # Increased from 1.0 to emphasize pole angle\n",
    "    w_theta_dot = 1.5  # Increased from 0.5 to emphasize angular velocity\n",
    "\n",
    "    # Calculate component rewards (using gaussian-like functions for smoother transitions)\n",
    "    r_x = np.exp(-0.5 * (x_error / 0.5)**2)  # Widened gaussian (0.5 instead of 0.1) for more lenient position reward\n",
    "    r_x_dot = np.exp(-0.5 * (x_dot / 2.0)**2)  # Slightly widened gaussian for velocity\n",
    "    r_theta = np.exp(-0.5 * (theta_error / 0.2)**2)  # Narrowed gaussian (0.2 instead of 0.5) for stricter angle reward\n",
    "    r_theta_dot = np.exp(-0.5 * (theta_dot / 1.0)**2)  # Narrowed gaussian for stricter angular velocity reward\n",
    "\n",
    "    # Combine component rewards with weights\n",
    "    reward = w_x * r_x + w_x_dot * r_x_dot + w_theta * r_theta + w_theta_dot * r_theta_dot\n",
    "    \n",
    "    # Normalize reward to [0, 1] range\n",
    "    reward = reward / (w_x + w_x_dot + w_theta + w_theta_dot)\n",
    "    \n",
    "    return reward\n",
    "```\n",
    "\n",
    "Key changes and explanations:\n",
    "\n",
    "1. Increased emphasis on stability:\n",
    "   - Increased weight for theta (w_theta) from 1.0 to 2.0\n",
    "   - Increased weight for theta_dot (w_theta_dot) from 0.5 to 1.5\n",
    "   - Decreased weights for x and x_dot to shift focus to pole stability\n",
    "\n",
    "2. Modified gaussian functions for smoother rewards:\n",
    "   - Widened the gaussian for x (0.5 instead of 0.1) to be more lenient on cart position\n",
    "   - Narrowed the gaussian for theta (0.2 instead of 0.5) to be stricter on pole angle\n",
    "   - Adjusted gaussians for x_dot and theta_dot for better balance\n",
    "\n",
    "3. Normalized the final reward to ensure it stays in the [0, 1] range, making it easier to interpret and use in learning algorithms.\n",
    "\n",
    "These changes should result in a reward function that places higher importance on keeping the pole upright and stable, while being more forgiving of the cart's exact position. This should lead to improved stability in the pole balancing task.\n",
    "\n",
    "Waiting 10 seconds before critic evaluation...\n",
    "\n",
    "Getting critic's evaluation...\n",
    "\n",
    "Critic Response:\n",
    "1. The function is properly named 'reward_function_1', which is clear and distinct.\n",
    "\n",
    "2. The reward calculations appear mathematically sound. The use of Gaussian-like functions for smoother transitions is a good approach, and the normalization of the final reward to the [0, 1] range is appropriate.\n",
    "\n",
    "3. The function focuses appropriately on stability, particularly emphasizing the pole angle and angular velocity. This is achieved through increased weights for theta and theta_dot, and decreased weights for cart position and velocity.\n",
    "\n",
    "4. Potential issues:\n",
    "   - The function might be overly lenient on cart position, which could lead to the cart drifting too far from the center.\n",
    "   - The narrowed Gaussian for theta might make the task too difficult initially, potentially slowing down learning.\n",
    "   - The weights and Gaussian parameters are somewhat arbitrary and might need fine-tuning based on empirical results.\n",
    "   - There's no explicit penalty for failure states (e.g., pole falling past a certain angle), which might be beneficial.\n",
    "\n",
    "Overall, the function seems well-designed for its purpose, but may require some empirical testing and potential adjustments.\n",
    "\n",
    "Decision: Yes\n",
    "\n",
    "Critic Decision: Approved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/500, Total Reward: 23.902152061462402\n",
      "Episode 2/500, Total Reward: 8.916226029396057\n",
      "Episode 3/500, Total Reward: 20.897331058979034\n",
      "Episode 4/500, Total Reward: 19.92051750421524\n",
      "Episode 5/500, Total Reward: 21.866618514060974\n",
      "Episode 6/500, Total Reward: 20.923004388809204\n",
      "Episode 7/500, Total Reward: 13.902259349822998\n",
      "Episode 8/500, Total Reward: 18.893116295337677\n",
      "Episode 9/500, Total Reward: 22.837060272693634\n",
      "Episode 10/500, Total Reward: 20.760030388832092\n",
      "Episode 11/500, Total Reward: 26.80826872587204\n",
      "Episode 12/500, Total Reward: 16.93121910095215\n",
      "Episode 13/500, Total Reward: 22.86255443096161\n",
      "Episode 14/500, Total Reward: 15.938780307769775\n",
      "Episode 15/500, Total Reward: 15.905032396316528\n",
      "Episode 16/500, Total Reward: 17.873260021209717\n",
      "Episode 17/500, Total Reward: 73.56826657056808\n",
      "Episode 18/500, Total Reward: 21.909946739673615\n",
      "Episode 19/500, Total Reward: 41.82305556535721\n",
      "Episode 20/500, Total Reward: 12.914095520973206\n",
      "Episode 21/500, Total Reward: 21.90552407503128\n",
      "Episode 22/500, Total Reward: 11.894662618637085\n",
      "Episode 23/500, Total Reward: 16.90883558988571\n",
      "Episode 24/500, Total Reward: 12.930692434310913\n",
      "Episode 25/500, Total Reward: 13.892930030822754\n",
      "Episode 26/500, Total Reward: 12.924044847488403\n",
      "Episode 27/500, Total Reward: 39.91977119445801\n",
      "Episode 28/500, Total Reward: 15.93460875749588\n",
      "Episode 29/500, Total Reward: 35.816460728645325\n",
      "Episode 30/500, Total Reward: 12.915684521198273\n",
      "Episode 31/500, Total Reward: 34.79059398174286\n",
      "Episode 32/500, Total Reward: 15.902614414691925\n",
      "Episode 33/500, Total Reward: 13.921321988105774\n",
      "Episode 34/500, Total Reward: 24.87704050540924\n",
      "Episode 35/500, Total Reward: 23.899779617786407\n",
      "Episode 36/500, Total Reward: 14.88031381368637\n",
      "Episode 37/500, Total Reward: 13.933926105499268\n",
      "Episode 38/500, Total Reward: 15.922894358634949\n",
      "Episode 39/500, Total Reward: 8.910295963287354\n",
      "Episode 40/500, Total Reward: 16.887507736682892\n",
      "Episode 41/500, Total Reward: 18.92707669734955\n",
      "Episode 42/500, Total Reward: 14.91852182149887\n",
      "Episode 43/500, Total Reward: 8.918877065181732\n",
      "Episode 44/500, Total Reward: 11.926542341709137\n",
      "Episode 45/500, Total Reward: 27.84447318315506\n",
      "Episode 46/500, Total Reward: 15.928816378116608\n",
      "Episode 47/500, Total Reward: 11.932809591293335\n",
      "Episode 48/500, Total Reward: 19.888459026813507\n",
      "Episode 49/500, Total Reward: 26.855718433856964\n",
      "Episode 50/500, Total Reward: 33.85300809144974\n",
      "Episode 51/500, Total Reward: 11.911342680454254\n",
      "Episode 52/500, Total Reward: 11.911852240562439\n",
      "Episode 53/500, Total Reward: 8.929083228111267\n",
      "Episode 54/500, Total Reward: 15.892263114452362\n",
      "Episode 55/500, Total Reward: 15.88486760854721\n",
      "Episode 56/500, Total Reward: 18.90873873233795\n",
      "Episode 57/500, Total Reward: 13.945194840431213\n",
      "Episode 58/500, Total Reward: 21.90770810842514\n",
      "Episode 59/500, Total Reward: 31.908500254154205\n",
      "Episode 60/500, Total Reward: 13.92505657672882\n",
      "Episode 61/500, Total Reward: 12.909906148910522\n",
      "Episode 62/500, Total Reward: 19.894253492355347\n",
      "Episode 63/500, Total Reward: 14.91524076461792\n",
      "Episode 64/500, Total Reward: 13.878604650497437\n",
      "Episode 65/500, Total Reward: 9.92320328950882\n",
      "Episode 66/500, Total Reward: 22.872262239456177\n",
      "Episode 67/500, Total Reward: 9.926365554332733\n",
      "Episode 68/500, Total Reward: 11.933684468269348\n",
      "Episode 69/500, Total Reward: 18.842812418937683\n",
      "Episode 70/500, Total Reward: 23.89674800634384\n",
      "Episode 71/500, Total Reward: 25.85741776227951\n",
      "Episode 72/500, Total Reward: 34.82338398694992\n",
      "Episode 73/500, Total Reward: 13.921322107315063\n",
      "Episode 74/500, Total Reward: 22.933088839054108\n",
      "Episode 75/500, Total Reward: 11.924884915351868\n",
      "Episode 76/500, Total Reward: 35.858353674411774\n",
      "Episode 77/500, Total Reward: 10.922159135341644\n",
      "Episode 78/500, Total Reward: 18.927187144756317\n",
      "Episode 79/500, Total Reward: 26.88473331928253\n",
      "Episode 80/500, Total Reward: 29.88483077287674\n",
      "Episode 81/500, Total Reward: 10.902979016304016\n",
      "Episode 82/500, Total Reward: 11.908820033073425\n",
      "Episode 83/500, Total Reward: 9.929243326187134\n",
      "Episode 84/500, Total Reward: 25.915854036808014\n",
      "Episode 85/500, Total Reward: 15.914108872413635\n",
      "Episode 86/500, Total Reward: 15.898838937282562\n",
      "Episode 87/500, Total Reward: 13.907862961292267\n",
      "Episode 88/500, Total Reward: 20.928519070148468\n",
      "Episode 89/500, Total Reward: 13.93072134256363\n",
      "Episode 90/500, Total Reward: 11.902306914329529\n",
      "Episode 91/500, Total Reward: 15.916359901428223\n",
      "Episode 92/500, Total Reward: 13.946705281734467\n",
      "Episode 93/500, Total Reward: 17.911989271640778\n",
      "Episode 94/500, Total Reward: 10.919867873191833\n",
      "Episode 95/500, Total Reward: 11.902166485786438\n",
      "Episode 96/500, Total Reward: 9.9344984292984\n",
      "Episode 97/500, Total Reward: 11.905964016914368\n",
      "Episode 98/500, Total Reward: 12.910173237323761\n",
      "Episode 99/500, Total Reward: 11.922431468963623\n",
      "Episode 100/500, Total Reward: 10.940325438976288\n",
      "\n",
      "Attempting reward function update at episode 100\n",
      "\n",
      "Generating new reward function for component 1...\n",
      "\n",
      "Proposed Function:\n",
      "Here's a modified reward function with detailed inline comments explaining the changes:\n",
      "\n",
      "```python\n",
      "def reward_function_1(state):\n",
      "    \"\"\"\n",
      "    Reward function focused on stability (pole angle and angular velocity).\n",
      "    \n",
      "    Args:\n",
      "    state (list): The current state [x, x_dot, theta, theta_dot]\n",
      "    \n",
      "    Returns:\n",
      "    float: The calculated reward\n",
      "    \"\"\"\n",
      "    # Unpack state variables\n",
      "    x, x_dot, theta, theta_dot = state\n",
      "    \n",
      "    # Define angle thresholds (in radians)\n",
      "    angle_threshold = 0.1  # Reduced from 0.2 for tighter control\n",
      "    \n",
      "    # Define angular velocity threshold (in radians/second)\n",
      "    angular_velocity_threshold = 0.5  # Reduced from 1.0 for smoother motion\n",
      "    \n",
      "    # Base reward\n",
      "    reward = 1.0\n",
      "    \n",
      "    # Penalize based on pole angle\n",
      "    if abs(theta) > angle_threshold:\n",
      "        # Quadratic penalty for more aggressive correction\n",
      "        # Old: reward -= 0.5\n",
      "        # New: Scaled quadratic penalty\n",
      "        reward -= 0.8 * (abs(theta) / angle_threshold) ** 2\n",
      "    \n",
      "    # Penalize based on angular velocity\n",
      "    if abs(theta_dot) > angular_velocity_threshold:\n",
      "        # Linear penalty with increased weight\n",
      "        # Old: reward -= 0.1\n",
      "        # New: Increased penalty and made it proportional\n",
      "        reward -= 0.4 * (abs(theta_dot) / angular_velocity_threshold)\n",
      "    \n",
      "    # Bonus for keeping the pole very stable\n",
      "    if abs(theta) < 0.05 and abs(theta_dot) < 0.1:\n",
      "        # Increased bonus for perfect stability\n",
      "        # Old: No bonus\n",
      "        # New: Significant bonus for maintaining near-perfect balance\n",
      "        reward += 0.5\n",
      "    \n",
      "    # Ensure reward is not negative\n",
      "    reward = max(reward, 0)\n",
      "    \n",
      "    return reward\n",
      "```\n",
      "\n",
      "This modified reward function focuses more on stability by:\n",
      "\n",
      "1. Reducing the angle and angular velocity thresholds for tighter control.\n",
      "2. Implementing a quadratic penalty for pole angle deviation, encouraging more aggressive correction for larger angles.\n",
      "3. Increasing the penalty for high angular velocities and making it proportional to the excess velocity.\n",
      "4. Adding a significant bonus for maintaining near-perfect balance, incentivizing the agent to achieve and maintain a very stable position.\n",
      "\n",
      "These changes should result in a more stable cart-pole system with smoother motion and quicker corrections to deviations.\n",
      "\n",
      "Waiting 10 seconds before critic evaluation...\n",
      "\n",
      "Getting critic's evaluation...\n",
      "\n",
      "Critic Response:\n",
      "1. The function is properly named 'reward_function_1', which is appropriate for a first version or variant of a reward function.\n",
      "\n",
      "2. The reward calculations are mathematically sound. The use of quadratic and linear penalties, as well as the bonus for stability, are all mathematically valid approaches to shaping the reward.\n",
      "\n",
      "3. The function focuses appropriately on its specific aspect, which is stability. It considers both the pole angle and angular velocity, which are key factors in maintaining stability in a cart-pole system.\n",
      "\n",
      "4. Potential issues:\n",
      "   - The function doesn't consider the cart's position or velocity, which could lead to the cart moving off the track while maintaining a stable pole.\n",
      "   - The reward is capped at a minimum of 0, which might limit the agent's ability to learn from severely unstable states.\n",
      "   - The abrupt threshold for the stability bonus (abs(theta) < 0.05 and abs(theta_dot) < 0.1) might create a discontinuity in the reward function, potentially leading to erratic behavior near these thresholds.\n",
      "   - The function doesn't consider the cumulative stability over time, which might be important for long-term balance.\n",
      "\n",
      "Despite these potential issues, the reward function is well-structured and focuses on its intended aspect of stability.\n",
      "\n",
      "Decision: Yes\n",
      "\n",
      "Critic Decision: Approved\n",
      "\n",
      "Updating reward component 1...\n",
      "Episode 101/500, Total Reward: 33.905257761478424\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 178\u001b[0m\n\u001b[0;32m    175\u001b[0m     env\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 178\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 149\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m    148\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mchooseAction(observation)\n\u001b[1;32m--> 149\u001b[0m     nextObservation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m     done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[0;32m    151\u001b[0m     totalReward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "File \u001b[1;32mc:\\Users\\samdd\\Desktop\\College\\4th Year\\FYProject\\SimpleCartPoleImplementation\\ExtraNotebooksCodeExamples\\cartPoleShared.py:93\u001b[0m, in \u001b[0;36mCustomCartPoleEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     91\u001b[0m             rewards\u001b[38;5;241m.\u001b[39mappend(component_reward)\n\u001b[0;32m     92\u001b[0m             info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomponent_rewards\u001b[39m\u001b[38;5;124m'\u001b[39m][name] \u001b[38;5;241m=\u001b[39m component_reward\n\u001b[1;32m---> 93\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(rewards) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(rewards) \u001b[38;5;28;01mif\u001b[39;00m rewards \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrewardFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;66;03m# Original behavior\u001b[39;00m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewardFunction \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewardFunction):\n",
      "\u001b[1;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "import anthropic\n",
    "import gymnasium as gym\n",
    "# from cartPoleShared import *\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import base64\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "# API configuration\n",
    "apiKey = \"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\"\n",
    "modelName = \"claude-3-5-sonnet-20240620\"\n",
    "\n",
    "class RewardUpdateSystem:\n",
    "    def __init__(self, apiKey: str, modelName: str, maxHistoryLength: int = 100, target_component: int = 1):\n",
    "        self.apiKey = apiKey\n",
    "        self.modelName = modelName\n",
    "        self.rewardHistory = deque(maxlen=maxHistoryLength)\n",
    "        self.episodeCount = 0\n",
    "        self.lastUpdateEpisode = 0\n",
    "        self.target_component = target_component\n",
    "    \n",
    "    def generateRewardGraph(self):\n",
    "        \"\"\"Generate a base64 encoded string of the reward history plot.\"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(list(self.rewardHistory), label=f'Reward Component {self.target_component}')\n",
    "        plt.title(f'Component {self.target_component} Reward History')\n",
    "        plt.xlabel('Steps')\n",
    "        plt.ylabel('Reward Value')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        buffer = io.BytesIO()\n",
    "        plt.savefig(buffer, format='png')\n",
    "        buffer.seek(0)\n",
    "        image_png = buffer.getvalue()\n",
    "        buffer.close()\n",
    "        plt.close()\n",
    "        \n",
    "        return base64.b64encode(image_png).decode()\n",
    "\n",
    "    def createUpdatePrompt(self, currentFunction: str, graph: str):\n",
    "        return [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"Analyze this reward component function and its performance graph, then suggest specific modifications.\n",
    "                \n",
    "                Current Function:\n",
    "                {currentFunction}\n",
    "                \n",
    "                Performance Graph (base64 encoded):\n",
    "                {graph}\n",
    "                \n",
    "                Requirements:\n",
    "                1. The function MUST be named 'reward_function_{self.target_component}'\n",
    "                2. Focus on stability (pole angle and angular velocity)\n",
    "                3. Include detailed inline comments explaining each change\n",
    "                4. Compare old vs new values in comments\n",
    "                \n",
    "                Output only the modified function with detailed inline comments explaining changes.\"\"\"\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    def createCriticPrompt(self, proposedFunction: str):\n",
    "        return [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"Act as a reward function critic. Analyze this proposed reward component:\n",
    "\n",
    "                {proposedFunction}\n",
    "\n",
    "                Evaluate:\n",
    "                1. Is the function properly named 'reward_function_{self.target_component}'?\n",
    "                2. Are reward calculations mathematically sound?\n",
    "                3. Does it focus appropriately on its specific aspect?\n",
    "                4. Are there any potential issues?\n",
    "                \n",
    "                End your response with EXACTLY one line containing only \"Decision: Yes\" or \"Decision: No\".\"\"\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "    def recordReward(self, info: dict):\n",
    "        \"\"\"Record the specific component reward value.\"\"\"\n",
    "        if 'component_rewards' in info:\n",
    "            component_reward = info['component_rewards'].get(f'reward_function_{self.target_component}')\n",
    "            if component_reward is not None:\n",
    "                self.rewardHistory.append(component_reward)\n",
    "        \n",
    "    def validateAndUpdate(self, currentFunction: str):\n",
    "        \"\"\"Generate and validate updated reward function with delays between API calls.\"\"\"\n",
    "        try:\n",
    "            # First API call for function update\n",
    "            print(f\"\\nGenerating new reward function for component {self.target_component}...\")\n",
    "            graph = self.generateRewardGraph()\n",
    "            updatePrompt = self.createUpdatePrompt(currentFunction, graph)\n",
    "            proposedFunction = queryAnthropicApi(self.apiKey, self.modelName, updatePrompt)\n",
    "            \n",
    "            print(\"\\nProposed Function:\")\n",
    "            print(proposedFunction)\n",
    "            \n",
    "            print(\"\\nWaiting 10 seconds before critic evaluation...\")\n",
    "            time.sleep(10)\n",
    "            \n",
    "            print(\"\\nGetting critic's evaluation...\")\n",
    "            criticPrompt = self.createCriticPrompt(proposedFunction)\n",
    "            criticResponse = queryAnthropicApi(self.apiKey, self.modelName, criticPrompt)\n",
    "            \n",
    "            print(\"\\nCritic Response:\")\n",
    "            print(criticResponse)\n",
    "            \n",
    "            approved = criticResponse.strip().endswith(\"Decision: Yes\")\n",
    "            print(f\"\\nCritic Decision: {'Approved' if approved else 'Rejected'}\")\n",
    "            \n",
    "            if approved:\n",
    "                return proposedFunction, True\n",
    "            return currentFunction, False\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError during update: {e}\")\n",
    "            return currentFunction, False\n",
    "\n",
    "    def waitingTime(self, episode: int):\n",
    "        \"\"\"Determine if it's time to update the reward function.\"\"\"\n",
    "        UPDATE_INTERVAL = 100\n",
    "        return episode - self.lastUpdateEpisode >= UPDATE_INTERVAL\n",
    "\n",
    "def main():\n",
    "    # Initialize with 3 reward components\n",
    "    env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "    env = CustomCartPoleEnv(env, num_components=3)\n",
    "    \n",
    "    # Initialize the update system targeting first component\n",
    "    updateSystem = RewardUpdateSystem(apiKey=apiKey, modelName=modelName, target_component=1)\n",
    "    \n",
    "    # Initialize agent\n",
    "    stateSize = env.observation_space.shape[0]\n",
    "    actionSize = env.action_space.n\n",
    "    agent = DQLearningAgent(env, stateSize, actionSize, device)\n",
    "    \n",
    "    # Training loop\n",
    "    numEpisodes = 500\n",
    "    for episode in range(numEpisodes):\n",
    "        observation = env.reset()[0]\n",
    "        totalReward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.chooseAction(observation)\n",
    "            nextObservation, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            totalReward += reward\n",
    "            \n",
    "            # Record the specific component reward\n",
    "            updateSystem.recordReward(info)\n",
    "            \n",
    "            agent.remember(observation, action, reward, nextObservation, done)\n",
    "            observation = nextObservation\n",
    "        \n",
    "        if updateSystem.waitingTime(episode):\n",
    "            print(f\"\\nAttempting reward function update at episode {episode}\")\n",
    "            func_name = f'reward_function_{updateSystem.target_component}'\n",
    "            current_func = env.reward_components[func_name]\n",
    "            newFunction, updated = updateSystem.validateAndUpdate(current_func)\n",
    "            \n",
    "            if updated:\n",
    "                print(f\"\\nUpdating reward component {updateSystem.target_component}...\")\n",
    "                env.set_component_reward(updateSystem.target_component, newFunction)\n",
    "                updateSystem.lastUpdateEpisode = episode\n",
    "            else:\n",
    "                print(\"\\nKeeping current reward function.\")\n",
    "        \n",
    "        agent.replay(32)\n",
    "        print(f\"Episode {episode + 1}/{numEpisodes}, Total Reward: {totalReward}\")\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gymenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
