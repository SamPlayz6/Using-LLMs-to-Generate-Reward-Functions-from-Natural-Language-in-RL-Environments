{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **This is a notebook to give proof of concept of Claude being able to explain the reward functions generated.**\n",
    "\n",
    "**H0: Claude can explain how its generated reward functions work and also explain the changes it makes to reward functions based of information in its context.**\n",
    "\n",
    "\n",
    "Answer: Yes\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Composite Reward Function with Focused Components:\n",
    "\n",
    "```python\n",
    "def stabilityReward(observation, action):\n",
    "    x, xDot, angle, angleDot = observation\n",
    "    # Focus purely on angle stability\n",
    "    angle_stability = 1.0 - abs(angle) / 0.209  # 0.209 radians is about 12 degrees\n",
    "    angular_velocity_component = -abs(angleDot) / 10.0  # Penalize fast angle changes\n",
    "    return float(angle_stability + angular_velocity_component)\n",
    "```\n",
    "\n",
    "```python\n",
    "def energyEfficiencyReward(observation, action):\n",
    "    x, xDot, angle, angleDot = observation\n",
    "    # Focus purely on minimizing movement and energy use\n",
    "    cart_movement_penalty = -abs(xDot) / 5.0  # Penalize cart velocity\n",
    "    angular_movement_penalty = -abs(angleDot) / 5.0  # Penalize pole angular velocity\n",
    "    return float(1.0 + cart_movement_penalty + angular_movement_penalty)\n",
    "```\n",
    "\n",
    "```python\n",
    "def timeBasedReward(observation, action):\n",
    "    x, xDot, angle, angleDot = observation\n",
    "    # Simple time-based reward that encourages survival\n",
    "    base_reward = 1.0\n",
    "    # Add small penalties for extreme positions/angles to prevent gaming\n",
    "    if abs(angle) > 0.209 or abs(x) > 2.4:  # If about to fail\n",
    "        base_reward = 0.0\n",
    "    return float(base_reward)\n",
    "```\n",
    "\n",
    "--\n",
    "\n",
    "```python\n",
    "def dynamicRewardFunction(observation, action):\n",
    "    x, xDot, angle, angleDot = observation\n",
    "    \n",
    "    # Get individual rewards\n",
    "    stability = stabilityReward(observation, action)\n",
    "    efficiency = energyEfficiencyReward(observation, action)\n",
    "    timeReward = timeBasedReward(observation, action)\n",
    "    \n",
    "    # Combine rewards with equal weights\n",
    "    return (stability + efficiency + timeReward) / 3.0\n",
    "```\n",
    "\n",
    "==================================================\n",
    "\n",
    "Stability Reward Function Explanation:\n",
    "Certainly! This stability reward function is designed to encourage the cart-pole system to maintain a stable, upright position. Let's break down the components and explain how they contribute to the goal of stability:\n",
    "\n",
    "1. Input:\n",
    "   - The function takes two parameters: `observation` and `action`.\n",
    "   - `observation` is unpacked into four variables: `x`, `xDot`, `angle`, and `angleDot`.\n",
    "   - `x`: Position of the cart\n",
    "   - `xDot`: Velocity of the cart\n",
    "   - `angle`: Angle of the pole from vertical\n",
    "   - `angleDot`: Angular velocity of the pole\n",
    "\n",
    "2. Angle Stability Component:\n",
    "   ```python\n",
    "   angle_stability = 1.0 - abs(angle) / 0.209\n",
    "   ```\n",
    "   - This component focuses on keeping the pole as close to vertical as possible.\n",
    "   - `abs(angle)` gives the absolute value of the angle, ensuring symmetrical treatment for both positive and negative angles.\n",
    "   - 0.209 radians is approximately 12 degrees, which is often used as a termination condition in cart-pole problems.\n",
    "   - The subtraction from 1.0 inverts the scale, so smaller angles result in higher rewards.\n",
    "   - This component will be 1.0 when the pole is perfectly vertical (angle = 0) and approach 0 as the angle nears ±0.209 radians.\n",
    "\n",
    "3. Angular Velocity Component:\n",
    "   ```python\n",
    "   angular_velocity_component = -abs(angleDot) / 10.0\n",
    "   ```\n",
    "   - This component discourages rapid changes in the pole's angle.\n",
    "   - `abs(angleDot)` gives the absolute magnitude of the angular velocity.\n",
    "   - The negative sign ensures that faster angular velocities result in larger penalties (smaller rewards).\n",
    "   - Division by 10.0 scales this component to be in a similar range as the angle stability component.\n",
    "\n",
    "4. Combination:\n",
    "   ```python\n",
    "   return float(angle_stability + angular_velocity_component)\n",
    "   ```\n",
    "   - The function returns the sum of these two components.\n",
    "   - This combination encourages both a vertical pole position and minimal angular velocity.\n",
    "\n",
    "5. Focus on Stability:\n",
    "   - The function ignores the cart's position (`x`) and velocity (`xDot`), focusing solely on the pole's angle and angular velocity.\n",
    "   - This design prioritizes keeping the pole upright over the cart's position on the track.\n",
    "\n",
    "6. Why it works for stability:\n",
    "   - Highest rewards are given when the pole is vertical (angle near 0) and not moving quickly (low angular velocity).\n",
    "   - As the pole deviates from vertical or starts moving faster, the reward decreases.\n",
    "   - This incentivizes actions that keep the pole upright and still, which is the essence of stability in the cart-pole system.\n",
    "\n",
    "In summary, this reward function is tailored specifically for stability by rewarding a vertical pole position and penalizing both angle deviations and rapid angular movements. It disregards the cart's horizontal motion, emphasizing the pole's stability above all else.\n",
    "\n",
    "=================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Composite Reward Function with Focused Components:\n",
      "\n",
      "def stabilityReward(observation, action):\n",
      "    x, xDot, angle, angleDot = observation\n",
      "    # Focus purely on angle stability\n",
      "    angle_stability = 1.0 - abs(angle) / 0.209  # 0.209 radians is about 12 degrees\n",
      "    angular_velocity_component = -abs(angleDot) / 10.0  # Penalize fast angle changes\n",
      "    return float(angle_stability + angular_velocity_component)\n",
      "\n",
      "def energyEfficiencyReward(observation, action):\n",
      "    x, xDot, angle, angleDot = observation\n",
      "    # Focus purely on minimizing movement and energy use\n",
      "    cart_movement_penalty = -abs(xDot) / 5.0  # Penalize cart velocity\n",
      "    angular_movement_penalty = -abs(angleDot) / 5.0  # Penalize pole angular velocity\n",
      "    return float(1.0 + cart_movement_penalty + angular_movement_penalty)\n",
      "\n",
      "def timeBasedReward(observation, action):\n",
      "    x, xDot, angle, angleDot = observation\n",
      "    # Simple time-based reward that encourages survival\n",
      "    base_reward = 1.0\n",
      "    # Add small penalties for extreme positions/angles to prevent gaming\n",
      "    if abs(angle) > 0.209 or abs(x) > 2.4:  # If about to fail\n",
      "        base_reward = 0.0\n",
      "    return float(base_reward)\n",
      "\n",
      "def analyzeFailure(lastObservation):\n",
      "    x, xDot, angle, angleDot = lastObservation\n",
      "    \n",
      "    if abs(x) > 2.4:  # Position failure\n",
      "        return 'position'\n",
      "    elif abs(angle) > 0.209:  # Angle failure\n",
      "        return 'angle'\n",
      "    elif abs(xDot) > 3.0:  # Velocity failure\n",
      "        return 'velocity'\n",
      "    return 'timeout'  # Episode ended due to max steps\n",
      "\n",
      "def adjustWeightsAfterEpisode(weights, failureType):\n",
      "    weightChanges = {\n",
      "        'stability': 0.0,\n",
      "        'efficiency': 0.0,\n",
      "        'time': 0.0\n",
      "    }\n",
      "    \n",
      "    # Adjust based on failure type\n",
      "    if failureType == 'position':\n",
      "        weightChanges['stability'] += 0.1\n",
      "        weightChanges['efficiency'] -= 0.05\n",
      "        weightChanges['time'] -= 0.05\n",
      "    elif failureType == 'angle':\n",
      "        weightChanges['stability'] += 0.15\n",
      "        weightChanges['efficiency'] -= 0.1\n",
      "        weightChanges['time'] -= 0.05\n",
      "    elif failureType == 'velocity':\n",
      "        weightChanges['efficiency'] += 0.1\n",
      "        weightChanges['stability'] -= 0.05\n",
      "        weightChanges['time'] -= 0.05\n",
      "    elif failureType == 'timeout':  # Successful run\n",
      "        weightChanges['time'] += 0.1\n",
      "        weightChanges['efficiency'] += 0.05\n",
      "        weightChanges['stability'] -= 0.15\n",
      "    \n",
      "    # Apply changes with bounds\n",
      "    for key in weights:\n",
      "        weights[key] = max(0.1, min(0.8, weights[key] + weightChanges[key]))\n",
      "    \n",
      "    # Normalize\n",
      "    total = sum(weights.values())\n",
      "    for key in weights:\n",
      "        weights[key] /= total\n",
      "    \n",
      "    return weights\n",
      "\n",
      "def dynamicRewardFunction(observation, action):\n",
      "    x, xDot, angle, angleDot = observation\n",
      "    \n",
      "    # Initialize weights if not exist\n",
      "    if not hasattr(dynamicRewardFunction, 'weights'):\n",
      "        dynamicRewardFunction.weights = {\n",
      "            'stability': 0.33,\n",
      "            'efficiency': 0.33,\n",
      "            'time': 0.34\n",
      "        }\n",
      "        dynamicRewardFunction.lastObservation = None\n",
      "        dynamicRewardFunction.episodeEnded = False\n",
      "    \n",
      "    # Store last observation for failure analysis\n",
      "    dynamicRewardFunction.lastObservation = observation\n",
      "    \n",
      "    # Get individual rewards\n",
      "    stability = stabilityReward(observation, action)\n",
      "    efficiency = energyEfficiencyReward(observation, action)\n",
      "    timeReward = timeBasedReward(observation, action)\n",
      "    \n",
      "    # Combine rewards with current weights\n",
      "    return (stability * dynamicRewardFunction.weights['stability'] + \n",
      "            efficiency * dynamicRewardFunction.weights['efficiency'] + \n",
      "            timeReward * dynamicRewardFunction.weights['time'])\n",
      "\n",
      "def updateWeightsAfterEpisode():\n",
      "    if hasattr(dynamicRewardFunction, 'lastObservation'):\n",
      "        failureType = analyzeFailure(dynamicRewardFunction.lastObservation)\n",
      "        dynamicRewardFunction.weights = adjustWeightsAfterEpisode(\n",
      "            dynamicRewardFunction.weights, \n",
      "            failureType\n",
      "        )\n",
      "        # Optional: store weight history\n",
      "        if not hasattr(dynamicRewardFunction, 'weightHistory'):\n",
      "            dynamicRewardFunction.weightHistory = []\n",
      "        dynamicRewardFunction.weightHistory.append({\n",
      "            'weights': dynamicRewardFunction.weights.copy(),\n",
      "            'failureType': failureType\n",
      "        })\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Stability Reward Function Explanation:\n",
      "Certainly! This stability reward function is designed to encourage the cart-pole system to maintain a stable, upright position. Let's break down the components and explain how they contribute to the goal of stability:\n",
      "\n",
      "1. Input:\n",
      "   - The function takes two parameters: `observation` and `action`.\n",
      "   - The `observation` is unpacked into four variables: `x`, `xDot`, `angle`, and `angleDot`.\n",
      "   - `x`: Position of the cart\n",
      "   - `xDot`: Velocity of the cart\n",
      "   - `angle`: Angle of the pole from vertical\n",
      "   - `angleDot`: Angular velocity of the pole\n",
      "\n",
      "2. Angle Stability Component:\n",
      "   ```python\n",
      "   angle_stability = 1.0 - abs(angle) / 0.209\n",
      "   ```\n",
      "   - This component focuses on the angle of the pole.\n",
      "   - The absolute value of the angle is used to treat deviations in both directions equally.\n",
      "   - 0.209 radians is approximately 12 degrees, which is often used as the failure threshold in cart-pole problems.\n",
      "   - As the angle approaches 0 (vertical position), this component approaches 1 (maximum reward).\n",
      "   - As the angle approaches ±0.209 radians, this component approaches 0 (minimum reward).\n",
      "\n",
      "3. Angular Velocity Component:\n",
      "   ```python\n",
      "   angular_velocity_component = -abs(angleDot) / 10.0\n",
      "   ```\n",
      "   - This component focuses on the angular velocity of the pole.\n",
      "   - The negative sign means that faster angular velocities result in lower rewards.\n",
      "   - The absolute value is used to penalize both clockwise and counterclockwise rotations equally.\n",
      "   - Dividing by 10.0 scales this component to have less impact than the angle stability component.\n",
      "\n",
      "4. Combining Components:\n",
      "   ```python\n",
      "   return float(angle_stability + angular_velocity_component)\n",
      "   ```\n",
      "   - The two components are added together to produce the final reward.\n",
      "   - The `float()` function ensures the result is a floating-point number.\n",
      "\n",
      "Why this function focuses on stability:\n",
      "\n",
      "1. Angle-based reward: By rewarding smaller angles, the function encourages the pole to stay close to vertical.\n",
      "\n",
      "2. Angular velocity penalty: By penalizing higher angular velocities, the function discourages rapid swinging or oscillations of the pole.\n",
      "\n",
      "3. Ignoring cart position and velocity: By not including `x` and `xDot` in the calculation, the function focuses solely on the pole's stability, regardless of where the cart is on the track.\n",
      "\n",
      "4. Continuous reward: Instead of a binary reward (e.g., 1 for upright, 0 for fallen), this function provides a continuous range of values, allowing for more nuanced feedback during learning.\n",
      "\n",
      "5. Bounded reward: The reward is designed to stay within a reasonable range, typically between -1 and 1, which is often beneficial for learning algorithms.\n",
      "\n",
      "By focusing on these specific aspects of stability (angle and angular velocity), the reward function guides the learning algorithm towards solutions that keep the pole balanced and minimize oscillations, which are key objectives in the cart-pole problem.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Episode 1/100, Total Reward: 21.72293234710753\n",
      "Episode 2/100, Total Reward: 35.16896188172743\n",
      "Episode 3/100, Total Reward: 42.555530950869894\n",
      "Episode 4/100, Total Reward: 25.833867776093754\n",
      "Episode 5/100, Total Reward: 8.179279038548863\n",
      "Episode 6/100, Total Reward: 7.887970243839842\n",
      "Episode 7/100, Total Reward: 25.162594867399342\n",
      "Episode 8/100, Total Reward: 5.5049459196235\n",
      "Episode 9/100, Total Reward: 10.903631107213503\n",
      "Episode 10/100, Total Reward: 11.53045799092919\n",
      "Episode 11/100, Total Reward: 20.857187879000175\n",
      "Episode 12/100, Total Reward: 15.299505169434381\n",
      "Episode 13/100, Total Reward: 25.243176456305054\n",
      "Episode 14/100, Total Reward: 21.923158302647707\n",
      "Episode 15/100, Total Reward: 22.983382996797662\n",
      "Episode 16/100, Total Reward: 10.202607162911297\n",
      "Episode 17/100, Total Reward: 24.722223061773025\n",
      "Episode 18/100, Total Reward: 9.014733268837396\n",
      "Episode 19/100, Total Reward: 7.350449835078105\n",
      "Episode 20/100, Total Reward: 46.0472269219955\n",
      "Episode 21/100, Total Reward: 14.504245730090593\n",
      "Episode 22/100, Total Reward: 27.562803120945006\n",
      "Episode 23/100, Total Reward: 12.55229421336655\n",
      "Episode 24/100, Total Reward: 13.770899920471894\n",
      "Episode 25/100, Total Reward: 19.693061964842197\n",
      "Episode 26/100, Total Reward: 44.84236203151519\n",
      "Episode 27/100, Total Reward: 7.100684447862601\n",
      "Episode 28/100, Total Reward: 25.092155269390076\n",
      "Episode 29/100, Total Reward: 14.872520213923272\n",
      "Episode 30/100, Total Reward: 6.21104475784194\n",
      "Episode 31/100, Total Reward: 9.307075605986494\n",
      "Episode 32/100, Total Reward: 12.605431108125243\n",
      "Episode 33/100, Total Reward: 8.796950288985542\n",
      "Episode 34/100, Total Reward: 11.000178985457477\n",
      "Episode 35/100, Total Reward: 38.28371776655246\n",
      "Episode 36/100, Total Reward: 11.724219314642546\n",
      "Episode 37/100, Total Reward: 23.414023475987353\n",
      "Episode 38/100, Total Reward: 8.32628397613656\n",
      "Episode 39/100, Total Reward: 11.237053417118563\n",
      "Episode 40/100, Total Reward: 17.540797087389095\n",
      "Episode 41/100, Total Reward: 10.72200471291456\n",
      "Episode 42/100, Total Reward: 29.823176306306234\n",
      "Episode 43/100, Total Reward: 27.675191927129323\n",
      "Episode 44/100, Total Reward: 8.72778163508365\n",
      "Episode 45/100, Total Reward: 15.375470402071254\n",
      "Episode 46/100, Total Reward: 29.10673759546817\n",
      "Episode 47/100, Total Reward: 22.476406532904722\n",
      "Episode 48/100, Total Reward: 16.676168632214594\n",
      "Episode 49/100, Total Reward: 15.173893561154898\n",
      "Episode 50/100, Total Reward: 7.684358684133935\n",
      "Episode 51/100, Total Reward: 7.498585963737417\n",
      "Episode 52/100, Total Reward: 23.626336195812275\n",
      "Episode 53/100, Total Reward: 4.720760581145162\n",
      "Episode 54/100, Total Reward: 24.49848106781036\n",
      "Episode 55/100, Total Reward: 5.468187468307582\n",
      "Episode 56/100, Total Reward: 41.92415876135661\n",
      "Episode 57/100, Total Reward: 30.1529774506408\n",
      "Episode 58/100, Total Reward: 11.734929628157794\n",
      "Episode 59/100, Total Reward: 16.464397907483832\n",
      "Episode 60/100, Total Reward: 9.605653411412709\n",
      "Episode 61/100, Total Reward: 11.151401535533175\n",
      "Episode 62/100, Total Reward: 8.1142784121711\n",
      "Episode 63/100, Total Reward: 8.752052684849813\n",
      "Episode 64/100, Total Reward: 16.111520170300892\n",
      "Episode 65/100, Total Reward: 20.560506752641604\n",
      "Episode 66/100, Total Reward: 6.357638710921139\n",
      "Episode 67/100, Total Reward: 11.695792057450813\n",
      "Episode 68/100, Total Reward: 15.057472696230874\n",
      "Episode 69/100, Total Reward: 5.526891925548253\n",
      "Episode 70/100, Total Reward: 19.47216444158986\n",
      "Episode 71/100, Total Reward: 11.056159400923573\n",
      "Episode 72/100, Total Reward: 9.141833571790865\n",
      "Episode 73/100, Total Reward: 6.288607954290352\n",
      "Episode 74/100, Total Reward: 17.466696426418466\n",
      "Episode 75/100, Total Reward: 17.573568613866755\n",
      "Episode 76/100, Total Reward: 52.416368197055604\n",
      "Episode 77/100, Total Reward: 9.680693042711994\n",
      "Episode 78/100, Total Reward: 14.555620420351843\n",
      "Episode 79/100, Total Reward: 20.76695976426396\n",
      "Episode 80/100, Total Reward: 6.276488666145425\n",
      "Episode 81/100, Total Reward: 6.055767562475644\n",
      "Episode 82/100, Total Reward: 38.523572123161756\n",
      "Episode 83/100, Total Reward: 7.328832169780608\n",
      "Episode 84/100, Total Reward: 15.863950679293394\n",
      "Episode 85/100, Total Reward: 23.997622142974627\n",
      "Episode 86/100, Total Reward: 14.758470719078849\n",
      "Episode 87/100, Total Reward: 5.518308110312411\n",
      "Episode 88/100, Total Reward: 10.40491655169506\n",
      "Episode 89/100, Total Reward: 21.648697083810244\n",
      "Episode 90/100, Total Reward: 9.901020840352304\n",
      "Episode 91/100, Total Reward: 24.35762700344277\n",
      "Episode 92/100, Total Reward: 10.839525342622478\n",
      "Episode 93/100, Total Reward: 14.149125872485518\n",
      "Episode 94/100, Total Reward: 11.309634738059225\n",
      "Episode 95/100, Total Reward: 7.98853850250652\n",
      "Episode 96/100, Total Reward: 5.6665792450967585\n",
      "Episode 97/100, Total Reward: 22.411764827077622\n",
      "Episode 98/100, Total Reward: 18.81848014798898\n",
      "Episode 99/100, Total Reward: 14.249946305196648\n",
      "Episode 100/100, Total Reward: 16.876289135316387\n"
     ]
    }
   ],
   "source": [
    "# Generating a Dynmaically Updating Reward Function\n",
    "\n",
    "import anthropic\n",
    "import gymnasium as gym\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "current_dir = os.getcwd()  \n",
    "project_root = str(Path(current_dir).parent.parent)\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# from ExtraNotebooksCodeExamples.cartPoleShared import *\n",
    "\n",
    "# anthropicAPI\n",
    "from AdaptiveRewardFunctionLearning.Prompts.APIQuery import queryAnthropicApi\n",
    "\n",
    "\n",
    "# API configuration\n",
    "from AdaptiveRewardFunctionLearning.Prompts.prompts import apiKey, modelName, device\n",
    "\n",
    "# rewardCompositeGenerator\n",
    "from AdaptiveRewardFunctionLearning.RewardGeneration.rewardCodeGeneration import createCompositeCode\n",
    "\n",
    "def main():\n",
    "    # Get the composite code\n",
    "    compositeCode = createCompositeCode()\n",
    "    \n",
    "    print(\"Composite Reward Function with Focused Components:\")\n",
    "    print(compositeCode)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    \n",
    "\n",
    "    # criticPrompts\n",
    "    # Get explanation for stability reward\n",
    "    from AdaptiveRewardFunctionLearning.Prompts.criticPrompts import  stabilityExplanationMessage\n",
    "\n",
    "    \n",
    "    stabilityExplanation = queryAnthropicApi(apiKey, modelName, stabilityExplanationMessage)\n",
    "    print(\"Stability Reward Function Explanation:\")\n",
    "    print(stabilityExplanation)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "    # logClaudeCall\n",
    "    from AdaptiveRewardFunctionLearning.Prompts.APIQuery import logClaudeCall\n",
    "    logClaudeCall(\n",
    "        rewardPrompt=\"Composite Reward Function with Focused Components\",\n",
    "        rewardResponse=compositeCode,\n",
    "        explanationPrompt=\"Stability Reward Function Explanation\",\n",
    "        explanationResponse=stabilityExplanation,\n",
    "        logFile=\"../../logs/LLMExplainabilityPOC.jsonl\"\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Create namespace and execute all functions\n",
    "    namespace = {}\n",
    "    exec(compositeCode, globals(), namespace)\n",
    "    \n",
    "\n",
    "    # env\n",
    "    from RLEnvironment.env import CustomCartPoleEnv\n",
    "    # Set up environment\n",
    "    env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "    env = CustomCartPoleEnv(env)\n",
    "    \n",
    "    # Update reward function with the namespace\n",
    "    for name, func in namespace.items():\n",
    "        globals()[name] = func\n",
    "    \n",
    "    env.setRewardFunction(namespace['dynamicRewardFunction'])\n",
    "    \n",
    "    # Initialize and train agent\n",
    "    stateSize = env.observation_space.shape[0]\n",
    "    actionSize = env.action_space.n\n",
    "    \n",
    "    # env\n",
    "    from RLEnvironment.training import DQLearningAgent\n",
    "    agent = DQLearningAgent(env, stateSize, actionSize, device)\n",
    "    \n",
    "\n",
    "    # training\n",
    "    from RLEnvironment.training import trainDQLearning\n",
    "\n",
    "    # Training loop\n",
    "    numEpisodes = 100\n",
    "    agent, env, rewards = trainDQLearning(agent, env, numEpisodes=numEpisodes)\n",
    "\n",
    "    for episode in range(numEpisodes):\n",
    "        print(f\"Episode {episode + 1}/{numEpisodes}, Total Reward: {rewards[episode]}\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Is reward a good proxy for effectivness of the reward function?*\n",
    "\n",
    "### **Dynamically Updating Reward function**\n",
    "\n",
    "\n",
    "Attempting reward function update at episode 100\n",
    "\n",
    "Generating new reward function for component 1...\n",
    "\n",
    "Proposed Function:\n",
    "Here's a modified reward function with detailed inline comments explaining the changes:\n",
    "\n",
    "```python\n",
    "def reward_function_1(state):\n",
    "    \"\"\"\n",
    "    Custom reward function focusing on stability (pole angle and angular velocity).\n",
    "    \n",
    "    Args:\n",
    "    state (list): The current state [x, x_dot, theta, theta_dot]\n",
    "    \n",
    "    Returns:\n",
    "    float: The calculated reward\n",
    "    \"\"\"\n",
    "    # Unpack state variables\n",
    "    x, x_dot, theta, theta_dot = state\n",
    "    \n",
    "    # Define target values (center position and upright)\n",
    "    x_target, theta_target = 0.0, 0.0\n",
    "    \n",
    "    # Calculate errors\n",
    "    x_error = abs(x - x_target)\n",
    "    theta_error = abs(theta - theta_target)\n",
    "    \n",
    "    # Define weight factors (increased emphasis on angle and angular velocity)\n",
    "    w_x = 0.2  # Reduced from 1.0 to lower importance of cart position\n",
    "    w_x_dot = 0.1  # Reduced from 0.5 to lower importance of cart velocity\n",
    "    w_theta = 2.0  # Increased from 1.0 to emphasize pole angle\n",
    "    w_theta_dot = 1.5  # Increased from 0.5 to emphasize angular velocity\n",
    "\n",
    "    # Calculate component rewards (using gaussian-like functions for smoother transitions)\n",
    "    r_x = np.exp(-0.5 * (x_error / 0.5)**2)  # Widened gaussian (0.5 instead of 0.1) for more lenient position reward\n",
    "    r_x_dot = np.exp(-0.5 * (x_dot / 2.0)**2)  # Slightly widened gaussian for velocity\n",
    "    r_theta = np.exp(-0.5 * (theta_error / 0.2)**2)  # Narrowed gaussian (0.2 instead of 0.5) for stricter angle reward\n",
    "    r_theta_dot = np.exp(-0.5 * (theta_dot / 1.0)**2)  # Narrowed gaussian for stricter angular velocity reward\n",
    "\n",
    "    # Combine component rewards with weights\n",
    "    reward = w_x * r_x + w_x_dot * r_x_dot + w_theta * r_theta + w_theta_dot * r_theta_dot\n",
    "    \n",
    "    # Normalize reward to [0, 1] range\n",
    "    reward = reward / (w_x + w_x_dot + w_theta + w_theta_dot)\n",
    "    \n",
    "    return reward\n",
    "```\n",
    "\n",
    "Key changes and explanations:\n",
    "\n",
    "1. Increased emphasis on stability:\n",
    "   - Increased weight for theta (w_theta) from 1.0 to 2.0\n",
    "   - Increased weight for theta_dot (w_theta_dot) from 0.5 to 1.5\n",
    "   - Decreased weights for x and x_dot to shift focus to pole stability\n",
    "\n",
    "2. Modified gaussian functions for smoother rewards:\n",
    "   - Widened the gaussian for x (0.5 instead of 0.1) to be more lenient on cart position\n",
    "   - Narrowed the gaussian for theta (0.2 instead of 0.5) to be stricter on pole angle\n",
    "   - Adjusted gaussians for x_dot and theta_dot for better balance\n",
    "\n",
    "3. Normalized the final reward to ensure it stays in the [0, 1] range, making it easier to interpret and use in learning algorithms.\n",
    "\n",
    "These changes should result in a reward function that places higher importance on keeping the pole upright and stable, while being more forgiving of the cart's exact position. This should lead to improved stability in the pole balancing task.\n",
    "\n",
    "Waiting 10 seconds before critic evaluation...\n",
    "\n",
    "Getting critic's evaluation...\n",
    "\n",
    "Critic Response:\n",
    "1. The function is properly named 'reward_function_1', which is clear and distinct.\n",
    "\n",
    "2. The reward calculations appear mathematically sound. The use of Gaussian-like functions for smoother transitions is a good approach, and the normalization of the final reward to the [0, 1] range is appropriate.\n",
    "\n",
    "3. The function focuses appropriately on stability, particularly emphasizing the pole angle and angular velocity. This is achieved through increased weights for theta and theta_dot, and decreased weights for cart position and velocity.\n",
    "\n",
    "4. Potential issues:\n",
    "   - The function might be overly lenient on cart position, which could lead to the cart drifting too far from the center.\n",
    "   - The narrowed Gaussian for theta might make the task too difficult initially, potentially slowing down learning.\n",
    "   - The weights and Gaussian parameters are somewhat arbitrary and might need fine-tuning based on empirical results.\n",
    "   - There's no explicit penalty for failure states (e.g., pole falling past a certain angle), which might be beneficial.\n",
    "\n",
    "Overall, the function seems well-designed for its purpose, but may require some empirical testing and potential adjustments.\n",
    "\n",
    "Decision: Yes\n",
    "\n",
    "Critic Decision: Approved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempting reward function update at episode 100\n",
      "\n",
      "Generating new reward function for component 1...\n",
      "\n",
      "Proposed Function:\n",
      "Here is a modified version of the reward function with detailed inline comments:\n",
      "\n",
      "```python\n",
      "def rewardFunction1(observation, action):\n",
      "    _, _, angle, angleDot = observation\n",
      "    \n",
      "    # Calculate base reward using cosine of angle (unchanged)\n",
      "    base_reward = np.cos(angle)\n",
      "    \n",
      "    # Penalize large angular velocities more aggressively\n",
      "    velocity_penalty = 0.2 * abs(angleDot)  # Increased from 0.1 to 0.2\n",
      "    \n",
      "    # Encourage staying upright by rewarding small angles\n",
      "    upright_bonus = 1.0 - abs(angle) / np.pi  # New bonus term\n",
      "    \n",
      "    # Penalize extreme actions to encourage smoother control\n",
      "    action_penalty = 0.1 * abs(action[0])  # New penalty term\n",
      "    \n",
      "    # Combine components into final reward\n",
      "    reward = base_reward - velocity_penalty + upright_bonus - action_penalty\n",
      "    \n",
      "    # Clip reward to reasonable range\n",
      "    reward = np.clip(reward, -1.0, 1.0)\n",
      "    \n",
      "    return reward\n",
      "\n",
      "# Key changes:\n",
      "# 1. Increased velocity penalty to discourage wild swinging\n",
      "# 2. Added upright bonus to reward staying balanced\n",
      "# 3. Added action penalty to encourage smoother control\n",
      "# 4. Clipped final reward to prevent extreme values\n",
      "# These changes aim to optimize reward while also extending episode duration\n",
      "```\n",
      "\n",
      "Waiting 10 seconds before critic evaluation...\n",
      "\n",
      "Getting critic's evaluation...\n",
      "\n",
      "Critic Response:\n",
      "Let's evaluate this reward function based on the given criteria:\n",
      "\n",
      "1. Is the function properly named 'rewardFunction1'?\n",
      "Yes, the function is correctly named 'rewardFunction1'.\n",
      "\n",
      "2. Are reward calculations mathematically sound?\n",
      "The calculations appear to be mathematically sound. The base reward, penalties, and bonuses are all calculated using appropriate mathematical operations and are combined in a logical manner.\n",
      "\n",
      "3. Does it focus on both reward quality and episode duration?\n",
      "Yes, this function seems to balance both reward quality and episode duration. The base reward encourages the pole to stay upright, while the added penalties and bonuses aim to extend episode duration by discouraging extreme movements and actions.\n",
      "\n",
      "4. Are there any potential issues?\n",
      "While the function looks generally good, there are a few potential issues to consider:\n",
      "- The weights for different components (e.g., 0.2 for velocity_penalty, 0.1 for action_penalty) might need tuning based on empirical results.\n",
      "- The upright_bonus term might overshadow other components if not balanced properly.\n",
      "- The clipping at the end might hide important information about extremely good or bad states.\n",
      "\n",
      "Overall, this reward function seems well-designed and addresses both reward quality and episode duration. The modifications appear to be thoughtful and aimed at improving performance. However, as with any reward function, it would need to be tested and potentially fine-tuned in practice.\n",
      "\n",
      "Decision: Yes\n",
      "\n",
      "Critic Decision: Approved\n",
      "\n",
      "Updating reward component 1...\n",
      "\n",
      "Attempting reward function update at episode 200\n",
      "\n",
      "Generating new reward function for component 1...\n",
      "\n",
      "Proposed Function:\n",
      "Here's the modified reward function with detailed inline comments:\n",
      "\n",
      "```python\n",
      "def rewardFunction1(observation, action):\n",
      "    _, _, angle, angleDot = observation\n",
      "    \n",
      "    # Calculate base reward using cosine of angle (unchanged)\n",
      "    base_reward = np.cos(angle)\n",
      "    \n",
      "    # Penalize large angular velocities more aggressively\n",
      "    velocity_penalty = 0.2 * abs(angleDot)  # Increased from 0.1 to 0.2\n",
      "    \n",
      "    # Encourage staying upright by rewarding small angles\n",
      "    upright_bonus = 1.0 - abs(angle) / np.pi  # New bonus term\n",
      "    \n",
      "    # Penalize extreme actions to encourage smoother control\n",
      "    action_penalty = 0.1 * abs(action[0])  # New penalty term\n",
      "    \n",
      "    # Combine components into final reward\n",
      "    reward = base_reward - velocity_penalty + upright_bonus - action_penalty\n",
      "    \n",
      "    # Clip reward to reasonable range\n",
      "    reward = np.clip(reward, -1.0, 1.0)\n",
      "    \n",
      "    return reward\n",
      "\n",
      "# Key changes:\n",
      "# 1. Increased velocity penalty to discourage wild swinging\n",
      "# 2. Added upright bonus to reward staying balanced\n",
      "# 3. Added action penalty to encourage smoother control\n",
      "# 4. Clipped final reward to prevent extreme values\n",
      "# These changes aim to optimize reward while also extending episode duration\n",
      "```\n",
      "\n",
      "Waiting 10 seconds before critic evaluation...\n",
      "\n",
      "Getting critic's evaluation...\n",
      "\n",
      "Critic Response:\n",
      "1. The function is properly named 'rewardFunction1', which is correct.\n",
      "\n",
      "2. The reward calculations appear mathematically sound. The base reward using cosine of angle, velocity penalty, upright bonus, and action penalty are all calculated and combined in a logical manner.\n",
      "\n",
      "3. The function does focus on both reward quality and episode duration:\n",
      "   - It rewards staying upright (quality) through the base reward and upright bonus.\n",
      "   - It penalizes large angular velocities and extreme actions (duration) to prevent quick failures.\n",
      "   - The upright bonus also encourages longer episodes by rewarding balance.\n",
      "\n",
      "4. Potential issues:\n",
      "   - The weights (0.2, 1.0, 0.1) for different components are somewhat arbitrary and may need tuning.\n",
      "   - The action penalty assumes actions are in a certain range; it might need adjustment for different action spaces.\n",
      "   - The clipping to [-1.0, 1.0] might limit the reward range too much for some scenarios.\n",
      "   - There's no explicit consideration of forward movement, which might be important depending on the specific task.\n",
      "\n",
      "Overall, the function seems well-designed and addresses both reward quality and episode duration, with room for fine-tuning based on specific requirements.\n",
      "\n",
      "Decision: Yes\n",
      "\n",
      "Critic Decision: Approved\n",
      "\n",
      "Updating reward component 1...\n",
      "\n",
      "Attempting reward function update at episode 300\n",
      "\n",
      "Generating new reward function for component 1...\n",
      "\n",
      "Proposed Function:\n",
      "Here's a modified version of the reward function with detailed inline comments addressing the requirements:\n",
      "\n",
      "```python\n",
      "def rewardFunction1(observation, action):\n",
      "    _, _, angle, angleDot = observation\n",
      "    \n",
      "    # Calculate base reward using cosine of angle (unchanged)\n",
      "    base_reward = np.cos(angle)\n",
      "    \n",
      "    # Penalize large angular velocities more aggressively\n",
      "    # Old value: 0.2 * abs(angleDot)\n",
      "    # New value: 0.3 * abs(angleDot)\n",
      "    velocity_penalty = 0.3 * abs(angleDot)  # Increased to further discourage rapid swings\n",
      "    \n",
      "    # Encourage staying upright by rewarding small angles\n",
      "    # Old value: 1.0 - abs(angle) / np.pi\n",
      "    # New value: 1.5 * (1.0 - abs(angle) / np.pi)\n",
      "    upright_bonus = 1.5 * (1.0 - abs(angle) / np.pi)  # Increased weight to prioritize upright position\n",
      "    \n",
      "    # Penalize extreme actions to encourage smoother control\n",
      "    # Old value: 0.1 * abs(action[0])\n",
      "    # New value: 0.15 * abs(action[0])\n",
      "    action_penalty = 0.15 * abs(action[0])  # Slightly increased to promote even smoother actions\n",
      "    \n",
      "    # New: Reward for maintaining low angular velocity\n",
      "    stability_bonus = 0.2 * np.exp(-abs(angleDot))  # Exponential decay rewards lower angular velocities\n",
      "    \n",
      "    # Combine components into final reward\n",
      "    reward = base_reward - velocity_penalty + upright_bonus - action_penalty + stability_bonus\n",
      "    \n",
      "    # Clip reward to reasonable range (unchanged)\n",
      "    reward = np.clip(reward, -1.0, 1.0)\n",
      "    \n",
      "    # New: Small constant reward for survival to encourage longer episodes\n",
      "    survival_bonus = 0.01  # Small constant value to incentivize longer episodes\n",
      "    \n",
      "    return reward + survival_bonus\n",
      "\n",
      "# Key changes:\n",
      "# 1. Increased velocity penalty to further discourage wild swinging\n",
      "# 2. Increased weight of upright bonus to prioritize balanced position\n",
      "# 3. Slightly increased action penalty for smoother control\n",
      "# 4. Added stability bonus to reward low angular velocities\n",
      "# 5. Introduced small constant survival bonus to encourage longer episodes\n",
      "# These changes aim to optimize reward while also extending episode duration\n",
      "```\n",
      "\n",
      "Waiting 10 seconds before critic evaluation...\n",
      "\n",
      "Getting critic's evaluation...\n",
      "\n",
      "Critic Response:\n",
      "Let's evaluate the proposed reward function:\n",
      "\n",
      "1. The function is indeed properly named 'rewardFunction1', which satisfies the first requirement.\n",
      "\n",
      "2. The reward calculations appear to be mathematically sound. The components are well-defined and use appropriate mathematical operations (cosine, absolute value, exponential decay) that make sense in the context of a pendulum balancing task.\n",
      "\n",
      "3. The function does focus on both reward quality and episode duration:\n",
      "   - Reward quality is addressed through various components like base reward, upright bonus, and penalties for excessive movement and actions.\n",
      "   - Episode duration is encouraged through the new survival_bonus, which adds a small constant reward for each step the episode continues.\n",
      "\n",
      "4. Potential issues:\n",
      "   - The survival_bonus is added after the reward clipping, which means it could potentially push the total reward above the intended maximum of 1.0. It might be better to include this within the clipping operation.\n",
      "   - The stability_bonus using exponential decay might be quite small for typical angular velocities, potentially having less impact than intended.\n",
      "   - The combination of multiple reward components might make it challenging to balance their relative importance, potentially leading to unexpected behavior.\n",
      "\n",
      "Overall, the reward function appears to be a well-thought-out improvement over a previous version, addressing the stated goals of optimizing reward while extending episode duration. However, careful tuning and testing would be necessary to ensure the desired balance is achieved.\n",
      "\n",
      "Decision: Yes\n",
      "\n",
      "Critic Decision: Approved\n",
      "\n",
      "Updating reward component 1...\n",
      "\n",
      "Attempting reward function update at episode 400\n",
      "\n",
      "Generating new reward function for component 1...\n",
      "\n",
      "Proposed Function:\n",
      "Here's the modified reward function with detailed inline comments addressing the requirements:\n",
      "\n",
      "```python\n",
      "def rewardFunction1(observation, action):\n",
      "    _, _, angle, angleDot = observation\n",
      "    \n",
      "    # Calculate base reward using cosine of angle (unchanged)\n",
      "    base_reward = np.cos(angle)\n",
      "    \n",
      "    # Penalize large angular velocities more aggressively\n",
      "    # Old value: 0.2 * abs(angleDot)\n",
      "    # New value: 0.3 * abs(angleDot)\n",
      "    velocity_penalty = 0.3 * abs(angleDot)  # Increased to further discourage rapid swings\n",
      "    \n",
      "    # Encourage staying upright by rewarding small angles\n",
      "    # Old value: 1.0 - abs(angle) / np.pi\n",
      "    # New value: 1.5 * (1.0 - abs(angle) / np.pi)\n",
      "    upright_bonus = 1.5 * (1.0 - abs(angle) / np.pi)  # Increased weight to prioritize upright position\n",
      "    \n",
      "    # Penalize extreme actions to encourage smoother control\n",
      "    # Old value: 0.1 * abs(action[0])\n",
      "    # New value: 0.15 * abs(action[0])\n",
      "    action_penalty = 0.15 * abs(action[0])  # Slightly increased to promote even smoother actions\n",
      "    \n",
      "    # New: Reward for maintaining low angular velocity\n",
      "    stability_bonus = 0.2 * np.exp(-abs(angleDot))  # Exponential decay rewards lower angular velocities\n",
      "    \n",
      "    # Combine components into final reward\n",
      "    reward = base_reward - velocity_penalty + upright_bonus - action_penalty + stability_bonus\n",
      "    \n",
      "    # Clip reward to reasonable range (unchanged)\n",
      "    reward = np.clip(reward, -1.0, 1.0)\n",
      "    \n",
      "    # New: Small constant reward for survival to encourage longer episodes\n",
      "    survival_bonus = 0.01  # Small constant value to incentivize longer episodes\n",
      "    \n",
      "    return reward + survival_bonus\n",
      "\n",
      "# Key changes:\n",
      "# 1. Increased velocity penalty to further discourage wild swinging\n",
      "# 2. Increased weight of upright bonus to prioritize balanced position\n",
      "# 3. Slightly increased action penalty for smoother control\n",
      "# 4. Added stability bonus to reward low angular velocities\n",
      "# 5. Introduced small constant survival bonus to encourage longer episodes\n",
      "# These changes aim to optimize reward while also extending episode duration\n",
      "```\n",
      "\n",
      "Waiting 10 seconds before critic evaluation...\n",
      "\n",
      "Getting critic's evaluation...\n",
      "\n",
      "Critic Response:\n",
      "Let's analyze this reward function:\n",
      "\n",
      "1. The function is correctly named 'rewardFunction1', so that requirement is met.\n",
      "\n",
      "2. The reward calculations appear mathematically sound. The components are well-defined and use appropriate mathematical operations. The use of cosine, absolute values, and exponential decay are all valid approaches for the intended purposes.\n",
      "\n",
      "3. The function does focus on both reward quality and episode duration:\n",
      "   - Reward quality is addressed through various components like base reward, penalties for angular velocity and actions, and bonuses for upright position and stability.\n",
      "   - Episode duration is encouraged through the new survival_bonus, which adds a small constant reward for each step the episode continues.\n",
      "\n",
      "4. Potential issues:\n",
      "   - The relative weights of the components might need fine-tuning through experimentation.\n",
      "   - The survival_bonus, while small, could potentially lead to the agent learning to exploit it by finding ways to survive without actually improving performance.\n",
      "   - The reward clipping happens before adding the survival_bonus, which means the total reward could exceed the intended [-1.0, 1.0] range.\n",
      "\n",
      "Overall, this reward function appears to be a well-thought-out improvement over the previous version, addressing the stated requirements and introducing new components to optimize performance and episode duration.\n",
      "\n",
      "Decision: Yes\n",
      "\n",
      "Critic Decision: Approved\n",
      "\n",
      "Updating reward component 1...\n"
     ]
    }
   ],
   "source": [
    "# Actively Changing the Composite Reward Function in the agent\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "def onEpisodeEnd(env, updateSystem, episode):\n",
    "    if updateSystem.waitingTime(episode):\n",
    "        print(f\"\\nAttempting reward function update at episode {episode}\")\n",
    "        funcName = f'rewardFunction{updateSystem.targetComponent}'\n",
    "        currentFunc = env.rewardComponents[funcName]\n",
    "        newFunction, updated = updateSystem.validateAndUpdate(currentFunc)\n",
    "        \n",
    "        if updated:\n",
    "            print(f\"\\nUpdating reward component {updateSystem.targetComponent}...\")\n",
    "            env.setComponentReward(updateSystem.targetComponent, newFunction)\n",
    "            updateSystem.lastUpdateEpisode = episode\n",
    "        else:\n",
    "            print(\"\\nKeeping current reward function.\")\n",
    "\n",
    "def main():\n",
    "    # Initialize with 3 reward components\n",
    "    env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "    # CustomCartPoleEnv\n",
    "    from RLEnvironment.env import CustomCartPoleEnv\n",
    "    env = CustomCartPoleEnv(env, numComponents=3)\n",
    "    \n",
    "    initialRewards = [\n",
    "        # Stability reward\n",
    "        \"\"\"def rewardFunction1(observation, action):\n",
    "            _, _, angle, angleDot = observation\n",
    "            return np.cos(angle) - 0.1 * abs(angleDot)\"\"\",\n",
    "        \n",
    "        # Position reward\n",
    "        \"\"\"def rewardFunction2(observation, action):\n",
    "            x, xDot, _, _ = observation\n",
    "            return -abs(x) - 0.1 * abs(xDot)\"\"\",\n",
    "        \n",
    "        # Energy efficiency reward\n",
    "        \"\"\"def rewardFunction3(observation, action):\n",
    "            _, xDot, _, angleDot = observation\n",
    "            return -0.1 * (abs(xDot) + abs(angleDot))\"\"\"\n",
    "    ]\n",
    "    \n",
    "    # Set initial reward functions\n",
    "    for i, rewardFunc in enumerate(initialRewards, 1):\n",
    "        env.setComponentReward(i, rewardFunc)\n",
    "    \n",
    "    # RewardUpdateSystem\n",
    "    from AdaptiveRewardFunctionLearning.RewardGeneration.rewardCritic import RewardUpdateSystem\n",
    "\n",
    "    updateSystem = RewardUpdateSystem(\n",
    "        apiKey=apiKey, \n",
    "        modelName=modelName, \n",
    "        targetComponent=1 \n",
    "    )\n",
    "    \n",
    "    # Initialize agent\n",
    "    stateSize = env.observation_space.shape[0]\n",
    "    actionSize = env.action_space.n\n",
    "\n",
    "    #DQLearningAgent\n",
    "    from RLEnvironment.training import DQLearningAgent\n",
    "\n",
    "    agent = DQLearningAgent(env, stateSize, actionSize, device)\n",
    "    \n",
    "    # Training loop with reward updates\n",
    "    numEpisodes = 500\n",
    "    \n",
    "    #trainDQLearning\n",
    "    from RLEnvironment.training import trainDQLearning\n",
    "    agent, env, rewards = trainDQLearning(\n",
    "        agent=agent,\n",
    "        env=env,\n",
    "        numEpisodes=numEpisodes,\n",
    "        updateSystem=updateSystem,\n",
    "        onEpisodeEnd=onEpisodeEnd\n",
    "    )\n",
    "\n",
    "    env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
