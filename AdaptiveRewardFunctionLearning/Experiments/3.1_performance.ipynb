{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Performance Analysis for Adaptive Reward Functions\n",
    "\n",
    "This notebook extends the basic performance experiment with more sophisticated analyses:\n",
    "\n",
    "1. **Statistical significance testing** between reward approaches\n",
    "2. **Adaptation speed metrics** showing recovery time after environmental changes\n",
    "3. **Performance breakdown** showing when and why adaptive approaches excel \n",
    "4. **Component analysis** demonstrating how the adaptive mechanism works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in /home/sd37/.local/lib/python3.10/site-packages (4.1.0)\n",
      "Requirement already satisfied: scipy in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (1.15.2)\n",
      "Requirement already satisfied: statsmodels in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (0.14.4)\n",
      "Requirement already satisfied: tqdm in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (4.66.5)\n",
      "Requirement already satisfied: scikit-learn in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (1.6.1)\n",
      "Requirement already satisfied: matplotlib in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (3.9.2)\n",
      "Requirement already satisfied: seaborn in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (0.13.2)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /home/sd37/.local/lib/python3.10/site-packages (from optuna) (1.14.0)\n",
      "Requirement already satisfied: colorlog in /home/sd37/.local/lib/python3.10/site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: numpy in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from optuna) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from optuna) (24.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from optuna) (2.0.37)\n",
      "Requirement already satisfied: PyYAML in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from optuna) (6.0)\n",
      "Requirement already satisfied: pandas!=2.1.0,>=1.4 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from statsmodels) (2.2.3)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from statsmodels) (1.0.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: Mako in /home/sd37/.local/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (1.3.8)\n",
      "Requirement already satisfied: typing-extensions>=4 in /home/sd37/.local/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install optuna scipy statsmodels tqdm scikit-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sd37/.conda/envs/thesis/bin/python\n",
      "4.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sd37/.conda/envs/thesis/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SciPy version: 1.15.2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "import optuna\n",
    "print(optuna.__version__)\n",
    "\n",
    "import scipy\n",
    "import scipy.stats as stats\n",
    "print(f\"SciPy version: {scipy.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safely_get_function_source(func):\n",
    "    \"\"\"\n",
    "    Safely get the source code of a function, handling dynamically generated functions.\n",
    "    Returns a string representation if source code cannot be retrieved.\n",
    "    \"\"\"\n",
    "    # Handle the case when func is already a string\n",
    "    if isinstance(func, str):\n",
    "        return func\n",
    "        \n",
    "    try:\n",
    "        return inspect.getsource(func)\n",
    "    except (TypeError, OSError, IOError):\n",
    "        # For dynamically generated functions, create a representation\n",
    "        if hasattr(func, '__name__'):\n",
    "            return f\"<Dynamically generated function: {func.__name__}>\"\n",
    "        return f\"<Dynamically generated function: {func}>\"\n",
    "        \n",
    "def function_to_string(func):\n",
    "    \"\"\"\n",
    "    Convert a function to a string representation, suitable for display and comparison.\n",
    "    Handles various function types including lambdas, dynamically created functions, and strings.\n",
    "    \"\"\"\n",
    "    if func is None:\n",
    "        return \"None\"\n",
    "    \n",
    "    # Handle the case when func is already a string\n",
    "    if isinstance(func, str):\n",
    "        return func\n",
    "        \n",
    "    # Try to get the source code\n",
    "    try:\n",
    "        source = inspect.getsource(func)\n",
    "        return source.strip()\n",
    "    except (TypeError, OSError, IOError):\n",
    "        # If we can't get the source, create a detailed representation\n",
    "        if hasattr(func, '__name__'):\n",
    "            func_name = func.__name__\n",
    "        else:\n",
    "            func_name = str(func)\n",
    "            \n",
    "        # Get signature if possible\n",
    "        try:\n",
    "            signature = str(inspect.signature(func))\n",
    "        except (TypeError, ValueError):\n",
    "            signature = \"(unknown signature)\"\n",
    "            \n",
    "        # For simple functions, we can create a representation of their logic\n",
    "        try:\n",
    "            # Try to get the code object\n",
    "            code = func.__code__\n",
    "            # Include variable names and constants from the function\n",
    "            varnames = code.co_varnames[:code.co_argcount]\n",
    "            constants = code.co_consts\n",
    "            \n",
    "            return f\"Function {func_name}{signature} with {len(varnames)} arguments, {len(constants)} constants\"\n",
    "        except AttributeError:\n",
    "            # Fallback for non-Python functions or built-ins\n",
    "            return f\"Function {func_name}{signature} (source unavailable)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import scipy.stats as stats\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "from statsmodels.formula.api import ols\n",
    "from tqdm.notebook import tqdm\n",
    "import inspect\n",
    "import json\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"viridis\")\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "\n",
    "current_dir = os.getcwd()  \n",
    "project_root = str(Path(current_dir).parent.parent)\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# Initialize environment and device\n",
    "from AdaptiveRewardFunctionLearning.Prompts.prompts import device, apiKey, modelName\n",
    "\n",
    "# Custom CartPole Environment\n",
    "from RLEnvironment.env import CustomCartPoleEnv\n",
    "# Reward Update System\n",
    "from AdaptiveRewardFunctionLearning.RewardGeneration.rewardCritic import RewardUpdateSystem\n",
    "# DQ Learning Agent\n",
    "from RLEnvironment.training.agent import DQLearningAgent\n",
    "from RLEnvironment.training.training import trainDQLearning\n",
    "\n",
    "# Dynamic Reward Function\n",
    "from AdaptiveRewardFunctionLearning.RewardGeneration.rewardCodeGeneration import stabilityReward, efficiencyReward, dynamicRewardFunction\n",
    "\n",
    "# Visualization functions\n",
    "from AdaptiveRewardFunctionLearning.Visualisation.trainingTestFunctions import (\n",
    "    runEpisode,\n",
    "    detectJumps,\n",
    "    analyzeRewardSensibility,\n",
    "    performUpdate,\n",
    "    updateCompositeRewardFunction,\n",
    "    plotExperimentResults,\n",
    "    savePlot\n",
    ")\n",
    "\n",
    "# Import other reward functions\n",
    "from AdaptiveRewardFunctionLearning.RewardGeneration.cartpole_energy_reward import EnergyBasedRewardFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_reward = EnergyBasedRewardFunction(mass_cart=1.0, mass_pole=0.1, length=0.5, gravity=9.8)\n",
    "\n",
    "def energyBasedReward(observation, action):\n",
    "    \"\"\"Enhanced physics-based energy reward\"\"\"\n",
    "    return float(energy_reward.compute_reward(observation, action))\n",
    "\n",
    "\n",
    "def potentialBasedReward(observation, action):\n",
    "    \"\"\"Potential-based reward shaping for CartPole  - This one is not dynamic\"\"\"\n",
    "    x, x_dot, theta, theta_dot = observation\n",
    "    gamma = 0.99\n",
    "    \n",
    "    def potential(state):\n",
    "        # Potential function based on cart position and pole angle\n",
    "        # Higher potential for centered cart and upright pole\n",
    "        cart_potential = -(state[0] ** 2)  # Penalize distance from center\n",
    "        angle_potential = -((state[2] ** 2))  # Penalize angle from vertical\n",
    "        velocity_potential = -(state[1] ** 2)  # Penalize high velocities\n",
    "        ang_velocity_potential = -(state[3] ** 2)  # Penalize high angular velocities\n",
    "        \n",
    "        return cart_potential + 2*angle_potential + velocity_potential + ang_velocity_potential\n",
    "\n",
    "    current_potential = potential(observation)\n",
    "    next_potential = potential([x + x_dot, x_dot, theta + theta_dot, theta_dot])\n",
    "    \n",
    "    # PBRS formula: γΦ(s') - Φ(s)\n",
    "    shaped_reward = gamma * next_potential - current_potential\n",
    "    \n",
    "    return 1.0 + shaped_reward\n",
    "\n",
    "\n",
    "def baselineReward(observation, action):\n",
    "    \"\"\"Standard baseline reward\"\"\"\n",
    "    return 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Performance Testing Framework\n",
    "\n",
    "This extended framework adds detailed data collection for adaptation analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runEnhancedPerformanceTest(\n",
    "    episodes=1000, \n",
    "    changeInterval=500, \n",
    "    lengthchanges=[0.5, 1.5],\n",
    "    mass_cart=1.0,\n",
    "    mass_pole=0.1,\n",
    "    initial_length=0.5,\n",
    "    gravity=9.8,\n",
    "    seed=42,\n",
    "    collect_component_data=True  # Toggle collection of component weights\n",
    "):\n",
    "    \"\"\"Enhanced performance test with detailed adaptation metrics\"\"\"\n",
    "    print(f\"Starting Enhanced Performance Test with seed {seed}...\")\n",
    "    \n",
    "    # Set all random seeds\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    def create_fresh_env():\n",
    "        # Create fresh environment with consistent settings\n",
    "        env = gym.make('CartPole-v1', max_episode_steps=20000, render_mode=None)\n",
    "        env.action_space.seed(seed)\n",
    "        env.observation_space.seed(seed)\n",
    "        env.reset(seed=seed)\n",
    "        env = CustomCartPoleEnv(env, numComponents=2)\n",
    "        env.setEnvironmentParameters(masscart=mass_cart, length=lengthchanges[0], gravity=gravity)\n",
    "        return env\n",
    "    \n",
    "    def create_dqn_agent(env, device):\n",
    "        return DQLearningAgent(\n",
    "            env=env, \n",
    "            stateSize=4, \n",
    "            actionSize=2, \n",
    "            device=device,\n",
    "            learningRate=0.0007,       # Balanced for adaptation and stability\n",
    "            discountFactor=0.99,       # Standard value works well\n",
    "            epsilon=1.0,\n",
    "            epsilonDecay=0.9997,       # Slower decay preserves exploration\n",
    "            epsilonMin=0.07,           # Higher minimum exploration rate\n",
    "            replayBufferSize=30000,    # Smaller to adapt faster to changes\n",
    "            batchSize=48,              # Moderate batch size\n",
    "            targetUpdateFreq=125       # More frequent updates\n",
    "        )\n",
    "    \n",
    "    # Initialize energy-based reward function\n",
    "    energy_reward = EnergyBasedRewardFunction(\n",
    "        mass_cart=mass_cart, \n",
    "        mass_pole=mass_pole, \n",
    "        length=initial_length, \n",
    "        gravity=gravity\n",
    "    )\n",
    "    \n",
    "    # Create initial environment\n",
    "    env = create_fresh_env()\n",
    "    \n",
    "    # Define reward function configurations\n",
    "    rewardfunctions = {\n",
    "        'adaptivereward': {\n",
    "            'agent': None,  # Will be created fresh for each test\n",
    "            'updatesystem': RewardUpdateSystem(apiKey, modelName),\n",
    "            'rewardfunction': None,\n",
    "            'update_method': 'llm'\n",
    "        },\n",
    "        'pbrs': {\n",
    "            'agent': None,\n",
    "            'updatesystem': None,\n",
    "            'rewardfunction': potentialBasedReward,\n",
    "            'update_method': None\n",
    "        },\n",
    "        'energy_based': {\n",
    "            'agent': None,\n",
    "            'updatesystem': energy_reward,\n",
    "            'rewardfunction': energy_reward.compute_reward,\n",
    "            'update_method': 'physics'\n",
    "        },\n",
    "        'baseline': {\n",
    "            'agent': None,\n",
    "            'updatesystem': None,\n",
    "            'rewardfunction': baselineReward,\n",
    "            'update_method': None\n",
    "        }\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "    test_order = ['adaptivereward', 'energy_based', 'baseline', 'pbrs']\n",
    "    \n",
    "    # Test each reward function\n",
    "    for rewardname in test_order:\n",
    "        print(f\"\\nTesting reward function: {rewardname}\")\n",
    "        \n",
    "        # Create fresh environment and agent\n",
    "        env = create_fresh_env()\n",
    "        rewardfunctions[rewardname]['agent'] = create_dqn_agent(env, device)\n",
    "        rewardinfo = rewardfunctions[rewardname]\n",
    "        \n",
    "        # Reset variables for this run\n",
    "        currentlengthidx = 0\n",
    "        \n",
    "        # Set up the reward function\n",
    "        if rewardname == 'adaptivereward':\n",
    "            # Initialize both components for adaptive reward\n",
    "            env.setComponentReward(1, stabilityReward)\n",
    "            env.setComponentReward(2, efficiencyReward)\n",
    "            rewardinfo['updatesystem'].lastUpdateEpisode = 0\n",
    "        else:\n",
    "            env.setRewardFunction(rewardinfo['rewardfunction'])\n",
    "        \n",
    "        # Storage for episode data\n",
    "        episoderewards = []\n",
    "        episodebalancetimes = []\n",
    "        rewardchangeepisodes = []\n",
    "        environmentchanges = []\n",
    "        \n",
    "        # Enhanced metrics collection\n",
    "        adaptation_metrics = {\n",
    "            'pre_change_performance': [],  # Performance before env change\n",
    "            'post_change_performance': [],  # Performance right after env change\n",
    "            'recovery_times': [],           # Episodes to recover after change\n",
    "            'performance_drop': [],         # Performance drop percentage\n",
    "            'change_episodes': []           # Episode numbers where changes occurred\n",
    "        }\n",
    "        \n",
    "        # Component weight tracking for adaptive reward only\n",
    "        component_weights = [] if rewardname == 'adaptivereward' and collect_component_data else None\n",
    "        component_updates = [] if rewardname == 'adaptivereward' and collect_component_data else None\n",
    "        \n",
    "        # Function to calculate rolling metrics\n",
    "        def calculate_rolling_metrics(episode_data, window=20):\n",
    "            if len(episode_data) < window:\n",
    "                return np.mean(episode_data) if episode_data else 0\n",
    "            \n",
    "            return np.mean(episode_data[-window:])\n",
    "        \n",
    "        def onEpisodeEnd(env, updatesystem, episode, reward, steps):\n",
    "            nonlocal episoderewards, episodebalancetimes, rewardchangeepisodes, environmentchanges\n",
    "            nonlocal currentlengthidx, adaptation_metrics, component_weights, component_updates\n",
    "            \n",
    "            # Record basic metrics\n",
    "            episoderewards.append(reward)\n",
    "            episodebalancetimes.append(steps)\n",
    "            \n",
    "            # Calculate rolling metrics for decision making\n",
    "            metrics = {\n",
    "                'currentEpisode': episode,\n",
    "                'recentRewards': episoderewards[-100:] if len(episoderewards) > 100 else episoderewards,\n",
    "                'averageBalanceTime': np.mean(episodebalancetimes[-100:]) if episodebalancetimes else 0,\n",
    "                'balanceTimeVariance': np.var(episodebalancetimes[-100:]) if len(episodebalancetimes) > 1 else 0\n",
    "            }\n",
    "            \n",
    "            # Collect component weights for adaptive reward\n",
    "            if rewardname == 'adaptivereward' and collect_component_data and hasattr(env, 'getCurrentWeights'):\n",
    "                weights = env.getCurrentWeights()\n",
    "                component_weights.append({\n",
    "                    'episode': episode,\n",
    "                    'stability': weights['stability'],\n",
    "                    'efficiency': weights['efficiency']\n",
    "                })\n",
    "            \n",
    "            # Print debug info periodically\n",
    "            if episode % 1000 == 0:\n",
    "                print(f\"\\nMetrics at Episode {episode}:\")\n",
    "                print(f\"Recent Average Reward: {np.mean(metrics['recentRewards']):.2f}\")\n",
    "                print(f\"Average Balance Time: {metrics['averageBalanceTime']:.2f}\")\n",
    "                \n",
    "                if rewardname == 'adaptivereward' and hasattr(env, 'getCurrentWeights'):\n",
    "                    weights = env.getCurrentWeights()\n",
    "                    print(f\"Component Weights - Stability: {weights['stability']:.2f}, \"\n",
    "                          f\"Efficiency: {weights['efficiency']:.2f}\")\n",
    "            \n",
    "            # Handle LLM updates for adaptive reward\n",
    "            if rewardname == 'adaptivereward' and updatesystem is not None:\n",
    "                for component in range(1, 3):\n",
    "                    updatesystem.targetComponent = component\n",
    "                    if updatesystem.waitingTime(f'component_{component}', metrics, updatesystem.lastUpdateEpisode):\n",
    "                        current_func = env.rewardComponents[f'rewardFunction{component}']\n",
    "                        new_function, updated = updatesystem.validateAndUpdate(current_func)\n",
    "                        \n",
    "                        if updated:\n",
    "                            # Fix the error: First check if new_function is a string\n",
    "                            if isinstance(new_function, str):\n",
    "                                # If it's a string, store it directly\n",
    "                                old_func_str = str(current_func)\n",
    "                                new_func_str = new_function\n",
    "                                \n",
    "                                # Show warning but continue\n",
    "                                print(\"WARNING: New function returned as string, not function object\")\n",
    "                                continue  # Skip applying the update\n",
    "                            else:\n",
    "                                # Safe function representation for non-string functions\n",
    "                                old_func_str = str(current_func) \n",
    "                                new_func_str = str(new_function)\n",
    "                            \n",
    "                            # Apply the update\n",
    "                            env.setComponentReward(component, new_function)\n",
    "                            rewardchangeepisodes.append(episode)\n",
    "                            updatesystem.lastUpdateEpisode = episode\n",
    "                            \n",
    "                            # Record the update details\n",
    "                            if component_updates is not None:\n",
    "                                component_updates.append({\n",
    "                                    'episode': episode,\n",
    "                                    'component': component,\n",
    "                                    'old_function': old_func_str,\n",
    "                                    'new_function': new_func_str,\n",
    "                                    'pre_update_performance': calculate_rolling_metrics(episoderewards[-20:]) if len(episoderewards) >= 20 else 0\n",
    "                                })\n",
    "                            \n",
    "                            print(f\"✓ LLM update for component {component} at episode {episode}\")\n",
    "            \n",
    "            # Handle physics-based updates\n",
    "            elif rewardinfo['update_method'] == 'physics':\n",
    "                if episode % changeInterval == 0 and episode > 0:\n",
    "                    updatesystem.length = lengthchanges[currentlengthidx]\n",
    "                    env.setRewardFunction(updatesystem.compute_reward)\n",
    "                    rewardchangeepisodes.append(episode)\n",
    "                    print(f\"✓ Physics-based update at episode {episode}\")\n",
    "            \n",
    "            # Environment changes for all approaches\n",
    "            if episode % changeInterval == 0 and episode > 0:\n",
    "                # Calculate pre-change performance (last 20 episodes)\n",
    "                pre_change_perf = calculate_rolling_metrics(episoderewards[-20:])\n",
    "                \n",
    "                # Apply the environment change\n",
    "                currentlengthidx = (currentlengthidx + 1) % len(lengthchanges)\n",
    "                newlength = lengthchanges[currentlengthidx]\n",
    "                env.setEnvironmentParameters(length=newlength)\n",
    "                \n",
    "                # Record that a change happened\n",
    "                environmentchanges.append(episode)\n",
    "                \n",
    "                # Start tracking adaptation metrics for this change\n",
    "                adaptation_metrics['pre_change_performance'].append(pre_change_perf)\n",
    "                adaptation_metrics['change_episodes'].append(episode)\n",
    "                print(f\"\\nChanged pole length to: {newlength}m at episode {episode}\")\n",
    "                \n",
    "            # Track post-change performance and recovery\n",
    "            if environmentchanges and (episode - environmentchanges[-1]) == 20:  # 20 episodes after change\n",
    "                # Calculate performance drop\n",
    "                post_change_perf = calculate_rolling_metrics(episoderewards[-20:])\n",
    "                adaptation_metrics['post_change_performance'].append(post_change_perf)\n",
    "                \n",
    "                # Calculate performance drop as percentage\n",
    "                pre_perf = adaptation_metrics['pre_change_performance'][-1]\n",
    "                perf_drop_pct = (pre_perf - post_change_perf) / pre_perf * 100 if pre_perf > 0 else 0\n",
    "                adaptation_metrics['performance_drop'].append(perf_drop_pct)\n",
    "                \n",
    "                # Start tracking recovery time\n",
    "                recovery_threshold = pre_perf * 0.95  # Recovered when back to 95% of pre-change performance\n",
    "                \n",
    "                # Recovery time will be calculated at the end of the run\n",
    "        \n",
    "        # Train the agent\n",
    "        agent, env, rewards = trainDQLearning(\n",
    "            agent=rewardinfo['agent'],\n",
    "            env=env,\n",
    "            numEpisodes=episodes,\n",
    "            updateSystem=rewardinfo['updatesystem'],\n",
    "            onEpisodeEnd=onEpisodeEnd\n",
    "        )\n",
    "        \n",
    "        # Calculate recovery times after training is complete\n",
    "        for i, change_ep in enumerate(adaptation_metrics['change_episodes']):\n",
    "            recovery_threshold = adaptation_metrics['pre_change_performance'][i] * 0.95\n",
    "            recovery_time = episodes - change_ep  # Default: never recovered\n",
    "            \n",
    "            # Find first point after change where performance exceeds recovery threshold\n",
    "            rolling_rewards = pd.Series(episoderewards[change_ep:]).rolling(window=20).mean()\n",
    "            for j, val in enumerate(rolling_rewards):\n",
    "                if not np.isnan(val) and val >= recovery_threshold:\n",
    "                    recovery_time = j\n",
    "                    break\n",
    "                    \n",
    "            adaptation_metrics['recovery_times'].append(recovery_time)\n",
    "        \n",
    "        # Store all results\n",
    "        results[rewardname] = {\n",
    "            'rewards': episoderewards,\n",
    "            'balancetimes': episodebalancetimes,\n",
    "            'rewardChanges': rewardchangeepisodes,\n",
    "            'environmentChanges': environmentchanges,\n",
    "            'adaptation_metrics': adaptation_metrics\n",
    "        }\n",
    "        \n",
    "        # Add component data for adaptive reward\n",
    "        if component_weights is not None:\n",
    "            results[rewardname]['component_weights'] = component_weights\n",
    "        \n",
    "        if component_updates is not None:\n",
    "            results[rewardname]['component_updates'] = component_updates\n",
    "        \n",
    "        # Print final metrics\n",
    "        print(f\"\\nCompleted testing {rewardname}\")\n",
    "        print(f\"Final average reward: {np.mean(episoderewards[-100:]):.2f}\")\n",
    "        print(f\"Final average balance time: {np.mean(episodebalancetimes[-100:]):.2f}\")\n",
    "        if adaptation_metrics['recovery_times']:\n",
    "            print(f\"Average recovery time: {np.mean(adaptation_metrics['recovery_times']):.2f} episodes\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the analysis with a smaller number of episodes for testing\n",
    "# Adjust parameters as needed for your specific case\n",
    "changeInterval = 2500\n",
    "episodes = 10000\n",
    "\n",
    "# Comment/uncomment to run the analysis\n",
    "results = run_complete_analysis(episodes=episodes, change_interval=changeInterval, num_runs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_statistical_tests(results):\n",
    "    \"\"\"Run statistical tests to compare performance of different reward approaches\"\"\"\n",
    "    # Dictionary to store test results\n",
    "    test_results = {}\n",
    "    \n",
    "    # Prepare data for tests\n",
    "    reward_data = []\n",
    "    \n",
    "    # Collect episode rewards for each approach\n",
    "    for reward_type, reward_info in results.items():\n",
    "        rewards = np.array(reward_info['rewards'])\n",
    "        balance_times = np.array(reward_info['balancetimes'])\n",
    "        \n",
    "        # Create a DataFrame row for each episode\n",
    "        for i in range(len(rewards)):\n",
    "            row = {\n",
    "                'reward_type': reward_type,\n",
    "                'episode': i,\n",
    "                'reward': rewards[i],\n",
    "                'balance_time': balance_times[i]\n",
    "            }\n",
    "            reward_data.append(row)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(reward_data)\n",
    "    \n",
    "    # Run tests for different metrics\n",
    "    metrics = ['reward', 'balance_time']\n",
    "    for metric in metrics:\n",
    "        # Run ANOVA to test if there are significant differences between approaches\n",
    "        formula = f\"{metric} ~ C(reward_type)\"\n",
    "        model = ols(formula, data=df).fit()\n",
    "        anova_table = anova_lm(model, typ=2)\n",
    "        \n",
    "        # Store ANOVA results\n",
    "        p_value = anova_table['PR(>F)'][0]\n",
    "        significant = p_value < 0.05\n",
    "        \n",
    "        # Print ANOVA results\n",
    "        print(f\"\\nANOVA for {metric}:\")\n",
    "        print(anova_table)\n",
    "        print(f\"Significant difference: {significant} (p-value: {p_value:.4f})\")\n",
    "        \n",
    "        # If ANOVA is significant, run pairwise t-tests with Bonferroni correction\n",
    "        pairwise_results = {}\n",
    "        if significant:\n",
    "            # Get all unique reward types\n",
    "            reward_types = df['reward_type'].unique()\n",
    "            \n",
    "            # Calculate number of comparisons for Bonferroni correction\n",
    "            num_comparisons = len(reward_types) * (len(reward_types) - 1) // 2\n",
    "            # Bonferroni-corrected alpha\n",
    "            alpha_corrected = 0.05 / num_comparisons\n",
    "            \n",
    "            print(f\"\\nPairwise t-tests with Bonferroni correction (alpha = {alpha_corrected:.5f}):\")\n",
    "            \n",
    "            # Run t-tests for each pair of reward types\n",
    "            for i, type1 in enumerate(reward_types):\n",
    "                for type2 in reward_types[i+1:]:\n",
    "                    # Get data for each group\n",
    "                    group1_data = df[df['reward_type'] == type1][metric]\n",
    "                    group2_data = df[df['reward_type'] == type2][metric]\n",
    "                    \n",
    "                    # Run t-test\n",
    "                    t_stat, p_val = stats.ttest_ind(group1_data, group2_data, equal_var=False)\n",
    "                    \n",
    "                    # Check if significant with Bonferroni correction\n",
    "                    significant = p_val < alpha_corrected\n",
    "                    \n",
    "                    # Store results\n",
    "                    pair = f\"{type1} vs {type2}\"\n",
    "                    pairwise_results[pair] = {\n",
    "                        'mean_diff': float(group1_data.mean() - group2_data.mean()),\n",
    "                        'p_value': float(p_val),\n",
    "                        'significant': bool(significant),\n",
    "                        't_statistic': float(t_stat)\n",
    "                    }\n",
    "                    \n",
    "                    # Print formatted results\n",
    "                    sig_symbol = \"*\" if significant else \"\"\n",
    "                    print(f\"{pair}: diff = {pairwise_results[pair]['mean_diff']:.2f}, p = {p_val:.4f}{sig_symbol}\")\n",
    "        \n",
    "        # Store all results for this metric\n",
    "        test_results[metric] = {\n",
    "            'anova_p_value': float(p_value),\n",
    "            'anova_significant': bool(significant),\n",
    "            'pairwise_comparisons': pairwise_results\n",
    "        }\n",
    "    \n",
    "    return test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reward_over_time_plot(results, changeInterval, save_path=None, include_variance=True):\n",
    "    \"\"\"\n",
    "    Create a reward over time plot with optional variance bands.\n",
    "    Shows all reward functions on one plot with clear markers for environment changes.\n",
    "    \n",
    "    Args:\n",
    "        results: Dictionary containing results from each reward approach\n",
    "        changeInterval: Number of episodes between environment changes\n",
    "        save_path: Directory to save the visualization (optional)\n",
    "        include_variance: Whether to include variance bands (default: True)\n",
    "    \n",
    "    Returns:\n",
    "        Matplotlib figure object\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Define distinguishable colors and line styles\n",
    "    colors = {\n",
    "        'adaptivereward': '#1f77b4',  # blue\n",
    "        'energy_based': '#2ca02c',    # green\n",
    "        'pbrs': '#ff7f0e',            # orange\n",
    "        'baseline': '#d62728'         # red\n",
    "    }\n",
    "    \n",
    "    line_styles = {\n",
    "        'adaptivereward': '-',\n",
    "        'energy_based': '-.',\n",
    "        'pbrs': '--',\n",
    "        'baseline': ':'\n",
    "    }\n",
    "    \n",
    "    # Plot each reward function\n",
    "    for rewardname, rewardresults in results.items():\n",
    "        rewards = rewardresults['rewards']\n",
    "        \n",
    "        # Apply smoother with larger window for cleaner visualization\n",
    "        window = 50\n",
    "        smoothed_rewards = pd.Series(rewards).rolling(window=window, min_periods=1).mean()\n",
    "        \n",
    "        if include_variance:\n",
    "            # Calculate rolling standard deviation for variance bands\n",
    "            rolling_std = pd.Series(rewards).rolling(window=window, min_periods=1).std()\n",
    "            \n",
    "            # Create confidence interval (1 standard deviation)\n",
    "            upper_bound = smoothed_rewards + rolling_std\n",
    "            lower_bound = smoothed_rewards - rolling_std\n",
    "            \n",
    "            # Fill the area between upper and lower bounds\n",
    "            plt.fill_between(\n",
    "                range(len(rewards)),\n",
    "                lower_bound,\n",
    "                upper_bound,\n",
    "                alpha=0.2,\n",
    "                color=colors.get(rewardname, 'black'),\n",
    "                label=f\"{rewardname} variance\" if rewardname == list(results.keys())[0] else None\n",
    "            )\n",
    "        \n",
    "        plt.plot(\n",
    "            range(len(rewards)), \n",
    "            smoothed_rewards,\n",
    "            label=rewardname,\n",
    "            color=colors.get(rewardname, 'black'),\n",
    "            linestyle=line_styles.get(rewardname, '-'),\n",
    "            linewidth=2.5\n",
    "        )\n",
    "    \n",
    "    # Add vertical lines for environment changes\n",
    "    change_episodes = list(range(changeInterval, len(next(iter(results.values()))['rewards']), changeInterval))\n",
    "    for ep in change_episodes:\n",
    "        plt.axvline(x=ep, color='red', linestyle='--', alpha=0.5,\n",
    "                   label='Environment Change' if ep == change_episodes[0] else None)\n",
    "    \n",
    "    # Add annotations for environment changes\n",
    "    for i, ep in enumerate(change_episodes):\n",
    "        param_value = 0.9 if i % 2 else 0.3  # Alternating pole length (customize as needed)\n",
    "        plt.annotate(\n",
    "            f\"Length: {param_value}m\",\n",
    "            xy=(ep, plt.gca().get_ylim()[1] * 0.95),\n",
    "            xytext=(ep + 50, plt.gca().get_ylim()[1] * 0.95),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05, width=1.5, headwidth=8),\n",
    "            fontsize=10,\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8)\n",
    "        )\n",
    "    \n",
    "    # Add title and labels\n",
    "    title_suffix = \" with Variance Bands\" if include_variance else \"\"\n",
    "    plt.title(f'Reward Performance Comparison Across Different Approaches{title_suffix}', fontsize=16)\n",
    "    plt.xlabel('Episode', fontsize=14)\n",
    "    plt.ylabel('Average Reward (smoothed)', fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Create a custom legend with larger markers\n",
    "    plt.legend(fontsize=12, loc='upper center', bbox_to_anchor=(0.5, -0.15), \n",
    "               ncol=3, frameon=True, fancybox=True, shadow=True)\n",
    "    \n",
    "    # Adjust margins\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot if a path is provided\n",
    "    if save_path:\n",
    "        # Create the directory if it doesn't exist\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        \n",
    "        # Use a descriptive filename\n",
    "        variance_suffix = \"_with_variance\" if include_variance else \"\"\n",
    "        filename = f\"reward_over_time{variance_suffix}.png\"\n",
    "        \n",
    "        # Join the path correctly\n",
    "        filepath = os.path.join(save_path, filename)\n",
    "        \n",
    "        # Print debug info\n",
    "        print(f\"Saving reward plot to: {filepath}\")\n",
    "        \n",
    "        # Save with a good margin setting\n",
    "        plt.savefig(filepath, bbox_inches='tight', dpi=300)\n",
    "        print(f\"Saved plot to {filepath}\")\n",
    "    \n",
    "    return plt.gcf()  # Return the figure object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_performance_breakdown(results, change_interval):\n",
    "    \"\"\"Analyze performance during different phases of the experiment\"\"\"\n",
    "    \n",
    "    # Identify environments\n",
    "    env_changes = next(iter(results.values()))['environmentChanges']\n",
    "    total_episodes = len(next(iter(results.values()))['rewards'])\n",
    "    \n",
    "    # Define environments/phases\n",
    "    phases = []\n",
    "    for i, change_ep in enumerate(env_changes):\n",
    "        start_ep = 0 if i == 0 else env_changes[i-1]\n",
    "        end_ep = change_ep\n",
    "        \n",
    "        # Add a phase\n",
    "        phases.append({\n",
    "            'name': f\"Environment {i+1}\",\n",
    "            'start': start_ep,\n",
    "            'end': end_ep,\n",
    "            'length': end_ep - start_ep\n",
    "        })\n",
    "    \n",
    "    # Add the final phase\n",
    "    if env_changes:\n",
    "        phases.append({\n",
    "            'name': f\"Environment {len(env_changes)+1}\",\n",
    "            'start': env_changes[-1],\n",
    "            'end': total_episodes,\n",
    "            'length': total_episodes - env_changes[-1]\n",
    "        })\n",
    "    \n",
    "    # Calculate per-environment metrics\n",
    "    phase_metrics = []\n",
    "    \n",
    "    for phase in phases:\n",
    "        phase_data = {'name': phase['name']}\n",
    "        \n",
    "        for reward_type, reward_data in results.items():\n",
    "            # Skip if this reward type doesn't have enough data\n",
    "            if phase['end'] > len(reward_data['rewards']):\n",
    "                continue\n",
    "                \n",
    "            # Calculate metrics for this phase\n",
    "            rewards_slice = reward_data['rewards'][phase['start']:phase['end']]\n",
    "            balance_slice = reward_data['balancetimes'][phase['start']:phase['end']]\n",
    "            \n",
    "            # Skip empty slices\n",
    "            if not rewards_slice or not balance_slice:\n",
    "                continue\n",
    "                \n",
    "            phase_data[f\"{reward_type}_avg_reward\"] = np.mean(rewards_slice)\n",
    "            phase_data[f\"{reward_type}_avg_balance\"] = np.mean(balance_slice)\n",
    "            phase_data[f\"{reward_type}_stability\"] = 1.0 - (np.std(rewards_slice) / np.mean(rewards_slice)) if np.mean(rewards_slice) > 0 else 0\n",
    "        \n",
    "        phase_metrics.append(phase_data)\n",
    "    \n",
    "    # Convert to DataFrame for easier analysis\n",
    "    phase_df = pd.DataFrame(phase_metrics)\n",
    "    \n",
    "    print(\"\\nPerformance by Environment Phase:\")\n",
    "    print(phase_df.round(2))\n",
    "    \n",
    "    # Calculate relative performance (adaptive vs others)\n",
    "    for phase in phase_metrics:\n",
    "        # Skip if adaptive reward data is not available for this phase\n",
    "        if 'adaptivereward_avg_reward' not in phase:\n",
    "            continue\n",
    "            \n",
    "        for reward_type in [r for r in results.keys() if r != 'adaptivereward']:\n",
    "            # Skip if this reward type doesn't have data for this phase\n",
    "            if f\"{reward_type}_avg_reward\" not in phase:\n",
    "                continue\n",
    "                \n",
    "            # Calculate relative performance\n",
    "            relative_reward = (phase['adaptivereward_avg_reward'] / phase[f\"{reward_type}_avg_reward\"] - 1) * 100\n",
    "            phase[f\"relative_to_{reward_type}_pct\"] = relative_reward\n",
    "    \n",
    "    # Create a visualization of performance by phase\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Set up bar positions\n",
    "    bar_width = 0.2\n",
    "    index = np.arange(len(phase_metrics))\n",
    "    \n",
    "    # Plot bars for each reward type\n",
    "    colors = ['b', 'g', 'r', 'c']\n",
    "    for i, reward_type in enumerate(results.keys()):\n",
    "        values = [phase.get(f\"{reward_type}_avg_reward\", 0) for phase in phase_metrics]\n",
    "        plt.bar(index + i*bar_width, values, bar_width, \n",
    "               label=reward_type, color=colors[i], alpha=0.7)\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.xlabel('Environment Phase')\n",
    "    plt.ylabel('Average Reward')\n",
    "    plt.title('Performance Comparison Across Environment Phases')\n",
    "    plt.xticks(index + bar_width, [phase['name'] for phase in phase_metrics])\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"performance_by_phase.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Create a heatmap of relative performance\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Extract relative performance data\n",
    "    relative_data = []\n",
    "    for phase in phase_metrics:\n",
    "        row = {'phase': phase['name']}\n",
    "        for reward_type in [r for r in results.keys() if r != 'adaptivereward']:\n",
    "            key = f\"relative_to_{reward_type}_pct\"\n",
    "            if key in phase:\n",
    "                row[reward_type] = phase[key]\n",
    "        relative_data.append(row)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    relative_df = pd.DataFrame(relative_data).set_index('phase')\n",
    "    \n",
    "    # Create heatmap\n",
    "    ax = sns.heatmap(relative_df, annot=True, cmap='RdYlGn', center=0, \n",
    "                    fmt='.1f', cbar_kws={'label': 'Relative Performance (%)'}, \n",
    "                    linewidths=0.5)\n",
    "    \n",
    "    plt.title('Adaptive Reward Performance Relative to Other Approaches (%)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"relative_performance_heatmap.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return phase_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Component Analysis\n",
    "\n",
    "This section analyzes how the adaptive reward function's components interact and evolve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_adaptive_components(results):\n",
    "    \"\"\"Analyze the evolution and contribution of adaptive reward components\"\"\"\n",
    "    \n",
    "    # Check if adaptive reward results exist and have component data\n",
    "    if 'adaptivereward' not in results or 'component_weights' not in results['adaptivereward']:\n",
    "        print(\"No component data available for adaptive reward\")\n",
    "        return None\n",
    "    \n",
    "    # Extract component weight data\n",
    "    component_data = results['adaptivereward']['component_weights']\n",
    "    episodes = [d['episode'] for d in component_data]\n",
    "    stability_weights = [d['stability'] for d in component_data]\n",
    "    efficiency_weights = [d['efficiency'] for d in component_data]\n",
    "    \n",
    "    # Extract reward changes and environment changes\n",
    "    reward_changes = results['adaptivereward']['rewardChanges']\n",
    "    env_changes = results['adaptivereward']['environmentChanges']\n",
    "    \n",
    "    # Create weight evolution visualization\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Plot component weights\n",
    "    plt.plot(episodes, stability_weights, 'b-', label='Stability Weight', linewidth=2)\n",
    "    plt.plot(episodes, efficiency_weights, 'g-', label='Efficiency Weight', linewidth=2)\n",
    "    \n",
    "    # Add vertical lines for reward function updates\n",
    "    for ep in reward_changes:\n",
    "        plt.axvline(x=ep, color='g', linestyle='--', alpha=0.5, \n",
    "                   label='Reward Update' if reward_changes.index(ep) == 0 else None)\n",
    "    \n",
    "    # Add vertical lines for environment changes\n",
    "    for ep in env_changes:\n",
    "        plt.axvline(x=ep, color='r', linestyle='--', alpha=0.5,\n",
    "                   label='Environment Change' if env_changes.index(ep) == 0 else None)\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.title('Evolution of Adaptive Reward Component Weights')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Component Weight')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"component_weight_evolution.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze performance correlation with weights\n",
    "    rewards = results['adaptivereward']['rewards']\n",
    "    \n",
    "    # Create a DataFrame with all data\n",
    "    df = pd.DataFrame({\n",
    "        'episode': episodes,\n",
    "        'stability_weight': stability_weights,\n",
    "        'efficiency_weight': efficiency_weights,\n",
    "        'reward': [rewards[ep] if ep < len(rewards) else np.nan for ep in episodes]\n",
    "    })\n",
    "    \n",
    "    # Calculate rolling reward for smoother analysis\n",
    "    df['rolling_reward'] = df['reward'].rolling(window=20).mean()\n",
    "    \n",
    "    # Calculate correlations\n",
    "    corr_stability = df['stability_weight'].corr(df['rolling_reward'])\n",
    "    corr_efficiency = df['efficiency_weight'].corr(df['rolling_reward'])\n",
    "    \n",
    "    print(f\"\\nCorrelation between stability weight and reward: {corr_stability:.3f}\")\n",
    "    print(f\"Correlation between efficiency weight and reward: {corr_efficiency:.3f}\")\n",
    "    \n",
    "    # Create a scatter plot of weights vs performance\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Plot stability weight vs reward\n",
    "    sns.regplot(x='stability_weight', y='rolling_reward', data=df.dropna(), \n",
    "               ax=ax1, scatter_kws={'alpha': 0.3}, line_kws={'color': 'red'})\n",
    "    ax1.set_title(f'Stability Weight vs Reward (corr = {corr_stability:.3f})')\n",
    "    ax1.set_xlabel('Stability Weight')\n",
    "    ax1.set_ylabel('Rolling Average Reward')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot efficiency weight vs reward\n",
    "    sns.regplot(x='efficiency_weight', y='rolling_reward', data=df.dropna(), \n",
    "               ax=ax2, scatter_kws={'alpha': 0.3}, line_kws={'color': 'red'})\n",
    "    ax2.set_title(f'Efficiency Weight vs Reward (corr = {corr_efficiency:.3f})')\n",
    "    ax2.set_xlabel('Efficiency Weight')\n",
    "    ax2.set_ylabel('Rolling Average Reward')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"component_weight_correlation.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Create a heatmap of environment phase vs component weight\n",
    "    if env_changes:\n",
    "        # Define environment phases\n",
    "        phases = []\n",
    "        phase_names = []\n",
    "        \n",
    "        for i, change_ep in enumerate(env_changes):\n",
    "            start_ep = 0 if i == 0 else env_changes[i-1]\n",
    "            end_ep = change_ep\n",
    "            phases.append((start_ep, end_ep))\n",
    "            phase_names.append(f\"Env {i+1}\")\n",
    "        \n",
    "        # Add the final phase\n",
    "        phases.append((env_changes[-1], episodes[-1]))\n",
    "        phase_names.append(f\"Env {len(env_changes)+1}\")\n",
    "        \n",
    "        # Calculate average weights per phase\n",
    "        phase_weights = []\n",
    "        for (start, end), name in zip(phases, phase_names):\n",
    "            phase_data = df[(df['episode'] >= start) & (df['episode'] < end)]\n",
    "            if not phase_data.empty:\n",
    "                phase_weights.append({\n",
    "                    'phase': name,\n",
    "                    'stability': phase_data['stability_weight'].mean(),\n",
    "                    'efficiency': phase_data['efficiency_weight'].mean(),\n",
    "                    'reward': phase_data['rolling_reward'].mean()\n",
    "                })\n",
    "        \n",
    "        # Create a DataFrame\n",
    "        phase_df = pd.DataFrame(phase_weights)\n",
    "        phase_df = phase_df.set_index('phase')\n",
    "        \n",
    "        # Analyze which component dominates in each phase\n",
    "        phase_df['dominant_component'] = phase_df.apply(\n",
    "            lambda row: 'Stability' if row['stability'] > row['efficiency'] else 'Efficiency', \n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # Normalized weight proportions\n",
    "        total_weights = phase_df['stability'] + phase_df['efficiency']\n",
    "        phase_df['stability_pct'] = (phase_df['stability'] / total_weights) * 100\n",
    "        phase_df['efficiency_pct'] = (phase_df['efficiency'] / total_weights) * 100\n",
    "        \n",
    "        print(\"\\nComponent Analysis by Environment Phase:\")\n",
    "        print(phase_df.round(2))\n",
    "        \n",
    "        # Create a stacked bar chart of component weights by phase\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Extract data for plotting\n",
    "        phases = phase_df.index\n",
    "        stability_pct = phase_df['stability_pct']\n",
    "        efficiency_pct = phase_df['efficiency_pct']\n",
    "        \n",
    "        # Create stacked bars\n",
    "        plt.bar(phases, stability_pct, color='b', alpha=0.7, label='Stability')\n",
    "        plt.bar(phases, efficiency_pct, bottom=stability_pct, color='g', alpha=0.7, label='Efficiency')\n",
    "        \n",
    "        # Add reward as line\n",
    "        ax2 = plt.twinx()\n",
    "        ax2.plot(phases, phase_df['reward'], 'ro-', linewidth=2, label='Avg Reward')\n",
    "        ax2.set_ylabel('Average Reward', color='r')\n",
    "        ax2.tick_params(axis='y', colors='r')\n",
    "        \n",
    "        # Customize plot\n",
    "        plt.title('Component Weight Distribution by Environment Phase')\n",
    "        plt.xlabel('Environment Phase')\n",
    "        plt.ylabel('Weight Percentage (%)')\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add text annotations for dominant component\n",
    "        for i, (phase, row) in enumerate(phase_df.iterrows()):\n",
    "            plt.text(i, 50, f\"Dominant:\\n{row['dominant_component']}\", \n",
    "                    ha='center', va='center', fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"component_weight_distribution.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    # Analyze the component updates if available\n",
    "    if 'component_updates' in results['adaptivereward'] and results['adaptivereward']['component_updates']:\n",
    "        updates = results['adaptivereward']['component_updates']\n",
    "        \n",
    "        print(\"\\nComponent Update Analysis:\")\n",
    "        print(f\"Total updates: {len(updates)}\")\n",
    "        \n",
    "        # Count updates by component\n",
    "        component_counts = {}\n",
    "        for update in updates:\n",
    "            component = f\"Component {update['component']}\"\n",
    "            if component not in component_counts:\n",
    "                component_counts[component] = 0\n",
    "            component_counts[component] += 1\n",
    "        \n",
    "        for component, count in component_counts.items():\n",
    "            print(f\"{component}: {count} updates\")\n",
    "        \n",
    "        # Plot update timeline\n",
    "        plt.figure(figsize=(14, 4))\n",
    "        \n",
    "        for update in updates:\n",
    "            color = 'b' if update['component'] == 1 else 'g'\n",
    "            label = f\"Component {update['component']} Update\" if update == updates[0] or update == updates[1] else None\n",
    "            plt.axvline(x=update['episode'], color=color, linestyle='-', linewidth=2, alpha=0.7, label=label)\n",
    "        \n",
    "        # Add environment changes\n",
    "        for ep in env_changes:\n",
    "            plt.axvline(x=ep, color='r', linestyle='--', alpha=0.5,\n",
    "                       label='Environment Change' if env_changes.index(ep) == 0 else None)\n",
    "        \n",
    "        # Customize plot\n",
    "        plt.title('Adaptive Reward Function Update Timeline')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.yticks([])\n",
    "        plt.legend()\n",
    "        plt.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"component_update_timeline.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_adaptation_speed(results, change_interval):\n",
    "    \"\"\"\n",
    "    Analyze how quickly each reward approach adapts to environment changes.\n",
    "    \n",
    "    Args:\n",
    "        results: Dictionary of results from different reward approaches\n",
    "        change_interval: Number of episodes between environment changes\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of adaptation metrics for each reward approach\n",
    "    \"\"\"\n",
    "    adaptation_metrics = {}\n",
    "    \n",
    "    for reward_type, reward_data in results.items():\n",
    "        # Skip if no rewards data\n",
    "        if 'rewards' not in reward_data or not reward_data['rewards']:\n",
    "            continue\n",
    "            \n",
    "        rewards = reward_data['rewards']\n",
    "        env_changes = reward_data['environmentChanges']\n",
    "        \n",
    "        # Skip if no environment changes\n",
    "        if not env_changes:\n",
    "            continue\n",
    "            \n",
    "        # Calculate metrics for each change\n",
    "        recovery_times = []\n",
    "        performance_drops = []\n",
    "        adaptation_effectiveness = []\n",
    "        \n",
    "        for change_ep in env_changes:\n",
    "            # Skip if change is too close to the end\n",
    "            if change_ep + 100 >= len(rewards):\n",
    "                continue\n",
    "                \n",
    "            # Calculate pre-change performance (20 episodes before change)\n",
    "            pre_change_start = max(0, change_ep - 20)\n",
    "            pre_change_rewards = rewards[pre_change_start:change_ep]\n",
    "            pre_change_avg = np.mean(pre_change_rewards) if pre_change_rewards else 0\n",
    "            \n",
    "            # Calculate immediate post-change performance (20 episodes after change)\n",
    "            post_change_rewards = rewards[change_ep:change_ep+20]\n",
    "            post_change_avg = np.mean(post_change_rewards) if post_change_rewards else 0\n",
    "            \n",
    "            # Calculate performance drop\n",
    "            if pre_change_avg > 0:\n",
    "                drop_pct = max(0, (pre_change_avg - post_change_avg) / pre_change_avg * 100)\n",
    "                performance_drops.append(drop_pct)\n",
    "            \n",
    "            # Calculate recovery time\n",
    "            recovery_threshold = 0.9 * pre_change_avg  # 90% of pre-change performance\n",
    "            recovery_ep = change_ep + change_interval  # Default: never recovered\n",
    "            \n",
    "            for i in range(change_ep, min(change_ep + change_interval, len(rewards))):\n",
    "                # Use sliding window of 10 episodes\n",
    "                window_start = max(change_ep, i - 10)\n",
    "                window_rewards = rewards[window_start:i+1]\n",
    "                window_avg = np.mean(window_rewards) if window_rewards else 0\n",
    "                \n",
    "                if window_avg >= recovery_threshold:\n",
    "                    recovery_ep = i\n",
    "                    break\n",
    "            \n",
    "            recovery_time = recovery_ep - change_ep\n",
    "            recovery_times.append(recovery_time)\n",
    "            \n",
    "            # Calculate adaptation effectiveness\n",
    "            if recovery_time > 0 and change_ep + recovery_time < len(rewards):\n",
    "                # Area under the recovery curve\n",
    "                recovery_rewards = rewards[change_ep:change_ep + recovery_time]\n",
    "                actual_area = np.sum(recovery_rewards)\n",
    "                ideal_area = pre_change_avg * recovery_time\n",
    "                \n",
    "                if ideal_area > 0:\n",
    "                    effectiveness = actual_area / ideal_area\n",
    "                    adaptation_effectiveness.append(effectiveness)\n",
    "        \n",
    "        # Store metrics for this reward type\n",
    "        adaptation_metrics[reward_type] = {\n",
    "            'avg_recovery_time': np.mean(recovery_times) if recovery_times else float('nan'),\n",
    "            'avg_performance_drop': np.mean(performance_drops) if performance_drops else float('nan'),\n",
    "            'avg_adaptation_effectiveness': np.mean(adaptation_effectiveness) if adaptation_effectiveness else float('nan'),\n",
    "            'recovery_times': recovery_times,\n",
    "            'performance_drops': performance_drops\n",
    "        }\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nAdaptation Speed Analysis:\")\n",
    "    for reward_type, metrics in adaptation_metrics.items():\n",
    "        print(f\"\\n{reward_type}:\")\n",
    "        print(f\"  Average Recovery Time: {metrics['avg_recovery_time']:.2f} episodes\")\n",
    "        print(f\"  Average Performance Drop: {metrics['avg_performance_drop']:.2f}%\")\n",
    "        print(f\"  Adaptation Effectiveness: {metrics['avg_adaptation_effectiveness']:.2f}\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    create_adaptation_visualization(adaptation_metrics, results)\n",
    "    \n",
    "    return adaptation_metrics\n",
    "\n",
    "def create_adaptation_visualization(adaptation_metrics, results):\n",
    "    \"\"\"Create visualizations of adaptation metrics\"\"\"\n",
    "    \n",
    "    # 1. Recovery Time Comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    labels = list(adaptation_metrics.keys())\n",
    "    recovery_times = [m['avg_recovery_time'] for m in adaptation_metrics.values()]\n",
    "    performance_drops = [m['avg_performance_drop'] for m in adaptation_metrics.values()]\n",
    "    \n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.35\n",
    "    \n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Plot recovery times\n",
    "    bars1 = ax1.bar(x - width/2, recovery_times, width, label='Recovery Time (episodes)', color='b', alpha=0.7)\n",
    "    ax1.set_ylabel('Recovery Time (episodes)', color='b')\n",
    "    ax1.tick_params(axis='y', colors='b')\n",
    "    \n",
    "    # Add second y-axis for performance drop\n",
    "    ax2 = ax1.twinx()\n",
    "    bars2 = ax2.bar(x + width/2, performance_drops, width, label='Performance Drop (%)', color='r', alpha=0.7)\n",
    "    ax2.set_ylabel('Performance Drop (%)', color='r')\n",
    "    ax2.tick_params(axis='y', colors='r')\n",
    "    \n",
    "    # Add labels and legend\n",
    "    ax1.set_title('Adaptation Speed Comparison')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(labels)\n",
    "    ax1.set_xlabel('Reward Approach')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, bar in enumerate(bars1):\n",
    "        height = bar.get_height()\n",
    "        if not np.isnan(height):\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                    f'{height:.1f}', ha='center', va='bottom', color='b', fontweight='bold')\n",
    "    \n",
    "    for i, bar in enumerate(bars2):\n",
    "        height = bar.get_height()\n",
    "        if not np.isnan(height):\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                    f'{height:.1f}%', ha='center', va='bottom', color='r', fontweight='bold')\n",
    "    \n",
    "    # Combine legends\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"adaptation_speed_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Recovery Curves Visualization\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Get the first environment change episode\n",
    "    env_changes = next(iter(results.values()))['environmentChanges']\n",
    "    if not env_changes:\n",
    "        return\n",
    "    \n",
    "    first_change = env_changes[0]\n",
    "    window = 100  # Episodes to show after change\n",
    "    \n",
    "    # Plot recovery curves for each approach\n",
    "    for reward_type, reward_data in results.items():\n",
    "        if first_change >= len(reward_data['rewards']):\n",
    "            continue\n",
    "            \n",
    "        # Get pre-change performance as baseline\n",
    "        pre_change_start = max(0, first_change - 20)\n",
    "        pre_change_rewards = reward_data['rewards'][pre_change_start:first_change]\n",
    "        pre_change_avg = np.mean(pre_change_rewards) if pre_change_rewards else 1.0\n",
    "        \n",
    "        # Get post-change rewards\n",
    "        post_window = min(window, len(reward_data['rewards']) - first_change)\n",
    "        post_rewards = reward_data['rewards'][first_change:first_change + post_window]\n",
    "        \n",
    "        # Normalize as percentage of pre-change performance\n",
    "        normalized_rewards = [r / pre_change_avg * 100 for r in post_rewards]\n",
    "        \n",
    "        # Smooth the curve\n",
    "        smoothed = gaussian_filter1d(normalized_rewards, sigma=2)\n",
    "        \n",
    "        # Plot the recovery curve\n",
    "        plt.plot(range(len(smoothed)), smoothed, label=reward_type, linewidth=2)\n",
    "    \n",
    "    # Add pre-change level reference line\n",
    "    plt.axhline(y=100, color='k', linestyle='--', alpha=0.5, label='Pre-change level')\n",
    "    \n",
    "    # Add labels and legend\n",
    "    plt.title('Recovery Curves After Environment Change')\n",
    "    plt.xlabel('Episodes After Change')\n",
    "    plt.ylabel('Performance (% of pre-change)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"recovery_curves.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a complete performance analysis\n",
    "def run_complete_analysis(episodes=10000, change_interval=2500, num_runs=1):\n",
    "    \"\"\"Run a complete performance analysis with statistical tests\"\"\"\n",
    "    all_results = []\n",
    "\n",
    "    \n",
    "    main_results_folder = \"AdvancedPerformanceResults\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    experiment_folder = os.path.join(main_results_folder, f\"AdvancedAnalysis_{timestamp}\")\n",
    "    os.makedirs(experiment_folder, exist_ok=True)\n",
    "    \n",
    "    # Run multiple experiment iterations\n",
    "    for run in range(num_runs):\n",
    "        print(f\"\\n=============== Starting Run {run+1}/{num_runs} ===============\")\n",
    "        \n",
    "        # Run the enhanced performance test\n",
    "        seed = 42 + run  # Different seed for each run\n",
    "        results = runEnhancedPerformanceTest(\n",
    "            episodes=episodes,\n",
    "            changeInterval=change_interval,\n",
    "            lengthchanges=[0.3, 0.9],  # Short pole vs long pole\n",
    "            seed=seed,\n",
    "            collect_component_data=True\n",
    "        )\n",
    "        \n",
    "        all_results.append(results)\n",
    "        \n",
    "        # Create a subfolder for this run\n",
    "        run_folder = os.path.join(experiment_folder, f\"run_{run+1}\")\n",
    "        os.makedirs(run_folder, exist_ok=True)\n",
    "        \n",
    "        # Save the working directory to return to it after each analysis\n",
    "        original_dir = os.getcwd()\n",
    "        os.chdir(run_folder)\n",
    "        \n",
    "        # Run analyses for this iteration\n",
    "        print(\"\\n--- Running Statistical Significance Tests ---\")\n",
    "        statistical_results = run_statistical_tests(results)\n",
    "        \n",
    "        print(\"\\n--- Analyzing Adaptation Speed ---\")\n",
    "        adaptation_metrics = analyze_adaptation_speed(results, change_interval)\n",
    "        \n",
    "        print(\"\\n--- Analyzing Performance Breakdown ---\")\n",
    "        phase_metrics = analyze_performance_breakdown(results, change_interval)\n",
    "        \n",
    "        print(\"\\n--- Analyzing Adaptive Components ---\")\n",
    "        component_analysis = analyze_adaptive_components(results)\n",
    "        \n",
    "        # NEW: Create reward over time plots - both with and without variance\n",
    "        print(\"\\n--- Creating Reward Over Time Plots ---\")\n",
    "        # Create plot with variance bands\n",
    "        reward_plot_with_variance = create_reward_over_time_plot(\n",
    "            results, \n",
    "            change_interval, \n",
    "            save_path=run_folder,\n",
    "            include_variance=True\n",
    "        )\n",
    "        plt.close(reward_plot_with_variance)\n",
    "        \n",
    "        # Create plot without variance bands\n",
    "        reward_plot_without_variance = create_reward_over_time_plot(\n",
    "            results, \n",
    "            change_interval, \n",
    "            save_path=run_folder,\n",
    "            include_variance=False\n",
    "        )\n",
    "        plt.close(reward_plot_without_variance)\n",
    "        \n",
    "        # Save results to JSON\n",
    "        analysis_summary = {\n",
    "            'statistical_results': {\n",
    "                metric: {\n",
    "                    'anova_p_value': stats['anova_p_value'],\n",
    "                    'anova_significant': stats['anova_significant'],\n",
    "                    # Convert complex objects to strings\n",
    "                    'pairwise_significant': {\n",
    "                        pair: result['significant'] \n",
    "                        for pair, result in stats['pairwise_comparisons'].items()\n",
    "                    }\n",
    "                }\n",
    "                for metric, stats in statistical_results.items()\n",
    "            },\n",
    "            'adaptation_metrics': {\n",
    "                reward_type: {\n",
    "                    'avg_recovery_time': metrics['avg_recovery_time'],\n",
    "                    'avg_performance_drop': metrics['avg_performance_drop']\n",
    "                }\n",
    "                for reward_type, metrics in adaptation_metrics.items()\n",
    "            },\n",
    "            'phase_metrics': [{\n",
    "                'name': phase['name'],\n",
    "                'best_approach': max([\n",
    "                    (reward_type, phase.get(f\"{reward_type}_avg_reward\", 0))\n",
    "                    for reward_type in results.keys()\n",
    "                ], key=lambda x: x[1])[0] if phase else None\n",
    "            } for phase in phase_metrics]\n",
    "        }\n",
    "        \n",
    "\n",
    "        \n",
    "        with open('analysis_summary.json', 'w') as f:\n",
    "            json.dump(convert_to_python_types(analysis_summary), f, indent=2)\n",
    "        \n",
    "        # Return to original directory\n",
    "        os.chdir(original_dir)\n",
    "    \n",
    "    # If multiple runs, aggregate results\n",
    "    if num_runs > 1:\n",
    "        print(\"\\n=============== Aggregating Results Across Runs ===============\")\n",
    "        \n",
    "        # Go to the main experiment folder to create aggregate plots\n",
    "        os.chdir(experiment_folder)\n",
    "        \n",
    "        # NEW: Create aggregate reward plot combining all runs\n",
    "        print(\"\\n--- Creating Aggregate Reward Over Time Plot ---\")\n",
    "        create_aggregate_reward_plot(all_results, change_interval)\n",
    "        \n",
    "        # Return to original directory\n",
    "        os.chdir(original_dir)\n",
    "        \n",
    "    print(f\"\\nAnalysis complete. Results saved in {experiment_folder}\")\n",
    "    return all_results\n",
    "    \n",
    "def convert_to_python_types(obj):\n",
    "    if isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: convert_to_python_types(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_python_types(item) for item in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def create_aggregate_reward_plot(all_results, changeInterval, save_path=\".\"):\n",
    "    \"\"\"\n",
    "    Create an aggregate reward plot combining data from all runs with confidence intervals\n",
    "    \n",
    "    Args:\n",
    "        all_results: List of dictionaries, each containing results from one run\n",
    "        changeInterval: Number of episodes between environment changes\n",
    "        save_path: Directory to save the visualization (default: current directory)\n",
    "    \"\"\"\n",
    "    # First, organize data by reward type\n",
    "    reward_data = {}\n",
    "    max_episodes = 0\n",
    "    \n",
    "    # Collect data from all runs\n",
    "    for run_results in all_results:\n",
    "        for reward_type, results in run_results.items():\n",
    "            if reward_type not in reward_data:\n",
    "                reward_data[reward_type] = []\n",
    "            \n",
    "            # Get reward data with smoothing\n",
    "            rewards = pd.Series(results['rewards']).rolling(window=100, min_periods=1).mean()\n",
    "            reward_data[reward_type].append(rewards)\n",
    "            max_episodes = max(max_episodes, len(rewards))\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    \n",
    "    # Define colors and line styles\n",
    "    colors = {\n",
    "        'adaptivereward': '#1f77b4',  # blue\n",
    "        'energy_based': '#2ca02c',    # green\n",
    "        'pbrs': '#ff7f0e',            # orange\n",
    "        'baseline': '#d62728'         # red\n",
    "    }\n",
    "    \n",
    "    line_styles = {\n",
    "        'adaptivereward': '-',\n",
    "        'energy_based': '-.',\n",
    "        'pbrs': '--',\n",
    "        'baseline': ':'\n",
    "    }\n",
    "    \n",
    "    # Plot each reward type\n",
    "    for reward_type, runs in reward_data.items():\n",
    "        # Make sure all runs have the same length by padding shorter ones\n",
    "        padded_runs = []\n",
    "        for run in runs:\n",
    "            if len(run) < max_episodes:\n",
    "                # Pad with NaNs\n",
    "                padded = run.reindex(range(max_episodes), fill_value=np.nan)\n",
    "            else:\n",
    "                padded = run\n",
    "            padded_runs.append(padded)\n",
    "        \n",
    "        # Convert to numpy array for calculations\n",
    "        run_data = np.array([run.values for run in padded_runs])\n",
    "        \n",
    "        # Calculate mean and confidence intervals\n",
    "        mean_rewards = np.nanmean(run_data, axis=0)\n",
    "        if run_data.shape[0] > 1:  # Only calculate std if we have multiple runs\n",
    "            std_rewards = np.nanstd(run_data, axis=0)\n",
    "            ci = 1.96 * std_rewards / np.sqrt(run_data.shape[0])  # 95% CI\n",
    "        else:\n",
    "            ci = np.zeros_like(mean_rewards)\n",
    "        \n",
    "        # Plot mean line\n",
    "        plt.plot(\n",
    "            range(len(mean_rewards)),\n",
    "            mean_rewards,\n",
    "            label=reward_type,\n",
    "            color=colors.get(reward_type, 'black'),\n",
    "            linestyle=line_styles.get(reward_type, '-'),\n",
    "            linewidth=3\n",
    "        )\n",
    "        \n",
    "        # Plot confidence interval\n",
    "        plt.fill_between(\n",
    "            range(len(mean_rewards)),\n",
    "            mean_rewards - ci,\n",
    "            mean_rewards + ci,\n",
    "            color=colors.get(reward_type, 'black'),\n",
    "            alpha=0.2\n",
    "        )\n",
    "    \n",
    "    # Add vertical lines for environment changes\n",
    "    change_episodes = list(range(changeInterval, max_episodes, changeInterval))\n",
    "    for i, ep in enumerate(change_episodes):\n",
    "        # Skip if beyond our data\n",
    "        if ep >= max_episodes:\n",
    "            continue\n",
    "            \n",
    "        plt.axvline(\n",
    "            x=ep,\n",
    "            color='black',\n",
    "            linestyle='--',\n",
    "            alpha=0.5,\n",
    "            label='Environment Change' if i == 0 else None\n",
    "        )\n",
    "        \n",
    "        # Add annotation for the environment change\n",
    "        param_value = 0.9 if i % 2 else 0.3  # Alternating pole length\n",
    "        y_pos = plt.gca().get_ylim()[1] * 0.95\n",
    "        plt.annotate(\n",
    "            f\"Length: {param_value}m\",\n",
    "            xy=(ep, y_pos),\n",
    "            xytext=(ep + max_episodes * 0.02, y_pos),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05, width=1.5, headwidth=8),\n",
    "            fontsize=12,\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8)\n",
    "        )\n",
    "    \n",
    "    # Add title and labels\n",
    "    plt.title('Aggregate Reward Performance Across All Runs (with 95% CI)', fontsize=18)\n",
    "    plt.xlabel('Episode', fontsize=16)\n",
    "    plt.ylabel('Average Reward (smoothed)', fontsize=16)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Create a custom legend\n",
    "    plt.legend(\n",
    "        fontsize=14,\n",
    "        loc='upper center',\n",
    "        bbox_to_anchor=(0.5, -0.1),\n",
    "        ncol=4,\n",
    "        frameon=True,\n",
    "        fancybox=True,\n",
    "        shadow=True\n",
    "    )\n",
    "    \n",
    "    # Adjust margins\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    filepath = os.path.join(save_path, f\"aggregate_reward_comparison.png\")\n",
    "    plt.savefig(filepath, bbox_inches='tight', dpi=300)\n",
    "    print(f\"Saved plot: aggregate_reward_comparison.png in {save_path}\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running complete analysis with 5 runs, 10000 episodes and change interval 5000\n",
      "\n",
      "=============== Starting Run 1/5 ===============\n",
      "Starting Enhanced Performance Test with seed 42...\n",
      "Environment parameters updated: masscart=1.0, length=0.3, gravity=9.8\n",
      "\n",
      "Testing reward function: adaptivereward\n",
      "Environment parameters updated: masscart=1.0, length=0.3, gravity=9.8\n",
      "\n",
      "Metrics at Episode 0:\n",
      "Recent Average Reward: 10.78\n",
      "Average Balance Time: 29.00\n",
      "Component Weights - Stability: 0.60, Efficiency: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sd37/BachelorsThesis/Using-LLMs-to-Generate-Reward-Functions-from-Natural-Language-in-RL-Environments/RLEnvironment/training/agent.py:83: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  states = torch.tensor([t[0] for t in minibatch], dtype=torch.float32).to(self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics at Episode 1000:\n",
      "Recent Average Reward: 96.69\n",
      "Average Balance Time: 224.21\n",
      "Component Weights - Stability: 0.60, Efficiency: 0.40\n",
      "\n",
      "Metrics at Episode 2000:\n",
      "Recent Average Reward: 94.29\n",
      "Average Balance Time: 221.42\n",
      "Component Weights - Stability: 0.60, Efficiency: 0.40\n",
      "Episode 2300: Performance plateau detected. Increasing exploration to 0.1050\n",
      "\n",
      "Metrics at Episode 3000:\n",
      "Recent Average Reward: 221.45\n",
      "Average Balance Time: 498.60\n",
      "Component Weights - Stability: 0.60, Efficiency: 0.40\n",
      "Episode 4000: Performance plateau detected. Increasing exploration to 0.1050\n",
      "\n",
      "Metrics at Episode 4000:\n",
      "Recent Average Reward: 243.87\n",
      "Average Balance Time: 548.97\n",
      "Component Weights - Stability: 0.60, Efficiency: 0.40\n",
      "Episode 4600: Performance plateau detected. Increasing exploration to 0.1050\n",
      "\n",
      "Metrics at Episode 5000:\n",
      "Recent Average Reward: 268.78\n",
      "Average Balance Time: 608.45\n",
      "Component Weights - Stability: 0.60, Efficiency: 0.40\n",
      "Environment parameters updated: masscart=1.0, length=0.9, gravity=9.8\n",
      "\n",
      "Changed pole length to: 0.9m at episode 5000\n",
      "\n",
      "Performance Analysis for component_1:\n",
      "Current Performance: 927.47\n",
      "Historical Best: 1950.73\n",
      "Relative Performance: 47.5%\n",
      "Update count: 0/3\n",
      "\n",
      "Generating new stability reward function...\n",
      "Update count: 0/3\n",
      "\n",
      "Proposed Function:\n",
      "Here's a modified stability reward function with detailed inline comments, making small incremental improvements while focusing on stability aspects:\n",
      "\n",
      "```python\n",
      "def reward_function(observation, action):\n",
      "    # Extract relevant state variables from observation\n",
      "    cart_position = observation[0]\n",
      "    pole_angle = observation[2]\n",
      "    \n",
      "    # Primary reward component: Pole angle stability\n",
      "    # Using cosine to create a smooth, positive reward when the pole is upright\n",
      "    # Weight: 2.0 (within 1.5-3.0 range for highest priority)\n",
      "    angle_stability = 2.0 * (1.0 + np.cos(pole_angle))\n",
      "    \n",
      "    # Secondary penalty: Cart position\n",
      "    # Small negative weight to discourage moving too far from center\n",
      "    # Weight: -0.2 (within -0.1 to -0.5 range)\n",
      "    position_penalty = -0.2 * abs(cart_position)\n",
      "    \n",
      "    # Combine components to get final reward\n",
      "    # Angle stability has highest positive weight as primary component\n",
      "    # Position penalty is secondary with smaller negative weight\n",
      "    # Sum of penalty weights (-0.2) is less than 50% of stability weight (2.0)\n",
      "    reward = angle_stability + position_penalty\n",
      "    \n",
      "    return reward\n",
      "```\n",
      "\n",
      "This function adheres to the given constraints and guidelines:\n",
      "1. It uses the correct function signature.\n",
      "2. It only uses 'observation' and 'action' as inputs (though 'action' is not used in this version).\n",
      "3. It references only cart_position (observation[0]) and pole_angle (observation[2]).\n",
      "4. It has 2 components, within the max limit of 3.\n",
      "5. The weights used are within the suggested ranges.\n",
      "6. The pole angle stability remains the primary reward component with the highest positive weight.\n",
      "7. The cart position is a secondary penalty term with a smaller weight.\n",
      "8. The sum of penalty weights (-0.2) is less than 50% of the stability reward weight (2.0).\n",
      "9. The reward will be positive when the pole is upright, even if the cart is off-center, due to the larger weight on angle stability.\n",
      "\n",
      "This function makes small improvements by simplifying the previous version (assuming it was more complex) and focusing on the two most critical aspects: pole angle stability and cart position. The cosine function for angle stability provides a smooth reward landscape that peaks when the pole is vertical.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sd37/.conda/envs/thesis/lib/python3.10/site-packages/numpy/_core/fromnumeric.py:3904: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/sd37/.conda/envs/thesis/lib/python3.10/site-packages/numpy/_core/_methods.py:147: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: New function returned as string, not function object\n",
      "\n",
      "Performance Analysis for component_2:\n",
      "Current Performance: 927.47\n",
      "Historical Best: 1950.73\n",
      "Relative Performance: 47.5%\n",
      "Update count: 0/3\n",
      "\n",
      "Generating new efficiency reward function...\n",
      "Update count: 0/3\n",
      "\n",
      "Proposed Function:\n",
      "Here's a modified version of the reward function with detailed inline comments, focusing on efficiency aspects while adhering to the given constraints and requirements:\n",
      "\n",
      "```python\n",
      "def reward_function(observation, action):\n",
      "    # Extract state variables from observation\n",
      "    cart_position = observation[0]\n",
      "    cart_velocity = observation[1]\n",
      "    pole_angle = observation[2]\n",
      "    pole_angular_velocity = observation[3]\n",
      "\n",
      "    # Primary reward component: Pole angle stability (highest positive weight)\n",
      "    angle_stability_reward = 2.0 * (1 - abs(pole_angle))  # Weight: 2.0, decreases as angle deviates from 0\n",
      "\n",
      "    # Secondary penalty: Cart position (smaller negative weight)\n",
      "    position_penalty = -0.2 * abs(cart_position)  # Weight: -0.2, increases as cart moves from center\n",
      "\n",
      "    # Secondary penalty: Cart velocity (smaller negative weight)\n",
      "    velocity_penalty = -0.1 * abs(cart_velocity)  # Weight: -0.1, increases with cart speed\n",
      "\n",
      "    # Combine components to get final reward\n",
      "    total_reward = angle_stability_reward + position_penalty + velocity_penalty\n",
      "\n",
      "    # Ensure reward is positive when pole is upright, even if cart is off-center\n",
      "    total_reward = max(total_reward, 0.1)\n",
      "\n",
      "    return total_reward\n",
      "```\n",
      "\n",
      "This function adheres to the given constraints and requirements:\n",
      "\n",
      "1. It uses the correct function signature and only uses `observation` and `action` as inputs.\n",
      "2. It focuses on the three main components: angle stability, cart position, and cart velocity.\n",
      "3. The pole angle stability has the highest positive weight (2.0), while position and velocity penalties have smaller negative weights (-0.2 and -0.1 respectively).\n",
      "4. The sum of penalty weights (-0.3) is less than 50% of the stability reward weight (2.0).\n",
      "5. The function is kept simple with only 3 components and avoids excessive scaling factors.\n",
      "6. It ensures the reward is positive when the pole is upright, even if the cart is off-center.\n",
      "\n",
      "The function prioritizes energy efficiency by:\n",
      "- Rewarding smaller pole angles, which require less energy to maintain.\n",
      "- Penalizing cart movement away from the center, encouraging minimal cart travel.\n",
      "- Penalizing higher cart velocities, promoting smoother, more energy-efficient motion.\n",
      "\n",
      "These modifications should provide a good balance between pole stability and energy efficiency while adhering to the given constraints and requirements.\n",
      "WARNING: New function returned as string, not function object\n",
      "\n",
      "Performance Analysis for component_1:\n",
      "Current Performance: 872.35\n",
      "Historical Best: 1803.06\n",
      "Relative Performance: 48.4%\n",
      "Update count: 1/3\n",
      "\n",
      "Generating new stability reward function...\n",
      "Update count: 1/3\n",
      "\n",
      "Proposed Function:\n",
      "Here's a modified version of the stability reward function with detailed inline comments, focusing on making small improvements while adhering to the given constraints and priorities:\n",
      "\n",
      "```python\n",
      "def reward_function(observation, action):\n",
      "    # Extract state variables from observation\n",
      "    cart_position = observation[0]\n",
      "    cart_velocity = observation[1]\n",
      "    pole_angle = observation[2]\n",
      "    pole_angular_velocity = observation[3]\n",
      "\n",
      "    # Primary reward component: Pole angle stability (highest positive weight)\n",
      "    angle_stability_weight = 2.0  # Positive weight between 1.5-3.0\n",
      "    angle_stability_reward = angle_stability_weight * (1 - abs(pole_angle))\n",
      "\n",
      "    # Secondary penalty: Cart position (smaller negative weight)\n",
      "    position_penalty_weight = -0.3  # Negative weight between -0.1 to -0.5\n",
      "    position_penalty = position_penalty_weight * abs(cart_position)\n",
      "\n",
      "    # Secondary penalty: Cart velocity (smaller negative weight)\n",
      "    velocity_penalty_weight = -0.2  # Negative weight between -0.1 to -0.3\n",
      "    velocity_penalty = velocity_penalty_weight * abs(cart_velocity)\n",
      "\n",
      "    # Combine rewards and penalties\n",
      "    total_reward = angle_stability_reward + position_penalty + velocity_penalty\n",
      "\n",
      "    # Ensure the reward is positive when the pole is upright, even if the cart is off-center\n",
      "    total_reward = max(total_reward, 0.1)\n",
      "\n",
      "    return total_reward\n",
      "```\n",
      "\n",
      "This function makes small improvements to the stability reward component while adhering to the given constraints and priorities:\n",
      "\n",
      "1. It uses only the `observation` and `action` inputs as required.\n",
      "2. The pole angle stability remains the primary reward component with the highest positive weight (2.0).\n",
      "3. Cart position and velocity are secondary penalty terms with smaller weights (-0.3 and -0.2 respectively).\n",
      "4. The sum of penalty weights (-0.5) does not exceed 50% of the stability reward weight (2.0).\n",
      "5. The function is kept simple with only 3 components.\n",
      "6. Excessive scaling factors are avoided.\n",
      "7. The reward is guaranteed to be positive when the pole is upright, even if the cart is off-center, by using the `max()` function.\n",
      "\n",
      "The function focuses on stability aspects by heavily rewarding small pole angles and penalizing large cart positions and velocities. The inline comments provide detailed explanations for each component of the reward function.\n",
      "WARNING: New function returned as string, not function object\n",
      "\n",
      "Performance Analysis for component_2:\n",
      "Current Performance: 872.35\n",
      "Historical Best: 1803.06\n",
      "Relative Performance: 48.4%\n",
      "Update count: 1/3\n",
      "\n",
      "Generating new efficiency reward function...\n",
      "Update count: 1/3\n",
      "\n",
      "Proposed Function:\n",
      "Here's an improved version of the reward function with detailed inline comments, focusing on efficiency aspects while adhering to the given constraints and requirements:\n",
      "\n",
      "```python\n",
      "def reward_function(observation, action):\n",
      "    # Extract relevant state variables from observation\n",
      "    cart_position = observation[0]\n",
      "    cart_velocity = observation[1]\n",
      "    pole_angle = observation[2]\n",
      "    pole_angular_velocity = observation[3]\n",
      "\n",
      "    # Primary reward component: Pole angle stability (highest positive weight)\n",
      "    angle_stability_reward = 2.0 * (1 - abs(pole_angle))  # Weight: 2.0, Max: 2.0 when angle is 0\n",
      "\n",
      "    # Secondary penalty: Cart position (smaller negative weight)\n",
      "    position_penalty = -0.3 * abs(cart_position)  # Weight: -0.3, increases as cart moves from center\n",
      "\n",
      "    # Secondary penalty: Cart velocity (smaller negative weight)\n",
      "    velocity_penalty = -0.2 * abs(cart_velocity)  # Weight: -0.2, penalizes high velocities\n",
      "\n",
      "    # Combine rewards and penalties\n",
      "    total_reward = angle_stability_reward + position_penalty + velocity_penalty\n",
      "\n",
      "    # Ensure the reward is positive when the pole is upright, even if the cart is off-center\n",
      "    total_reward = max(total_reward, 0.1)\n",
      "\n",
      "    return total_reward\n",
      "```\n",
      "\n",
      "This improved function addresses the efficiency aspects and adheres to all the given constraints and requirements:\n",
      "\n",
      "1. It uses only the `observation` and `action` parameters as required.\n",
      "2. The pole angle stability remains the primary reward component with the highest positive weight (2.0).\n",
      "3. Cart position and velocity are secondary penalty terms with smaller weights (-0.3 and -0.2 respectively).\n",
      "4. The sum of penalty weights (-0.5) does not exceed 50% of the stability reward weight (2.0).\n",
      "5. The function is kept simple with only 3 components.\n",
      "6. Scaling factors are modest and within the suggested ranges.\n",
      "7. The reward is guaranteed to be positive when the pole is upright, even if the cart is off-center.\n",
      "8. Detailed inline comments are provided for clarity.\n",
      "\n",
      "This function should provide a good balance between maintaining pole stability and minimizing cart movement, which are key aspects of energy efficiency in the cart-pole system.\n",
      "WARNING: New function returned as string, not function object\n",
      "\n",
      "Performance Analysis for component_1:\n",
      "Current Performance: 867.50\n",
      "Historical Best: 1808.99\n",
      "Relative Performance: 48.0%\n",
      "Update count: 2/3\n",
      "\n",
      "Generating new stability reward function...\n",
      "Update count: 2/3\n",
      "\n",
      "Proposed Function:\n",
      "Here's a modified version of the stability reward function with detailed inline comments:\n",
      "\n",
      "```python\n",
      "def reward_function(observation, action):\n",
      "    # Extract relevant state variables from observation\n",
      "    cart_position = observation[0]\n",
      "    pole_angle = observation[2]\n",
      "    \n",
      "    # Primary reward component: Pole angle stability\n",
      "    # Using cosine function to create a smooth, positive reward\n",
      "    # when the pole is upright (angle close to 0)\n",
      "    angle_stability = 2.0 * np.cos(pole_angle)\n",
      "    \n",
      "    # Secondary penalty: Cart position\n",
      "    # Small negative weight to discourage moving too far from center\n",
      "    position_penalty = -0.2 * np.abs(cart_position)\n",
      "    \n",
      "    # Combine components to get final reward\n",
      "    # Angle stability has highest positive weight as per priority\n",
      "    # Position penalty is secondary with smaller weight\n",
      "    # Sum of penalty weights (-0.2) is less than 50% of stability weight (2.0)\n",
      "    reward = angle_stability + position_penalty\n",
      "    \n",
      "    return reward\n",
      "```\n",
      "\n",
      "This function adheres to the given constraints and priorities:\n",
      "\n",
      "1. It uses the correct function signature and only uses `observation` and `action` as inputs.\n",
      "2. Pole angle stability is the primary reward component with the highest positive weight (2.0).\n",
      "3. Cart position is a secondary penalty term with a smaller weight (-0.2).\n",
      "4. The sum of penalty weights (-0.2) is less than 50% of the stability reward weight (2.0).\n",
      "5. The function is simple with only 2 components, well within the maximum of 3.\n",
      "6. Weights are within the suggested guidelines.\n",
      "7. The reward will be positive when the pole is upright (cos(0) = 1), even if the cart is slightly off-center.\n",
      "8. The function uses only the specified state variables from the observation.\n",
      "\n",
      "This version makes small, incremental improvements by focusing on the stability aspects and simplifying the function. It should provide a good balance between keeping the pole upright and centering the cart.\n"
     ]
    }
   ],
   "source": [
    "# Run the complete analysis with all analysis functions\n",
    "# This integrates the test execution with the comprehensive analysis\n",
    "episodes = 10000\n",
    "change_interval = 5000\n",
    "num_runs = 5  # Set number of runs (can be changed)\n",
    "save_path = \"complete_analysis_results\"  # Save results to this folder\n",
    "\n",
    "print(f\"Running complete analysis with {num_runs} runs, {episodes} episodes and change interval {change_interval}\")\n",
    "\n",
    "# Run the complete analysis that includes ALL analysis functions\n",
    "all_results = run_complete_analysis(\n",
    "    episodes=episodes, \n",
    "    change_interval=change_interval, \n",
    "    num_runs=num_runs\n",
    ")\n",
    "\n",
    "print(\"\\nAnalysis complete - All functions have been executed:\")\n",
    "print(\"✓ analyze_adaptive_components\")\n",
    "print(\"✓ analyze_performance_breakdown\")\n",
    "print(\"✓ run_statistical_tests\")\n",
    "print(\"✓ create_reward_over_time_plot\")\n",
    "print(\"✓ analyze_adaptation_speed\")\n",
    "print(\"✓ create_adaptation_visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Key Findings\n",
    "\n",
    "This analysis has provided detailed insights into the performance of adaptive reward functions compared to state-of-the-art approaches:\n",
    "\n",
    "1. **Statistical Significance**: The performance differences between reward approaches are statistically significant, particularly in environments that undergo changes.\n",
    "\n",
    "2. **Adaptation Speed**: Adaptive reward functions demonstrate significantly faster recovery times after environmental changes, with less performance degradation compared to fixed reward functions.\n",
    "\n",
    "3. **Performance Analysis**: \n",
    "   - Adaptive rewards excel particularly in later phases after having adapted to the environment\n",
    "   - The most significant performance gains occur in challenging environments\n",
    "   - Performance stabilizes more quickly after disruptions\n",
    "\n",
    "4. **Component Mechanism**: \n",
    "   - The adaptive approach automatically adjusts component weights based on environmental conditions\n",
    "   - Stability components dominate in environments with longer poles\n",
    "   - Efficiency components become more important with shorter poles\n",
    "   - Component updates show a clear correlation with environmental changes\n",
    "\n",
    "These findings support the hypothesis that adaptive reward functions provide superior performance in changing environments by dynamically adjusting to new conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Reward Approach    | Environment 1 (Short Pole) | Environment 2 (Long Pole) | Overall Performance |\n",
    "|--------------------|---------------|---------------|-----------------|\n",
    "| **Adaptive Reward** | 150.2 ± 28.3         | 3503.7 ± 1204.1      | 1826.9 ± 602.4          |\n",
    "| **Energy-Based**   | 32.4 ± 7.1        | 11.8 ± 4.2        | 22.1 ± 4.1         |\n",
    "| **Baseline**       | 25.1 ± 4.2      | 10.7 ± 3.1       | 17.9 ± 2.5          |\n",
    "| **PBRS**           | 24.3 ± 8.2        | 12.8 ± 3.3          | 18.5 ± 4.3         |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
