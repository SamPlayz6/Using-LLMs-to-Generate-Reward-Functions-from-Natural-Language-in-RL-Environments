{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Also Implement these Ideas: vvvv\n",
    "\n",
    "# Have many possible starting points. Many starting reward functions. Test if they start to perform.\n",
    "\n",
    "# This bit will be an exploration of the weaknesses of the architecture, in turn checking robustness.\n",
    "\n",
    "# This could compose of creating many 'bad' initial reward functions and seeing if it can re-adjust.\n",
    "\n",
    "# I would also need to investigate the performance changes between changes of reward functions and see over time if this actually improves performance regularly, is it robust over many changes in this sense?\n",
    "\n",
    "\n",
    "#Effective waiting time.\n",
    "#Take the current implementation and change it to **measure balance time as a performance metric**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Here I am simply investigating the robustness of performance when adapting reward functions**\n",
    "\n",
    "-> Does the performance have an unexpected jumps? Does the performance drop? Are the reward functions sensible in the context?\n",
    "\n",
    "I do this in three stages of tests:\n",
    "1. *Test a badly initilized reward function. See how it adaptively updates.*\n",
    "2. *Start with good intial composite reward functions, but vary environment states over time. See if it is robust to varying environment variables. This includes a \"waiting time\" function, which chooses when to alter the composite reward functions.*\n",
    "3. *Altered pattern of enironment changes from stage 2.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Robustness Definition**\n",
    "In this experiment I have defined Robustness as a system's ability to:\n",
    "\n",
    "- Maintain acceptable performance levels during environmental changes\n",
    "- Adapt its reward function appropriately to new conditions\n",
    "- Recover from performance drops within a reasonable timeframe\n",
    "\n",
    "\n",
    "### **Robustness Metric**\n",
    "- Performance Stability (PS = minPerformance / averagePerformance)\n",
    "- Recovery Speed (RS = episodesToRecover / maxAcceptableRecovery)\n",
    "- Adaption Effectiveness (AE = postChangePerformance / preChangePerformance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Common Imports and Setup\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# plt.style.use('seaborn')\n",
    "# sns.set_palette(\"husl\")\n",
    "\n",
    "current_dir = os.getcwd()  \n",
    "project_root = str(Path(current_dir).parent.parent)\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# Initialize environment and device\n",
    "from AdaptiveRewardFunctionLearning.Prompts.prompts import device, apiKey,modelName\n",
    "\n",
    "#Cu stomCartPoleEnv\n",
    "from RLEnvironment.env import CustomCartPoleEnv\n",
    "#RewardUpdateSystem\n",
    "from AdaptiveRewardFunctionLearning.RewardGeneration.rewardCritic import RewardUpdateSystem\n",
    "#DQLearningAgent\n",
    "from RLEnvironment.training.agent import DQLearningAgent\n",
    "from RLEnvironment.training.training import trainDQLearning\n",
    "\n",
    "#DynamicRewardFunction\n",
    "from AdaptiveRewardFunctionLearning.RewardGeneration.rewardCodeGeneration import dynamicRewardFunction\n",
    "\n",
    "#import\n",
    "from AdaptiveRewardFunctionLearning.Visualisation.trainingTestFunctions import (\n",
    "    runEpisode,\n",
    "    detectJumps,\n",
    "    analyzeRewardSensibility,\n",
    "    performUpdate,\n",
    "    updateCompositeRewardFunction,\n",
    "    plotExperimentResults,\n",
    "    savePlot\n",
    ")\n",
    "\n",
    "# Set bad initial reward function\n",
    "def badReward(observation, action):\n",
    "    x, xDot, angle, angleDot = observation\n",
    "    return float(-5.0 * abs(x) + 0.1 * np.cos(angle) - 0.1 * abs(xDot))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Experiment 1: Bad Initialization Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Experiment 1 - Bad Initialization Test\n",
    "def runBadInitializationTest(episodes=1000):\n",
    "    print(\"Starting Bad Initialization Test...\")\n",
    "    \n",
    "    env = gym.make('CartPole-v1')\n",
    "\n",
    "\n",
    "    env = CustomCartPoleEnv(env)\n",
    "    \n",
    "    env.setRewardFunction(badReward)\n",
    "\n",
    "    updateSystem = RewardUpdateSystem(apiKey, modelName)\n",
    "\n",
    "    agent = DQLearningAgent(env, 4, 2, device)\n",
    "    \n",
    "    rewards = []\n",
    "    balance_times = [] \n",
    "    metrics = {}\n",
    "    rewardChangeEpisodes = []\n",
    "    \n",
    "\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        episode_reward = 0\n",
    "        timesteps = 0 \n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.chooseAction(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            episode_reward += reward\n",
    "            timesteps += 1 \n",
    "            \n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "\n",
    "        rewards.append(episode_reward)\n",
    "        balance_times.append(timesteps)\n",
    "        \n",
    "        if episode % 100 == 0:\n",
    "            metrics[episode] = {\n",
    "                'jumps': detectJumps(rewards),\n",
    "                'averageReward': np.mean(rewards[-100:]),\n",
    "                'sensibility': analyzeRewardSensibility(env.rewardFunction),\n",
    "                'averageBalanceTime': np.mean(balance_times[-100:])\n",
    "            }\n",
    "            \n",
    "\n",
    "        # print(\"Episode: \" + str(episode))\n",
    "        if updateSystem.waitingTimeConstant(episode, 100):\n",
    "            if performUpdate(env, updateSystem, episode):  # Check if update happened\n",
    "                rewardChangeEpisodes.append(episode)\n",
    "\n",
    "        # rewardChangeEpisodes = updateSystem.getUpdateEpisodes()\n",
    "    \n",
    "    # Plot results with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "    # Plot rewards\n",
    "    ax1.plot(rewards, alpha=0.6, label='Episode Reward')\n",
    "    ax1.plot(pd.Series(rewards).rolling(50).mean(), label='50-Episode Moving Average', linewidth=2)\n",
    "\n",
    "    # Add horizontal lines for reward averages\n",
    "    for i in range(0, len(rewards), 100):\n",
    "        avg_reward = np.mean(rewards[i:i+100])\n",
    "        ax1.axhline(y=avg_reward, xmin=i/len(rewards), xmax=(i+100)/len(rewards), \n",
    "                color='r', linestyle='--', alpha=0.5)\n",
    "\n",
    "    # Add vertical lines for reward function changes in reward plot\n",
    "    for ep in rewardChangeEpisodes:\n",
    "        ax1.axvline(x=ep, color='g', linestyle='--', alpha=0.3,\n",
    "                label='Reward Update' if ep == rewardChangeEpisodes[0] else None)\n",
    "\n",
    "    ax1.set_title('Rewards Over Time')\n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.set_ylabel('Total Reward')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Plot balance times\n",
    "    ax2.plot(balance_times, alpha=0.6, label='Balance Time')\n",
    "    ax2.plot(pd.Series(balance_times).rolling(50).mean(), label='50-Episode Moving Average', linewidth=2)\n",
    "\n",
    "    # Add horizontal lines for balance time averages\n",
    "    for i in range(0, len(balance_times), 100):\n",
    "        avg_time = np.mean(balance_times[i:i+100])\n",
    "        ax2.axhline(y=avg_time, xmin=i/len(balance_times), xmax=(i+100)/len(balance_times), \n",
    "                color='r', linestyle='--', alpha=0.5)\n",
    "\n",
    "    # Add vertical lines for reward function changes in balance time plot\n",
    "    for ep in rewardChangeEpisodes:\n",
    "        ax2.axvline(x=ep, color='g', linestyle='--', alpha=0.3,\n",
    "                label='Reward Update' if ep == rewardChangeEpisodes[0] else None)\n",
    "\n",
    "    ax2.set_title('Balance Time Over Episodes')\n",
    "    ax2.set_xlabel('Episode')\n",
    "    ax2.set_ylabel('Timesteps')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "    savePlot(fig, \"BadInitialization\",\"RobustnessResults\", plotType=\"training_results\")\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return rewards, metrics, balance_times\n",
    "\n",
    "rewards, metrics, balance_times = runBadInitializationTest(1000)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Experiment 2 - Environment Variation Test**\n",
    "\n",
    "\n",
    "All relationships connections can be shown mathematically. So if I can show that it can optimally adapt for change in one environment variable it should abstract to other environment changes also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runEnvironmentVariationTest(episodes=1000, changeInterval=200,length_changes=[0.5, 1, 1.5, 2, 2.5]):\n",
    "    print(\"Starting Environment Variation Test...\")\n",
    "    \n",
    "    # Define length changes (5 values)\n",
    "    \n",
    "    current_length_idx = 0\n",
    "    \n",
    "    # Setup environment with initial configuration\n",
    "    env = gym.make('CartPole-v1')\n",
    "    env = CustomCartPoleEnv(env)\n",
    "    env.setEnvironmentParameters(masscart=1.0, length=length_changes[0], gravity=9.8)\n",
    "\n",
    "\n",
    "    # Initialize update system and agent\n",
    "    updateSystem = RewardUpdateSystem(apiKey, modelName)\n",
    "    agent = DQLearningAgent(env, 4, 2, device)\n",
    "    \n",
    "    # Initialize metrics and tracking variables\n",
    "    metrics = {}\n",
    "    episode_rewards = []\n",
    "    episode_balance_times = []\n",
    "    reward_change_episodes = []  # Track reward function changes\n",
    "    \n",
    "    # Create callback for length changes\n",
    "    def onEpisodeEnd(env, updateSystem, episode, reward, steps):\n",
    "        nonlocal current_length_idx, episode_rewards, episode_balance_times, reward_change_episodes\n",
    "    \n",
    "        # Record reward and balance time\n",
    "        episode_rewards.append(reward)\n",
    "        episode_balance_times.append(steps)\n",
    "        \n",
    "        # Build metrics for composite reward update check\n",
    "        metrics = {\n",
    "            'currentEpisode': episode,\n",
    "            'recentRewards': episode_rewards[-100:] if len(episode_rewards) > 100 else episode_rewards,\n",
    "            'averageBalanceTime': np.mean(episode_balance_times[-100:]) if episode_balance_times else 0,\n",
    "            'balanceTimeVariance': np.var(episode_balance_times[-100:]) if len(episode_balance_times) > 1 else 0\n",
    "        }\n",
    "        \n",
    "        # Check for composite reward updates\n",
    "        updateCompositeRewardFunction(env, updateSystem, metrics, dynamicRewardFunction)\n",
    "        \n",
    "        # Check if any composite reward function was updated\n",
    "        if hasattr(dynamicRewardFunction, 'compositeHistory'):\n",
    "            latest_updates = [\n",
    "                update['episode'] for update in dynamicRewardFunction.compositeHistory \n",
    "                if update['episode'] == episode\n",
    "            ]\n",
    "            if latest_updates:\n",
    "                reward_change_episodes.append(episode)\n",
    "                print(f\"\\nReward function updated at episode {episode}\")\n",
    "        \n",
    "        if episode % changeInterval == 0 and episode > 0:\n",
    "            current_length_idx = (current_length_idx + 1) % len(length_changes)\n",
    "            new_length = length_changes[current_length_idx]\n",
    "            env.setEnvironmentParameters(length=new_length)\n",
    "            print(f\"\\nChanged pole length to: {new_length}m at episode {episode}\")\n",
    "        \n",
    "        # Record metrics every 50 episodes (for display purposes only)\n",
    "        if episode % 50 == 0:\n",
    "            metrics[episode] = {\n",
    "                'jumps': detectJumps(episode_rewards),\n",
    "                'averageReward': np.mean(episode_rewards[-100:]) if episode_rewards else 0,\n",
    "                'sensibility': analyzeRewardSensibility(env.rewardFunction),\n",
    "                'averageBalanceTime': np.mean(episode_balance_times[-100:]) if episode_balance_times else 0,\n",
    "                'currentLength': length_changes[current_length_idx]\n",
    "            }\n",
    "            print(f\"Episode {episode}: Avg Balance Time = {metrics[episode]['averageBalanceTime']:.2f}\")\n",
    "    \n",
    "    # Train the agent\n",
    "    agent, env, rewards = trainDQLearning(\n",
    "        agent=agent,\n",
    "        env=env,\n",
    "        numEpisodes=episodes,\n",
    "        updateSystem=updateSystem,\n",
    "        onEpisodeEnd=lambda env, updateSystem, episode, reward, steps: onEpisodeEnd(env, updateSystem, episode, reward, steps)\n",
    "    )\n",
    "    \n",
    "    # Create length history\n",
    "    length_history = []\n",
    "    for episode in range(episodes):\n",
    "        idx = (episode // changeInterval) % len(length_changes)\n",
    "        length_history.append(length_changes[idx])\n",
    "    \n",
    "    # Plot results\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 15))\n",
    "    \n",
    "    # Plot rewards\n",
    "    ax1.plot(episode_rewards, alpha=0.6, label='Episode Reward')\n",
    "    ax1.plot(pd.Series(episode_rewards).rolling(50).mean(), \n",
    "             label='50-Episode Moving Average', linewidth=2)\n",
    "    \n",
    "    # Add vertical lines for length changes (red)\n",
    "    for ep in range(changeInterval, episodes, changeInterval):\n",
    "        ax1.axvline(x=ep, color='r', linestyle='--', alpha=0.3, \n",
    "                   label='Length Change' if ep == changeInterval else None)\n",
    "    \n",
    "    # Add vertical lines for reward function changes (green)\n",
    "    for ep in reward_change_episodes:\n",
    "        ax1.axvline(x=ep, color='g', linestyle='--', alpha=0.3, \n",
    "                   label='Reward Update' if ep == reward_change_episodes[0] else None)\n",
    "    \n",
    "    ax1.set_title('Rewards Over Time with Length and Reward Function Changes')\n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.set_ylabel('Total Reward')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot balance times\n",
    "    ax2.plot(episode_balance_times, alpha=0.6, label='Balance Time')\n",
    "    ax2.plot(pd.Series(episode_balance_times).rolling(50).mean(), \n",
    "             label='50-Episode Moving Average', linewidth=2)\n",
    "    \n",
    "    # Add vertical lines for length changes (red)\n",
    "    for ep in range(changeInterval, episodes, changeInterval):\n",
    "        ax2.axvline(x=ep, color='r', linestyle='--', alpha=0.3,\n",
    "                   label='Length Change' if ep == changeInterval else None)\n",
    "    \n",
    "    # Add vertical lines for reward function changes (green)\n",
    "    for ep in reward_change_episodes:\n",
    "        ax2.axvline(x=ep, color='g', linestyle='--', alpha=0.3,\n",
    "                   label='Reward Update' if ep == reward_change_episodes[0] else None)\n",
    "    \n",
    "    ax2.set_title('Balance Time Over Episodes')\n",
    "    ax2.set_xlabel('Episode')\n",
    "    ax2.set_ylabel('Timesteps')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    # Plot pole length changes\n",
    "    ax3.plot(length_history, label='Pole Length')\n",
    "    ax3.set_title('Pole Length Over Episodes')\n",
    "    ax3.set_xlabel('Episode')\n",
    "    ax3.set_ylabel('Length (m)')\n",
    "    ax3.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    savePlot(fig, \"robustnessAnalysis\", \"RobustnessExperiment2\")\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"\\nExperiment completed!\")\n",
    "    \n",
    "    return {\n",
    "        'rewards': episode_rewards,\n",
    "        'balance_times': episode_balance_times,\n",
    "        'metrics': metrics,\n",
    "        'length_history': length_history,\n",
    "        'reward_changes': reward_change_episodes\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = runEnvironmentVariationTest(10000, 2000, length_changes=[0.5, 0.75, 1.0, 1.25, 1.5] )# Pole lengths in meters\n",
    "    \n",
    "    # Print final performance statistics\n",
    "    final_avg_reward = np.mean(results['rewards'][-100:])\n",
    "    final_avg_balance = np.mean(results['balance_times'][-100:])\n",
    "    print(f\"\\nFinal performance:\")\n",
    "    print(f\"Average Reward: {final_avg_reward:.2f}\")\n",
    "    print(f\"Average Balance Time: {final_avg_balance:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Experiment 3 -**\n",
    "\n",
    "Potential Improvments\n",
    "- Different Sequences of pole lengths\n",
    "- Testing with different initial conditions (Poor composite reward functions)\n",
    "- Add robustness metrics\n",
    "\n",
    "Considering Adding:\n",
    "- Confidence intervals for robustness metrics (Most papers only really seem to go this far)\n",
    "- Statistical significance tests between different approaches\n",
    "- Variance analysis across multiple runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    results = runEnvironmentVariationTest(10000, 2000, length_changes=[0.5, 6.4, 5, 1.25, 9.5] )# Pole lengths in meters\n",
    "    \n",
    "    # Print final performance statistics\n",
    "    final_avg_reward = np.mean(results['rewards'][-100:])\n",
    "    final_avg_balance = np.mean(results['balance_times'][-100:])\n",
    "    print(f\"\\nFinal performance:\")\n",
    "    print(f\"Average Reward: {final_avg_reward:.2f}\")\n",
    "    print(f\"Average Balance Time: {final_avg_balance:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
