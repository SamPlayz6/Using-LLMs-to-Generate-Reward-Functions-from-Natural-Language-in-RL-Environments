{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Also Implement these Ideas: vvvv\n",
    "\n",
    "# Have many possible starting points. Many starting reward functions. Test if they start to perform.\n",
    "\n",
    "# This bit will be an exploration of the weaknesses of the architecture, in turn checking robustness.\n",
    "\n",
    "# This could compose of creating many 'bad' initial reward functions and seeing if it can re-adjust.\n",
    "\n",
    "# I would also need to investigate the performance changes between changes of reward functions and see over time if this actually improves performance regularly, is it robust over many changes in this sense?\n",
    "\n",
    "\n",
    "#Effective waiting time.\n",
    "#Take the current implementation and change it to **measure balance time as a performance metric**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Here I am simply investigating the robustness of performance when adapting reward functions**\n",
    "\n",
    "-> Does the performance have an unexpected jumps? Does the performance drop? Are the reward functions sensible in the context?\n",
    "\n",
    "I do this in three stages of tests:\n",
    "1. *Test a badly initilized composite reward function. See how it adaptively updates.*\n",
    "2. *Start with good intial composite reward functions, but with varying environment states. See if it is robust to varying environment variables.*\n",
    "3. *Both a bad intial set of composite reward function and also varying environment variables.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Common Imports and Setup\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# plt.style.use('seaborn')\n",
    "# sns.set_palette(\"husl\")\n",
    "\n",
    "current_dir = os.getcwd()  \n",
    "project_root = str(Path(current_dir).parent.parent)\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# Initialize environment and device\n",
    "from AdaptiveRewardFunctionLearning.Prompts.prompts import device, apiKey,modelName\n",
    "\n",
    "#Cu stomCartPoleEnv\n",
    "from RLEnvironment.env import CustomCartPoleEnv\n",
    "#RewardUpdateSystem\n",
    "from AdaptiveRewardFunctionLearning.RewardGeneration.rewardCritic import RewardUpdateSystem\n",
    "#DQLearningAgent\n",
    "from RLEnvironment.training.agent import DQLearningAgent\n",
    "from RLEnvironment.training.training import trainDQLearning\n",
    "\n",
    "#DynamicRewardFunction\n",
    "from AdaptiveRewardFunctionLearning.RewardGeneration.rewardCodeGeneration import dynamicRewardFunction\n",
    "\n",
    "#import\n",
    "from AdaptiveRewardFunctionLearning.Visualisation.trainingTestFunctions import (\n",
    "    runEpisode,\n",
    "    detectJumps,\n",
    "    analyzeRewardSensibility,\n",
    "    performUpdate,\n",
    "    plotExperimentResults,\n",
    "    savePlot\n",
    ")\n",
    "\n",
    "# Set bad initial reward function\n",
    "def badReward(observation, action):\n",
    "    x, xDot, angle, angleDot = observation\n",
    "    return float(-5.0 * abs(x) + 0.1 * np.cos(angle) - 0.1 * abs(xDot))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Experiment 1: Bad Initialization Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Bad Initialization Test...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'CustomCartPoleEnv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 92\u001b[0m\n\u001b[0;32m     88\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rewards, metrics, balance_times\n\u001b[1;32m---> 92\u001b[0m rewards, metrics, balance_times \u001b[38;5;241m=\u001b[39m \u001b[43mrunBadInitializationTest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m  \n",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m, in \u001b[0;36mrunBadInitializationTest\u001b[1;34m(episodes)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting Bad Initialization Test...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCartPole-v1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mCustomCartPoleEnv\u001b[49m(env)\n\u001b[0;32m     10\u001b[0m env\u001b[38;5;241m.\u001b[39msetRewardFunction(badReward)\n\u001b[0;32m     12\u001b[0m updateSystem \u001b[38;5;241m=\u001b[39m RewardUpdateSystem(apiKey, modelName)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CustomCartPoleEnv' is not defined"
     ]
    }
   ],
   "source": [
    "# Cell 2: Experiment 1 - Bad Initialization Test\n",
    "def runBadInitializationTest(episodes=1000):\n",
    "    print(\"Starting Bad Initialization Test...\")\n",
    "    \n",
    "    env = gym.make('CartPole-v1')\n",
    "\n",
    "\n",
    "    env = CustomCartPoleEnv(env)\n",
    "    \n",
    "    env.setRewardFunction(badReward)\n",
    "\n",
    "    updateSystem = RewardUpdateSystem(apiKey, modelName)\n",
    "\n",
    "    agent = DQLearningAgent(env, 4, 2, device)\n",
    "    \n",
    "    rewards = []\n",
    "    balance_times = [] \n",
    "    metrics = {}\n",
    "\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        episode_reward = 0\n",
    "        timesteps = 0 \n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.chooseAction(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            episode_reward += reward\n",
    "            timesteps += 1 \n",
    "            \n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "\n",
    "        rewards.append(episode_reward)\n",
    "        balance_times.append(timesteps)\n",
    "        \n",
    "        if episode % 100 == 0:\n",
    "            metrics[episode] = {\n",
    "                'jumps': detectJumps(rewards),\n",
    "                'averageReward': np.mean(rewards[-100:]),\n",
    "                'sensibility': analyzeRewardSensibility(env.rewardFunction),\n",
    "                'averageBalanceTime': np.mean(balance_times[-100:])\n",
    "            }\n",
    "            \n",
    "        if updateSystem.waitingTime(episode):\n",
    "            performUpdate(env, updateSystem, episode)\n",
    "    \n",
    "    # Plot results with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "    \n",
    "    # Plot rewards\n",
    "    ax1.plot(rewards, alpha=0.6, label='Episode Reward')\n",
    "    ax1.plot(pd.Series(rewards).rolling(50).mean(), label='50-Episode Moving Average', linewidth=2)\n",
    "    \n",
    "    for i in range(0, len(rewards), 100):\n",
    "        avg_reward = np.mean(rewards[i:i+100])\n",
    "        ax1.axhline(y=avg_reward, xmin=i/len(rewards), xmax=(i+100)/len(rewards), \n",
    "                   color='r', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    ax1.set_title('Rewards Over Time')\n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.set_ylabel('Total Reward')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot balance times\n",
    "    ax2.plot(balance_times, alpha=0.6, label='Balance Time')\n",
    "    ax2.plot(pd.Series(balance_times).rolling(50).mean(), label='50-Episode Moving Average', linewidth=2)\n",
    "    \n",
    "    for i in range(0, len(balance_times), 100):\n",
    "        avg_time = np.mean(balance_times[i:i+100])\n",
    "        ax2.axhline(y=avg_time, xmin=i/len(balance_times), xmax=(i+100)/len(balance_times), \n",
    "                   color='r', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    ax2.set_title('Balance Time Over Episodes')\n",
    "    ax2.set_xlabel('Episode')\n",
    "    ax2.set_ylabel('Timesteps')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "    savePlot(fig, \"BadInitialization\", plotType=\"training_results\")\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return rewards, metrics, balance_times\n",
    "\n",
    "rewards, metrics, balance_times = runBadInitializationTest(1000)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Experiment 2 - Environment Variation Test**\n",
    "\n",
    "\n",
    "All relationships connections can be shown mathematically. So if I can show that it can optimally adapt for change in one environment variable it should abstract to other environment changes also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Environment Variation Test...\n",
      "Environment parameters updated: masscart=1.0, length=0.5, gravity=9.8\n",
      "Episode 0: Avg Balance Time = 44.00\n",
      "Episode 100: Avg Balance Time = 19.07\n",
      "Environment parameters updated: masscart=1.0, length=0.75, gravity=9.8\n",
      "\n",
      "Changed pole length to: 0.75m at episode 200\n",
      "Episode 200: Avg Balance Time = 34.45\n",
      "Episode 300: Avg Balance Time = 56.42\n",
      "Environment parameters updated: masscart=1.0, length=1.0, gravity=9.8\n",
      "\n",
      "Changed pole length to: 1.0m at episode 400\n",
      "Episode 400: Avg Balance Time = 80.40\n",
      "Episode 500: Avg Balance Time = 106.39\n",
      "Environment parameters updated: masscart=1.0, length=1.25, gravity=9.8\n",
      "\n",
      "Changed pole length to: 1.25m at episode 600\n",
      "Episode 600: Avg Balance Time = 121.73\n",
      "Episode 700: Avg Balance Time = 96.03\n",
      "Environment parameters updated: masscart=1.0, length=1.5, gravity=9.8\n",
      "\n",
      "Changed pole length to: 1.5m at episode 800\n",
      "Episode 800: Avg Balance Time = 196.46\n",
      "Episode 900: Avg Balance Time = 244.14\n",
      "\n",
      "Experiment completed!\n",
      "\n",
      "Final performance:\n",
      "Average Reward: 376.45\n",
      "Average Balance Time: 376.93\n"
     ]
    }
   ],
   "source": [
    "def runEnvironmentVariationTest(episodes=1000, changeInterval=200):\n",
    "    print(\"Starting Environment Variation Test...\")\n",
    "    \n",
    "    # Define length changes (5 values)\n",
    "    length_changes = [0.5, 0.75, 1.0, 1.25, 1.5]  # Pole lengths in meters\n",
    "    current_length_idx = 0\n",
    "    \n",
    "    # Setup environment with initial configuration\n",
    "    env = gym.make('CartPole-v1')\n",
    "    env = CustomCartPoleEnv(env)\n",
    "    env.setEnvironmentParameters(masscart=1.0, length=length_changes[0], gravity=9.8)\n",
    "    \n",
    "    # Initialize update system and agent\n",
    "    updateSystem = RewardUpdateSystem(apiKey, modelName)\n",
    "    agent = DQLearningAgent(env, 4, 2, device)\n",
    "    \n",
    "    # Initialize metrics and tracking variables\n",
    "    metrics = {}\n",
    "    episode_rewards = []\n",
    "    episode_balance_times = []\n",
    "    reward_change_episodes = []  # Track reward function changes\n",
    "    \n",
    "    # Create callback for length changes\n",
    "    def onEpisodeEnd(env, updateSystem, episode, reward, steps):\n",
    "        nonlocal current_length_idx, episode_rewards, episode_balance_times, reward_change_episodes\n",
    "        \n",
    "        # Record reward and balance time\n",
    "        episode_rewards.append(reward)\n",
    "        episode_balance_times.append(steps)\n",
    "        \n",
    "        # Check if any composite reward function was updated\n",
    "        if hasattr(dynamicRewardFunction, 'compositeHistory'):\n",
    "            latest_updates = [\n",
    "                update['episode'] for update in dynamicRewardFunction.compositeHistory \n",
    "                if update['episode'] == episode\n",
    "            ]\n",
    "            if latest_updates:\n",
    "                reward_change_episodes.append(episode)\n",
    "                print(f\"\\nReward function updated at episode {episode}\")\n",
    "        \n",
    "        if episode % changeInterval == 0 and episode > 0:\n",
    "            current_length_idx = (current_length_idx + 1) % len(length_changes)\n",
    "            new_length = length_changes[current_length_idx]\n",
    "            env.setEnvironmentParameters(length=new_length)\n",
    "            print(f\"\\nChanged pole length to: {new_length}m at episode {episode}\")\n",
    "        \n",
    "        # Record metrics every 100 episodes\n",
    "        if episode % 100 == 0:\n",
    "            metrics[episode] = {\n",
    "                'jumps': detectJumps(episode_rewards),\n",
    "                'averageReward': np.mean(episode_rewards[-100:]) if episode_rewards else 0,\n",
    "                'sensibility': analyzeRewardSensibility(env.rewardFunction),\n",
    "                'averageBalanceTime': np.mean(episode_balance_times[-100:]) if episode_balance_times else 0,\n",
    "                'currentLength': length_changes[current_length_idx]\n",
    "            }\n",
    "            print(f\"Episode {episode}: Avg Balance Time = {metrics[episode]['averageBalanceTime']:.2f}\")\n",
    "    \n",
    "    # Train the agent\n",
    "    agent, env, rewards = trainDQLearning(\n",
    "        agent=agent,\n",
    "        env=env,\n",
    "        numEpisodes=episodes,\n",
    "        updateSystem=updateSystem,\n",
    "        onEpisodeEnd=lambda env, updateSystem, episode, reward, steps: onEpisodeEnd(env, updateSystem, episode, reward, steps)\n",
    "    )\n",
    "    \n",
    "    # Create length history\n",
    "    length_history = []\n",
    "    for episode in range(episodes):\n",
    "        idx = (episode // changeInterval) % len(length_changes)\n",
    "        length_history.append(length_changes[idx])\n",
    "    \n",
    "    # Plot results\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 15))\n",
    "    \n",
    "    # Plot rewards\n",
    "    ax1.plot(episode_rewards, alpha=0.6, label='Episode Reward')\n",
    "    ax1.plot(pd.Series(episode_rewards).rolling(50).mean(), \n",
    "             label='50-Episode Moving Average', linewidth=2)\n",
    "    \n",
    "    # Add vertical lines for length changes (red)\n",
    "    for ep in range(changeInterval, episodes, changeInterval):\n",
    "        ax1.axvline(x=ep, color='r', linestyle='--', alpha=0.3, \n",
    "                   label='Length Change' if ep == changeInterval else None)\n",
    "    \n",
    "    # Add vertical lines for reward function changes (green)\n",
    "    for ep in reward_change_episodes:\n",
    "        ax1.axvline(x=ep, color='g', linestyle='--', alpha=0.3, \n",
    "                   label='Reward Update' if ep == reward_change_episodes[0] else None)\n",
    "    \n",
    "    ax1.set_title('Rewards Over Time with Length and Reward Function Changes')\n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.set_ylabel('Total Reward')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot balance times\n",
    "    ax2.plot(episode_balance_times, alpha=0.6, label='Balance Time')\n",
    "    ax2.plot(pd.Series(episode_balance_times).rolling(50).mean(), \n",
    "             label='50-Episode Moving Average', linewidth=2)\n",
    "    \n",
    "    # Add vertical lines for length changes (red)\n",
    "    for ep in range(changeInterval, episodes, changeInterval):\n",
    "        ax2.axvline(x=ep, color='r', linestyle='--', alpha=0.3,\n",
    "                   label='Length Change' if ep == changeInterval else None)\n",
    "    \n",
    "    # Add vertical lines for reward function changes (green)\n",
    "    for ep in reward_change_episodes:\n",
    "        ax2.axvline(x=ep, color='g', linestyle='--', alpha=0.3,\n",
    "                   label='Reward Update' if ep == reward_change_episodes[0] else None)\n",
    "    \n",
    "    ax2.set_title('Balance Time Over Episodes')\n",
    "    ax2.set_xlabel('Episode')\n",
    "    ax2.set_ylabel('Timesteps')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    # Plot pole length changes\n",
    "    ax3.plot(length_history, label='Pole Length')\n",
    "    ax3.set_title('Pole Length Over Episodes')\n",
    "    ax3.set_xlabel('Episode')\n",
    "    ax3.set_ylabel('Length (m)')\n",
    "    ax3.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save results with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%d%m%Y_%H%M%S\")\n",
    "    filename = f\"length_variation_test_{timestamp}.png\"\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"\\nExperiment completed!\")\n",
    "    \n",
    "    return {\n",
    "        'rewards': episode_rewards,\n",
    "        'balance_times': episode_balance_times,\n",
    "        'metrics': metrics,\n",
    "        'length_history': length_history,\n",
    "        'reward_changes': reward_change_episodes\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = runEnvironmentVariationTest(1000, 200)\n",
    "    \n",
    "    # Print final performance statistics\n",
    "    final_avg_reward = np.mean(results['rewards'][-100:])\n",
    "    final_avg_balance = np.mean(results['balance_times'][-100:])\n",
    "    print(f\"\\nFinal performance:\")\n",
    "    print(f\"Average Reward: {final_avg_reward:.2f}\")\n",
    "    print(f\"Average Balance Time: {final_avg_balance:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Experiment 3 - Combined Challenge Test** - Currently Underconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mzmq.error.ZMQError: Address in use (addr='tcp://127.0.0.1:9002'). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Cell 4: Experiment 3 - Combined Challenge Test\n",
    "def runCombinedChallengeTest(episodes=1000):\n",
    "   print(\"Starting Combined Challenge Test...\")\n",
    "   \n",
    "   envConfigs = [\n",
    "       {\"masscart\": 1.0, \"length\": 0.5, \"gravity\": 9.8},\n",
    "       {\"masscart\": 2.0, \"length\": 1.0, \"gravity\": 9.8},\n",
    "   ]\n",
    "   \n",
    "   results = {}\n",
    "   \n",
    "   for configIdx, config in enumerate(envConfigs):\n",
    "       print(f\"\\nTesting configuration {configIdx + 1}\")\n",
    "       \n",
    "       env = gym.make('CartPole-v1')\n",
    "       env = CustomCartPoleEnv(env)\n",
    "       env.setEnvironmentParameters(**config)\n",
    "       \n",
    "       env.setRewardFunction(badReward)  # Changed from eval(badReward) since we defined it as a function\n",
    "       \n",
    "       updateSystem = RewardUpdateSystem(apiKey, modelName)\n",
    "       agent = DQLearningAgent(env, 4, 2, device)\n",
    "       \n",
    "       rewards = []\n",
    "       balance_times = []  # Added balance times tracking\n",
    "       metrics = {}\n",
    "       \n",
    "       for episode in range(episodes):\n",
    "           # Modified to track balance time\n",
    "           state = env.reset()[0]\n",
    "           episode_reward = 0\n",
    "           timesteps = 0\n",
    "           done = False\n",
    "\n",
    "           while not done:\n",
    "               action = agent.chooseAction(state)\n",
    "               next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "               done = terminated or truncated\n",
    "               episode_reward += reward\n",
    "               timesteps += 1\n",
    "               \n",
    "               agent.remember(state, action, reward, next_state, done)\n",
    "               state = next_state\n",
    "\n",
    "           rewards.append(episode_reward)\n",
    "           balance_times.append(timesteps)\n",
    "           \n",
    "           if episode % 100 == 0:\n",
    "               metrics[episode] = {\n",
    "                   'jumps': detectJumps(rewards),\n",
    "                   'averageReward': np.mean(rewards[-100:]),\n",
    "                   'sensibility': analyzeRewardSensibility(env.rewardFunction),\n",
    "                   'averageBalanceTime': np.mean(balance_times[-100:])\n",
    "               }\n",
    "           \n",
    "           if updateSystem.waitingTime(episode):\n",
    "               performUpdate(env, updateSystem, episode)\n",
    "       \n",
    "       results[f\"config_{configIdx}\"] = {\n",
    "           \"rewards\": rewards,\n",
    "           \"balance_times\": balance_times,  # Added to results\n",
    "           \"metrics\": metrics,\n",
    "           \"config\": config\n",
    "       }\n",
    "       \n",
    "       # Plot individual configuration results\n",
    "       fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "       \n",
    "       # Plot rewards\n",
    "       ax1.plot(rewards, alpha=0.6, label='Episode Reward')\n",
    "       ax1.plot(pd.Series(rewards).rolling(50).mean(), label='50-Episode Moving Average', linewidth=2)\n",
    "       \n",
    "       # Add horizontal lines for 100-episode averages\n",
    "       for i in range(0, len(rewards), 100):\n",
    "           avg_reward = np.mean(rewards[i:i+100])\n",
    "           ax1.axhline(y=avg_reward, xmin=i/len(rewards), xmax=(i+100)/len(rewards), \n",
    "                      color='r', linestyle='--', alpha=0.5)\n",
    "       \n",
    "       ax1.set_title(f'Rewards Over Time - Combined Challenge Config {configIdx + 1}\\n{config}')\n",
    "       ax1.set_xlabel('Episode')\n",
    "       ax1.set_ylabel('Total Reward')\n",
    "       ax1.legend()\n",
    "       ax1.grid(True)\n",
    "       \n",
    "       # Plot balance times\n",
    "       ax2.plot(balance_times, alpha=0.6, label='Balance Time')\n",
    "       ax2.plot(pd.Series(balance_times).rolling(50).mean(), label='50-Episode Moving Average', linewidth=2)\n",
    "       \n",
    "       # Add horizontal lines for 100-episode averages\n",
    "       for i in range(0, len(balance_times), 100):\n",
    "           avg_time = np.mean(balance_times[i:i+100])\n",
    "           ax2.axhline(y=avg_time, xmin=i/len(balance_times), xmax=(i+100)/len(balance_times), \n",
    "                      color='r', linestyle='--', alpha=0.5)\n",
    "       \n",
    "       ax2.set_title('Balance Time Over Episodes')\n",
    "       ax2.set_xlabel('Episode')\n",
    "       ax2.set_ylabel('Timesteps')\n",
    "       ax2.legend()\n",
    "       ax2.grid(True)\n",
    "       \n",
    "       plt.tight_layout()\n",
    "       plt.show()\n",
    "   \n",
    "   # Plot comparison across configurations (now with both metrics)\n",
    "   fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "   \n",
    "   # Rewards comparison\n",
    "   for config, data in results.items():\n",
    "       ax1.plot(pd.Series(data['rewards']).rolling(50).mean(), \n",
    "               label=f\"Config {config}\")\n",
    "   ax1.set_title(\"Combined Challenge - Reward Comparison Across Configurations\")\n",
    "   ax1.set_xlabel(\"Episode\")\n",
    "   ax1.set_ylabel(\"Average Reward (50-episode window)\")\n",
    "   ax1.legend()\n",
    "   ax1.grid(True)\n",
    "   \n",
    "   # Balance times comparison\n",
    "   for config, data in results.items():\n",
    "       ax2.plot(pd.Series(data['balance_times']).rolling(50).mean(), \n",
    "               label=f\"Config {config}\")\n",
    "   ax2.set_title(\"Combined Challenge - Balance Time Comparison Across Configurations\")\n",
    "   ax2.set_xlabel(\"Episode\")\n",
    "   ax2.set_ylabel(\"Average Balance Time (50-episode window)\")\n",
    "   ax2.legend()\n",
    "   ax2.grid(True)\n",
    "   \n",
    "   plt.tight_layout()\n",
    "   plt.show()\n",
    "   \n",
    "   return results\n",
    "\n",
    "runCombinedChallengeTest(500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gymenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
