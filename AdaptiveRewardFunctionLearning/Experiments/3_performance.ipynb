{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering Adding:\n",
    "- Confidence intervals for performance metrics (Most papers only really seem to go this far)\n",
    "- Statistical significance tests between different approaches\n",
    "- Variance analysis across multiple runs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in /home/sd37/.local/lib/python3.10/site-packages (4.1.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /home/sd37/.local/lib/python3.10/site-packages (from optuna) (1.14.0)\n",
      "Requirement already satisfied: colorlog in /home/sd37/.local/lib/python3.10/site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: numpy in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from optuna) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from optuna) (24.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from optuna) (2.0.37)\n",
      "Requirement already satisfied: tqdm in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from optuna) (4.66.5)\n",
      "Requirement already satisfied: PyYAML in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from optuna) (6.0)\n",
      "Requirement already satisfied: Mako in /home/sd37/.local/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (1.3.8)\n",
      "Requirement already satisfied: typing-extensions>=4 in /home/sd37/.local/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sd37/.conda/envs/thesis/bin/python\n",
      "4.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sd37/.conda/envs/thesis/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "\n",
    "import optuna\n",
    "print(optuna.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **This experiment is investigating the performance of an adaptive reward function to state of the art reward functions in environments with environmentally variable changes.**\n",
    "\n",
    "-> Is there a statisitcally significant improvement in performance over time in this varying environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "current_dir = os.getcwd()  \n",
    "project_root = str(Path(current_dir).parent.parent)\n",
    "sys.path.append(project_root)\n",
    "\n",
    "\n",
    "# Initialize environment and device\n",
    "from AdaptiveRewardFunctionLearning.Prompts.prompts import device, apiKey,modelName\n",
    "\n",
    "#Cu stomCartPoleEnv\n",
    "from RLEnvironment.env import CustomCartPoleEnv\n",
    "#RewardUpdateSystem\n",
    "from AdaptiveRewardFunctionLearning.RewardGeneration.rewardCritic import RewardUpdateSystem\n",
    "#DQLearningAgent\n",
    "from RLEnvironment.training.agent import DQLearningAgent\n",
    "from RLEnvironment.training.training import trainDQLearning\n",
    "\n",
    "#DynamicRewardFunction\n",
    "from AdaptiveRewardFunctionLearning.RewardGeneration.rewardCodeGeneration import dynamicRewardFunction\n",
    "\n",
    "#import\n",
    "from AdaptiveRewardFunctionLearning.Visualisation.trainingTestFunctions import (\n",
    "    runEpisode,\n",
    "    detectJumps,\n",
    "    analyzeRewardSensibility,\n",
    "    performUpdate,\n",
    "    updateCompositeRewardFunction,\n",
    "    plotExperimentResults,\n",
    "    savePlot\n",
    ")\n",
    "\n",
    "# Import new reward functions\n",
    "from AdaptiveRewardFunctionLearning.RewardGeneration.cartpole_energy_reward import EnergyBasedRewardFunction\n",
    "from AdaptiveRewardFunctionLearning.RewardGeneration.cartpole_meta_learning import meta_learning_cartpole\n",
    "from AdaptiveRewardFunctionLearning.RewardGeneration.reward_meta_learning import RewardFunctionMetaLearner"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**State of the Art Reward Functions with Reference to Papers**\n",
    "\n",
    "**Potential-based Reward Shaping (PBRS):**\n",
    "```python\n",
    "def potentialBasedRewardShaping(observation, action):\n",
    "    x, xDot, angle, angleDot = observation\n",
    "    gamma = 0.99  # Example discount factor\n",
    "\n",
    "    def phi(x, xDot, angle, angleDot):\n",
    "        # Example potential function\n",
    "        return -abs(x) - abs(angle)\n",
    "\n",
    "    current_potential = phi(x, xDot, angle, angleDot)\n",
    "    next_potential = phi(x + xDot, angle + angleDot, xDot, angleDot)  # Simplified next state\n",
    "    return float(gamma * next_potential - current_potential)\n",
    "```\n",
    "\n",
    "Paper: \"Potential-based Shaping in Model-based Reinforcement Learning\"\n",
    "\n",
    "Link: https://cdn.aaai.org/AAAI/2008/AAAI08-096.pdf\n",
    "\n",
    "\n",
    "**Parameterized Reward Shaping:**\n",
    "```python\n",
    "def parameterizedRewardShaping(observation, action):\n",
    "    x, xDot, angle, angleDot = observation\n",
    "    original_reward = 1.0  # Assuming default CartPole reward\n",
    "\n",
    "    def f(x, xDot, angle, angleDot):\n",
    "        # Example shaping reward function\n",
    "        return -abs(angle)\n",
    "\n",
    "    def z_phi(x, xDot, angle, angleDot):\n",
    "        # Example shaping weight function\n",
    "        return 0.5\n",
    "\n",
    "    shaping_reward = f(x, xDot, angle, angleDot)\n",
    "    shaping_weight = z_phi(x, xDot, angle, angleDot)\n",
    "    return float(original_reward + shaping_weight * shaping_reward)\n",
    "```\n",
    "\n",
    "Paper: \"Learning to Utilize Shaping Rewards: A New Approach of Reward Shaping\"\n",
    "\n",
    "Link: http://arxiv.org/pdf/2011.02669.pdf\n",
    "\n",
    "\n",
    "**Energy Based Reward Function - Physics Based**\n",
    "\n",
    "```python\n",
    "def energyBasedReward(observation, action):\n",
    "    x, xDot, angle, angleDot = observation\n",
    "    \n",
    "    # Calculate kinetic and potential energy components\n",
    "    kineticEnergy = 0.5 * (xDot**2 + angleDot**2)\n",
    "    potentialEnergy = 9.8 * (1 + cos(angle))  # g * h, where h depends on angle\n",
    "    \n",
    "    # Reward is inverse of total energy (less energy = more stable = better reward)\n",
    "    energyPenalty = -(kineticEnergy + potentialEnergy)\n",
    "    return float(1.0 + 0.1 * energyPenalty)  # Base reward plus energy term\n",
    "```\n",
    "\n",
    "Paper: \"Energy-Based Control for Safe Robot Learning\" (2019)\n",
    "\n",
    "Link: https://ieeexplore.ieee.org/document/8794207\n",
    "\n",
    "\n",
    "**Baseline Reward Function:**\n",
    "```python\n",
    "def baselineCartPoleReward(observation, action):\n",
    "    return 1.0\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Performance Experiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize reward functions and meta-learners\n",
    "energy_reward = EnergyBasedRewardFunction(mass_cart=1.0, mass_pole=0.1, length=0.5, gravity=9.8)\n",
    "meta_reward = RewardFunctionMetaLearner(state_dim=4, action_dim=1)  # CartPole has 4 state dims, 1 action dim\n",
    "\n",
    "def potentialBasedRewardShaping(observation, action):\n",
    "    \"\"\"Advanced potential-based reward shaping using meta-learning\"\"\"\n",
    "    reward_func = meta_reward.generate_reward_function()\n",
    "    return float(reward_func(observation, action))\n",
    "\n",
    "def parameterizedRewardShaping(observation, action):\n",
    "    \"\"\"Meta-learning based parameterized reward shaping\"\"\"\n",
    "    # Use meta-learning framework for parameter optimization\n",
    "    learner = meta_learning_cartpole()\n",
    "    return float(learner.parameterized_reward(observation, action))\n",
    "\n",
    "def energyBasedReward(observation, action):\n",
    "    \"\"\"Enhanced physics-based energy reward\"\"\"\n",
    "    return float(energy_reward.compute_reward(observation, action))\n",
    "\n",
    "def baselineCartPoleReward(observation, action):\n",
    "    \"\"\"Standard baseline reward\"\"\"\n",
    "    return 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runPerformanceComparisonTest(\n",
    "    episodes=1000, \n",
    "    changeInterval=500, \n",
    "    lengthchanges=[0.5, 1.5],\n",
    "    mass_cart=1.0,\n",
    "    mass_pole=0.1,\n",
    "    initial_length=0.5,\n",
    "    gravity=9.8\n",
    "):\n",
    "    print(\"Starting Performance Comparison Test...\")\n",
    "    \n",
    "    currentlengthidx = 0\n",
    "    \n",
    "    env = gym.make('CartPole-v1')\n",
    "    env = CustomCartPoleEnv(env)\n",
    "    env.setEnvironmentParameters(masscart=mass_cart, length=lengthchanges[0], gravity=gravity)\n",
    "    \n",
    "    energy_reward = EnergyBasedRewardFunction(\n",
    "        mass_cart=mass_cart, \n",
    "        mass_pole=mass_pole, \n",
    "        length=initial_length, \n",
    "        gravity=gravity\n",
    "    )\n",
    "    meta_reward = RewardFunctionMetaLearner(state_dim=4, action_dim=1)\n",
    "    \n",
    "    rewardfunctions = {\n",
    "        'adaptivereward': {\n",
    "            'agent': DQLearningAgent(env, 4, 2, device),\n",
    "            'updatesystem': RewardUpdateSystem(apiKey, modelName),\n",
    "            'rewardfunction': None,\n",
    "            'update_method': 'llm'\n",
    "        },\n",
    "        'meta_learning': {\n",
    "            'agent': DQLearningAgent(env, 4, 2, device),\n",
    "            'updatesystem': meta_reward,\n",
    "            'rewardfunction': meta_reward.generate_reward_function(),\n",
    "            'update_method': 'meta'\n",
    "        },\n",
    "        'energy_based': {\n",
    "            'agent': DQLearningAgent(env, 4, 2, device),\n",
    "            'updatesystem': energy_reward,\n",
    "            'rewardfunction': energy_reward.compute_reward,\n",
    "            'update_method': 'physics'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for rewardname, rewardinfo in rewardfunctions.items():\n",
    "        print(f\"\\nTesting reward function: {rewardname}\")\n",
    "        \n",
    "        env.reset()\n",
    "        if rewardname != 'adaptivereward':\n",
    "            env.setRewardFunction(rewardinfo['rewardfunction'])\n",
    "        \n",
    "        episoderewards = []\n",
    "        episodebalancetimes = []\n",
    "        rewardchangeepisodes = []\n",
    "        \n",
    "        def onEpisodeEnd(env, updatesystem, episode, reward, steps):\n",
    "            nonlocal episoderewards, episodebalancetimes, rewardchangeepisodes, currentlengthidx\n",
    "            \n",
    "            episoderewards.append(reward)\n",
    "            episodebalancetimes.append(steps)\n",
    "            \n",
    "            metrics = {\n",
    "                'currentEpisode': episode,\n",
    "                'recentRewards': episoderewards[-100:] if len(episoderewards) > 100 else episoderewards,\n",
    "                'averageBalanceTime': np.mean(episodebalancetimes[-100:]) if episodebalancetimes else 0,\n",
    "                'balanceTimeVariance': np.var(episodebalancetimes[-100:]) if len(episodebalancetimes) > 1 else 0\n",
    "            }\n",
    "            \n",
    "            if rewardinfo['update_method'] == 'llm':\n",
    "                updateCompositeRewardFunction(env, updatesystem, metrics, dynamicRewardFunction)\n",
    "                if hasattr(env.rewardFunction, 'compositeHistory'):\n",
    "                    latest_updates = [update['episode'] for update in env.rewardFunction.compositeHistory if update['episode'] == episode]\n",
    "                    if latest_updates:\n",
    "                        rewardchangeepisodes.append(episode)\n",
    "                        print(f\"\\nReward function updated at episode {episode}\")\n",
    "                        \n",
    "            elif rewardinfo['update_method'] == 'meta':\n",
    "                if episode % 50 == 0 and episode > 0:\n",
    "                    trajectory = {\n",
    "                        'states': episoderewards[-100:],  # Using rewards as proxy for now\n",
    "                        'actions': range(len(episoderewards[-100:])),  # Simplified action tracking\n",
    "                        'rewards': episoderewards[-100:]\n",
    "                    }\n",
    "                    meta_loss = updatesystem.meta_update([trajectory])\n",
    "                    print(f\"\\nMeta-learning update at episode {episode}, loss: {meta_loss}\")\n",
    "                    env.setRewardFunction(updatesystem.generate_reward_function())\n",
    "                    rewardchangeepisodes.append(episode)\n",
    "                    \n",
    "            elif rewardinfo['update_method'] == 'physics':\n",
    "                if episode % changeInterval == 0 and episode > 0:\n",
    "                    updatesystem.length = lengthchanges[currentlengthidx]\n",
    "                    print(f\"\\nEnergy reward updated with new length: {updatesystem.length}\")\n",
    "                    env.setRewardFunction(updatesystem.compute_reward)\n",
    "                    rewardchangeepisodes.append(episode)\n",
    "            \n",
    "            if episode % changeInterval == 0 and episode > 0:\n",
    "                currentlengthidx = (currentlengthidx + 1) % len(lengthchanges)\n",
    "                newlength = lengthchanges[currentlengthidx]\n",
    "                env.setEnvironmentParameters(length=newlength)\n",
    "                print(f\"\\nChanged pole length to: {newlength}m at episode {episode}\")\n",
    "            \n",
    "            if episode % 50 == 0:\n",
    "                print(f\"Episode {episode}\")\n",
    "                print(f\"  Steps: {steps}\")\n",
    "                print(f\"  Total Reward: {reward}\")\n",
    "                print(f\"  Average Balance Time = {np.mean(episodebalancetimes[-50:]):.2f}\")\n",
    "                print(f\"  Average Reward = {np.mean(episoderewards[-50:]):.2f}\")\n",
    "        \n",
    "        agent, env, rewards = trainDQLearning(\n",
    "            agent=rewardinfo['agent'],\n",
    "            env=env,\n",
    "            numEpisodes=episodes,\n",
    "            updateSystem=rewardinfo['updatesystem'],\n",
    "            onEpisodeEnd=onEpisodeEnd\n",
    "        )\n",
    "        \n",
    "        results[rewardname] = {\n",
    "            'rewards': episoderewards,\n",
    "            'balancetimes': episodebalancetimes,\n",
    "            'rewardchanges': rewardchangeepisodes\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nCompleted testing {rewardname}\")\n",
    "        print(f\"Final average reward: {np.mean(episoderewards[-100:]):.2f}\")\n",
    "        print(f\"Final average balance time: {np.mean(episodebalancetimes[-100:]):.2f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Experiment\n",
    "\n",
    "changeInterval = 20000\n",
    "\n",
    "# results = runPerformanceComparisonTest(\n",
    "#     episodes=40000,  \n",
    "#     changeInterval=changeInterval,\n",
    "#     mass_cart=1.0,\n",
    "#     lengthchanges=[0.3, 0.9]  \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizePerformanceComparison(results, changeInterval, folder_path):\n",
    "    \"\"\"\n",
    "    Create and save performance comparison visualizations\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "    \n",
    "    # Color map for different reward functions\n",
    "    colors = ['b', 'g', 'r', 'c', 'm']\n",
    "    \n",
    "    # Plot rewards for each reward function with variance\n",
    "    for idx, (rewardname, rewardresults) in enumerate(results.items()):\n",
    "        rewards = pd.Series(rewardresults['rewards'])\n",
    "        \n",
    "        # Calculate rolling mean and standard deviation for rewards\n",
    "        window = 50\n",
    "        rolling_mean_rewards = rewards.rolling(window=window).mean()\n",
    "        rolling_std_rewards = rewards.rolling(window=window).std()\n",
    "        \n",
    "        # Plot mean line for rewards\n",
    "        ax1.plot(rolling_mean_rewards, \n",
    "                label=f'{rewardname}', \n",
    "                linewidth=2, \n",
    "                color=colors[idx])\n",
    "        \n",
    "        # Plot variance area for rewards\n",
    "        ax1.fill_between(\n",
    "            range(len(rewards)),\n",
    "            rolling_mean_rewards - rolling_std_rewards,\n",
    "            rolling_mean_rewards + rolling_std_rewards,\n",
    "            color=colors[idx],\n",
    "            alpha=0.2\n",
    "        )\n",
    "        \n",
    "        # Add vertical lines for environment changes\n",
    "        change_episodes = range(changeInterval, len(rewards), changeInterval)\n",
    "        for ep in change_episodes:\n",
    "            ax1.axvline(x=ep, color='r', linestyle='--', alpha=0.3,\n",
    "                       label='Environment Change' if ep == change_episodes[0] else None)\n",
    "        \n",
    "        # Add reward function change markers\n",
    "        if rewardname == 'adaptivereward' and rewardresults['rewardchanges']:\n",
    "            for episode in rewardresults['rewardchanges']:\n",
    "                ax1.axvline(x=episode, color='g', linestyle='--', alpha=0.3,\n",
    "                          label='Reward Update' if episode == rewardresults['rewardchanges'][0] else None)\n",
    "    \n",
    "    ax1.set_title('Average Reward Over Time with Variance')\n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.set_ylabel('Reward')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot balance times\n",
    "    for idx, (rewardname, rewardresults) in enumerate(results.items()):\n",
    "        balancetimes = pd.Series(rewardresults['balancetimes'])\n",
    "        \n",
    "        rolling_mean_balance = balancetimes.rolling(window=window).mean()\n",
    "        rolling_std_balance = balancetimes.rolling(window=window).std()\n",
    "        \n",
    "        ax2.plot(rolling_mean_balance,\n",
    "                label=f'{rewardname}', \n",
    "                linewidth=2, \n",
    "                color=colors[idx])\n",
    "        \n",
    "        ax2.fill_between(\n",
    "            range(len(balancetimes)),\n",
    "            rolling_mean_balance - rolling_std_balance,\n",
    "            rolling_mean_balance + rolling_std_balance,\n",
    "            color=colors[idx],\n",
    "            alpha=0.2\n",
    "        )\n",
    "        \n",
    "        # Add vertical lines\n",
    "        for ep in change_episodes:\n",
    "            ax2.axvline(x=ep, color='r', linestyle='--', alpha=0.3,\n",
    "                       label='Environment Change' if ep == change_episodes[0] else None)\n",
    "        \n",
    "        if rewardname == 'adaptivereward' and rewardresults['rewardchanges']:\n",
    "            for episode in rewardresults['rewardchanges']:\n",
    "                ax2.axvline(x=episode, color='g', linestyle='--', alpha=0.3,\n",
    "                          label='Reward Update' if episode == rewardresults['rewardchanges'][0] else None)\n",
    "    \n",
    "    ax2.set_title('Average Balance Time Over Episodes with Variance')\n",
    "    ax2.set_xlabel('Episode')\n",
    "    ax2.set_ylabel('Steps')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    savePlot(fig, \"performance_comparison\", folder_path)\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "    \n",
    "def calculateStability(rewards):\n",
    "    \"\"\"\n",
    "    Calculate stability score based on reward variance in the last 100 episodes\n",
    "    Lower variance = higher stability\n",
    "    \"\"\"\n",
    "    if len(rewards) < 100:\n",
    "        return 0.0\n",
    "    \n",
    "    last_hundred = rewards[-100:]\n",
    "    mean_reward = np.mean(last_hundred)\n",
    "    if mean_reward == 0:\n",
    "        return 0.0\n",
    "        \n",
    "    # Calculate coefficient of variation (normalized standard deviation)\n",
    "    stability = 1 - (np.std(last_hundred) / mean_reward)\n",
    "    return max(0, min(1, stability))  # Normalize between 0 and 1\n",
    "\n",
    "def calculateConvergenceTime(rewards, threshold=195, window=50):\n",
    "    \"\"\"\n",
    "    Calculate the number of episodes needed to reach and maintain a certain performance\n",
    "    threshold for a given window of episodes\n",
    "    \"\"\"\n",
    "    if len(rewards) < window:\n",
    "        return len(rewards)\n",
    "    \n",
    "    rolling_mean = pd.Series(rewards).rolling(window).mean()\n",
    "    \n",
    "    for episode in range(window, len(rewards)):\n",
    "        if rolling_mean[episode] >= threshold:\n",
    "            # Check if performance is maintained\n",
    "            maintained = all(avg >= threshold * 0.9 for avg in rolling_mean[episode:episode+window])\n",
    "            if maintained:\n",
    "                return episode\n",
    "    \n",
    "    return len(rewards)  # If never converged, return total episodes\n",
    "\n",
    "def calculatePerformanceMetrics(results):\n",
    "    metrics = {}\n",
    "    for rewardname, rewardresults in results.items():\n",
    "        metrics[rewardname] = {\n",
    "            'finalavgreward': np.mean(rewardresults['rewards'][-100:]),\n",
    "            'finalavgbalance': np.mean(rewardresults['balancetimes'][-100:]),\n",
    "            'convergencetime': calculateConvergenceTime(rewardresults['rewards']),\n",
    "            'stability': calculateStability(rewardresults['rewards'])\n",
    "        }\n",
    "    return pd.DataFrame(metrics).T\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def saveMetricsTable(metrics, filename=\"performanceMetricsTable\"):\n",
    "    # Style the DataFrame\n",
    "    styledMetrics = metrics.style.format({\n",
    "        'finalavgreward': '{:.2f}',\n",
    "        'finalavgbalance': '{:.2f}',\n",
    "        'convergencetime': '{:.1f}',\n",
    "        'stability': '{:.6f}'\n",
    "    })\n",
    "    \n",
    "    # Save as image using plt\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    table = ax.table(cellText=metrics.values.round(3),\n",
    "                    colLabels=metrics.columns,\n",
    "                    rowLabels=metrics.index,\n",
    "                    cellLoc='center',\n",
    "                    loc='center')\n",
    "    \n",
    "    # Adjust font size\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(9)\n",
    "    table.scale(1.2, 1.5)\n",
    "    \n",
    "    plt.title(\"Performance Metrics Comparison\")\n",
    "    savePlot(fig, filename, \"PerformanceExperiment\")\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize the results\n",
    "# visualizePerformanceComparison(results,changeInterval)\n",
    "\n",
    "\n",
    "# # Calculate and display the metrics\n",
    "# metrics = calculatePerformanceMetrics(results)\n",
    "# print(\"\\nPerformance Metrics:\")\n",
    "# print(metrics)\n",
    "    \n",
    "# Call the function\n",
    "# saveMetricsTable(metrics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple runs to generate confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createExperimentFolder():\n",
    "    \"\"\"Create a timestamped folder for experiment results\"\"\"\n",
    "    from datetime import datetime\n",
    "    import os\n",
    "    \n",
    "    # Create base experiment folder if it doesn't exist\n",
    "    if not os.path.exists(\"PerformanceExperiment\"):\n",
    "        os.makedirs(\"PerformanceExperiment\")\n",
    "    \n",
    "    # Create timestamped subfolder\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    experiment_folder = os.path.join(\"PerformanceExperiment\", f\"experiment_{timestamp}\")\n",
    "    os.makedirs(experiment_folder)\n",
    "    \n",
    "    return experiment_folder\n",
    "\n",
    "def savePlot(fig, filename, folder_path):\n",
    "    \"\"\"Save plot to specified folder with timestamp\"\"\"\n",
    "    from datetime import datetime\n",
    "    import os\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filepath = os.path.join(folder_path, f\"{filename}_{timestamp}.png\")\n",
    "    fig.savefig(filepath, bbox_inches='tight', dpi=300)\n",
    "    print(f\"Saved plot: {filename}_{timestamp}.png in {folder_path}\")\n",
    "    \n",
    "def saveMetricsTable(metrics, filename, folder_path):\n",
    "    \"\"\"Save metrics table to specified folder\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    from datetime import datetime\n",
    "    import os\n",
    "    \n",
    "    # Style the DataFrame for visualization\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    table = ax.table(cellText=metrics.values,\n",
    "                    colLabels=metrics.columns,\n",
    "                    rowLabels=metrics.index,\n",
    "                    cellLoc='center',\n",
    "                    loc='center')\n",
    "    \n",
    "    # Adjust font size and scaling\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(9)\n",
    "    table.scale(1.2, 1.5)\n",
    "    \n",
    "    plt.title(\"Performance Metrics Comparison\")\n",
    "    \n",
    "    # Save with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filepath = os.path.join(folder_path, f\"{filename}_{timestamp}.png\")\n",
    "    plt.savefig(filepath, bbox_inches='tight', dpi=300)\n",
    "    print(f\"Saved metrics table: {filename}_{timestamp}.png in {folder_path}\")\n",
    "    plt.close()\n",
    "\n",
    "def runMultipleExperiments(numRuns=4, episodes=40000, changeInterval=20000):\n",
    "    \"\"\"\n",
    "    Run multiple experiments and save results in organized folders\n",
    "    \"\"\"\n",
    "    # Create main experiment folder\n",
    "    experiment_folder = createExperimentFolder()\n",
    "    \n",
    "    allResults = []\n",
    "    allMetrics = []\n",
    "    aggregatedMetrics = {\n",
    "        'adaptivereward': [],\n",
    "        'meta_learning': [],\n",
    "        'energy_based': []\n",
    "    }\n",
    "    \n",
    "    # Create run folders\n",
    "    for run in range(numRuns):\n",
    "        print(f\"\\nStarting Run {run + 1}/{numRuns}\")\n",
    "        \n",
    "        # Create folder for this run\n",
    "        run_folder = os.path.join(experiment_folder, f\"run_{run + 1}\")\n",
    "        os.makedirs(run_folder)\n",
    "        \n",
    "        # Run experiment\n",
    "        results = runPerformanceComparisonTest(\n",
    "            episodes=episodes,\n",
    "            changeInterval=changeInterval,\n",
    "            mass_cart=1.0,\n",
    "            lengthchanges=[0.3, 0.9]\n",
    "        )\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = calculatePerformanceMetrics(results)\n",
    "        \n",
    "        # Store results\n",
    "        allResults.append(results)\n",
    "        allMetrics.append(metrics)\n",
    "        \n",
    "        # Store metrics by reward type\n",
    "        for rewardType in results.keys():\n",
    "            if rewardType not in aggregatedMetrics:\n",
    "                aggregatedMetrics[rewardType] = []\n",
    "            aggregatedMetrics[rewardType].append(metrics.loc[rewardType])\n",
    "        \n",
    "        # Visualize and save individual run results\n",
    "        visualizePerformanceComparison(results, changeInterval, run_folder)\n",
    "        saveMetricsTable(metrics, f\"metrics_run_{run + 1}\", run_folder)\n",
    "    \n",
    "    # Calculate confidence intervals (95%)\n",
    "    confidenceIntervals = {}\n",
    "    \n",
    "    for rewardType, metrics_list in aggregatedMetrics.items():\n",
    "        metrics_df = pd.DataFrame(metrics_list)\n",
    "        confidenceIntervals[rewardType] = {}\n",
    "        \n",
    "        for column in metrics_df.columns:\n",
    "            mean = metrics_df[column].mean()\n",
    "            ci = 1.96 * metrics_df[column].std() / np.sqrt(numRuns)\n",
    "            \n",
    "            confidenceIntervals[rewardType][column] = {\n",
    "                'mean': mean,\n",
    "                'ci': ci\n",
    "            }\n",
    "    \n",
    "    # Create final results table\n",
    "    resultsTable = pd.DataFrame({\n",
    "        rewardType: {\n",
    "            metric: f\"{data[metric]['mean']:.2f} ± {data[metric]['ci']:.2f}\"\n",
    "            for metric in ['finalavgreward', 'finalavgbalance', 'convergencetime', 'stability']\n",
    "        }\n",
    "        for rewardType, data in confidenceIntervals.items()\n",
    "    }).T\n",
    "    \n",
    "    # Save final results in main experiment folder\n",
    "    saveMetricsTable(resultsTable, \"final_results\", experiment_folder)\n",
    "    \n",
    "    # Save aggregate statistics\n",
    "    with open(os.path.join(experiment_folder, \"aggregate_statistics.txt\"), \"w\") as f:\n",
    "        f.write(\"Aggregate Statistics:\\n\")\n",
    "        for rewardType, metrics in confidenceIntervals.items():\n",
    "            f.write(f\"\\n{rewardType}:\\n\")\n",
    "            for metric, values in metrics.items():\n",
    "                f.write(f\"{metric}: {values['mean']:.2f} ± {values['ci']:.2f}\\n\")\n",
    "    \n",
    "    print(f\"\\nExperiment results saved in: {experiment_folder}\")\n",
    "    return confidenceIntervals, allResults, resultsTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidenceIntervals, allResults, resultsTable = runMultipleExperiments(\n",
    "    numRuns=4,\n",
    "    episodes=40000,\n",
    "    changeInterval=20000\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
