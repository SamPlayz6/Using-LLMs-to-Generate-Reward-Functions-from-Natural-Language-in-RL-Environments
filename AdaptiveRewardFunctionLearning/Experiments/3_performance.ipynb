{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering Adding:\n",
    "- Confidence intervals for performance metrics (Most papers only really seem to go this far)\n",
    "- Statistical significance tests between different approaches\n",
    "- Variance analysis across multiple runs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in /home/sd37/.local/lib/python3.10/site-packages (4.1.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /home/sd37/.local/lib/python3.10/site-packages (from optuna) (1.14.0)\n",
      "Requirement already satisfied: colorlog in /home/sd37/.local/lib/python3.10/site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: numpy in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from optuna) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from optuna) (24.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from optuna) (2.0.37)\n",
      "Requirement already satisfied: tqdm in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from optuna) (4.66.5)\n",
      "Requirement already satisfied: PyYAML in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from optuna) (6.0)\n",
      "Requirement already satisfied: Mako in /home/sd37/.local/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (1.3.8)\n",
      "Requirement already satisfied: typing-extensions>=4 in /home/sd37/.local/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sd37/.conda/envs/thesis/bin/python\n",
      "4.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sd37/.conda/envs/thesis/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "\n",
    "import optuna\n",
    "print(optuna.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **This experiment is investigating the performance of an adaptive reward function to state of the art reward functions in environments with environmentally variable changes.**\n",
    "\n",
    "-> Is there a statisitcally significant improvement in performance over time in this varying environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "current_dir = os.getcwd()  \n",
    "project_root = str(Path(current_dir).parent.parent)\n",
    "sys.path.append(project_root)\n",
    "\n",
    "\n",
    "# Initialize environment and device\n",
    "from AdaptiveRewardFunctionLearning.Prompts.prompts import device, apiKey,modelName\n",
    "\n",
    "#Cu stomCartPoleEnv\n",
    "from RLEnvironment.env import CustomCartPoleEnv\n",
    "#RewardUpdateSystem\n",
    "from AdaptiveRewardFunctionLearning.RewardGeneration.rewardCritic import RewardUpdateSystem\n",
    "#DQLearningAgent\n",
    "from RLEnvironment.training.agent import DQLearningAgent\n",
    "from RLEnvironment.training.training import trainDQLearning\n",
    "\n",
    "#DynamicRewardFunction\n",
    "from AdaptiveRewardFunctionLearning.RewardGeneration.rewardCodeGeneration import stabilityReward, efficiencyReward, dynamicRewardFunction\n",
    "\n",
    "#import\n",
    "from AdaptiveRewardFunctionLearning.Visualisation.trainingTestFunctions import (\n",
    "    runEpisode,\n",
    "    detectJumps,\n",
    "    analyzeRewardSensibility,\n",
    "    performUpdate,\n",
    "    updateCompositeRewardFunction,\n",
    "    plotExperimentResults,\n",
    "    savePlot\n",
    ")\n",
    "\n",
    "# Import new reward functions\n",
    "from AdaptiveRewardFunctionLearning.RewardGeneration.cartpole_energy_reward import EnergyBasedRewardFunction\n",
    "\n",
    "\n",
    "# from AdaptiveRewardFunctionLearning.RewardGeneration.cartpole_meta_learning import meta_learning_cartpole\n",
    "# from AdaptiveRewardFunctionLearning.RewardGeneration.reward_meta_learning import RewardFunctionMetaLearner"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**State of the Art Reward Functions with Reference to Papers**\n",
    "\n",
    "**Potential-based Reward Shaping (PBRS):**\n",
    "```python\n",
    "def potentialBasedRewardShaping(observation, action):\n",
    "    x, xDot, angle, angleDot = observation\n",
    "    gamma = 0.99  # Example discount factor\n",
    "\n",
    "    def phi(x, xDot, angle, angleDot):\n",
    "        # Example potential function\n",
    "        return -abs(x) - abs(angle)\n",
    "\n",
    "    current_potential = phi(x, xDot, angle, angleDot)\n",
    "    next_potential = phi(x + xDot, angle + angleDot, xDot, angleDot)  # Simplified next state\n",
    "    return float(gamma * next_potential - current_potential)\n",
    "```\n",
    "\n",
    "Paper: \"Potential-based Shaping in Model-based Reinforcement Learning\"\n",
    "\n",
    "Link: https://cdn.aaai.org/AAAI/2008/AAAI08-096.pdf\n",
    "\n",
    "\n",
    "**Parameterized Reward Shaping:**\n",
    "```python\n",
    "def parameterizedRewardShaping(observation, action):\n",
    "    x, xDot, angle, angleDot = observation\n",
    "    original_reward = 1.0  # Assuming default CartPole reward\n",
    "\n",
    "    def f(x, xDot, angle, angleDot):\n",
    "        # Example shaping reward function\n",
    "        return -abs(angle)\n",
    "\n",
    "    def z_phi(x, xDot, angle, angleDot):\n",
    "        # Example shaping weight function\n",
    "        return 0.5\n",
    "\n",
    "    shaping_reward = f(x, xDot, angle, angleDot)\n",
    "    shaping_weight = z_phi(x, xDot, angle, angleDot)\n",
    "    return float(original_reward + shaping_weight * shaping_reward)\n",
    "```\n",
    "\n",
    "Paper: \"Learning to Utilize Shaping Rewards: A New Approach of Reward Shaping\"\n",
    "\n",
    "Link: http://arxiv.org/pdf/2011.02669.pdf\n",
    "\n",
    "\n",
    "**Energy Based Reward Function - Physics Based**\n",
    "\n",
    "```python\n",
    "def energyBasedReward(observation, action):\n",
    "    x, xDot, angle, angleDot = observation\n",
    "    \n",
    "    # Calculate kinetic and potential energy components\n",
    "    kineticEnergy = 0.5 * (xDot**2 + angleDot**2)\n",
    "    potentialEnergy = 9.8 * (1 + cos(angle))  # g * h, where h depends on angle\n",
    "    \n",
    "    # Reward is inverse of total energy (less energy = more stable = better reward)\n",
    "    energyPenalty = -(kineticEnergy + potentialEnergy)\n",
    "    return float(1.0 + 0.1 * energyPenalty)  # Base reward plus energy term\n",
    "```\n",
    "\n",
    "Paper: \"Energy-Based Control for Safe Robot Learning\" (2019)\n",
    "\n",
    "Link: https://ieeexplore.ieee.org/document/8794207\n",
    "\n",
    "\n",
    "**Baseline Reward Function:**\n",
    "```python\n",
    "def baselineCartPoleReward(observation, action):\n",
    "    return 1.0\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Performance Experiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize reward functions and meta-learners\n",
    "energy_reward = EnergyBasedRewardFunction(mass_cart=1.0, mass_pole=0.1, length=0.5, gravity=9.8)\n",
    "# meta_reward = RewardFunctionMetaLearner(state_dim=4, action_dim=1)  # CartPole has 4 state dims, 1 action dim\n",
    "\n",
    "# def potentialBasedRewardShaping(observation, action):\n",
    "#     \"\"\"Advanced potential-based reward shaping using meta-learning\"\"\"\n",
    "#     reward_func = meta_reward.generate_reward_function()\n",
    "#     return float(reward_func(observation, action))\n",
    "\n",
    "# def parameterizedRewardShaping(observation, action):\n",
    "#     \"\"\"Meta-learning based parameterized reward shaping\"\"\"\n",
    "#     # Use meta-learning framework for parameter optimization\n",
    "#     learner = meta_learning_cartpole()\n",
    "#     return float(learner.parameterized_reward(observation, action))\n",
    "\n",
    "def energyBasedReward(observation, action):\n",
    "    \"\"\"Enhanced physics-based energy reward\"\"\"\n",
    "    return float(energy_reward.compute_reward(observation, action))\n",
    "\n",
    "\n",
    "def potentialBasedReward(observation, action):\n",
    "    \"\"\"Potential-based reward shaping for CartPole  - This one is not dynamic\"\"\"\n",
    "    x, x_dot, theta, theta_dot = observation\n",
    "    gamma = 0.99\n",
    "    \n",
    "    def potential(state):\n",
    "        # Potential function based on cart position and pole angle\n",
    "        # Higher potential for centered cart and upright pole\n",
    "        cart_potential = -(state[0] ** 2)  # Penalize distance from center\n",
    "        angle_potential = -((state[2] ** 2))  # Penalize angle from vertical\n",
    "        velocity_potential = -(state[1] ** 2)  # Penalize high velocities\n",
    "        ang_velocity_potential = -(state[3] ** 2)  # Penalize high angular velocities\n",
    "        \n",
    "        return cart_potential + 2*angle_potential + velocity_potential + ang_velocity_potential\n",
    "\n",
    "    current_potential = potential(observation)\n",
    "    next_potential = potential([x + x_dot, x_dot, theta + theta_dot, theta_dot])\n",
    "    \n",
    "    # PBRS formula: γΦ(s') - Φ(s)\n",
    "    shaped_reward = gamma * next_potential - current_potential\n",
    "    \n",
    "    return 1.0 + shaped_reward\n",
    "\n",
    "\n",
    "def baselineReward(observation, action):\n",
    "    \"\"\"Standard baseline reward\"\"\"\n",
    "    return 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runPerformanceComparisonTest(\n",
    "    episodes=1000, \n",
    "    changeInterval=500, \n",
    "    lengthchanges=[0.5, 1.5],\n",
    "    mass_cart=1.0,\n",
    "    mass_pole=0.1,\n",
    "    initial_length=0.5,\n",
    "    gravity=9.8,\n",
    "    seed=42\n",
    "):\n",
    "    print(f\"Starting Performance Comparison Test with seed {seed}...\")\n",
    "    \n",
    "    # Set all random seeds\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # Initialize base environment with seed\n",
    "    env = gym.make('CartPole-v1', max_episode_steps=2000, render_mode=None)\n",
    "    env.action_space.seed(seed)\n",
    "    env.observation_space.seed(seed)\n",
    "    env.reset(seed=seed)\n",
    "    \n",
    "    env = CustomCartPoleEnv(env, numComponents=2)\n",
    "    env.setEnvironmentParameters(masscart=mass_cart, length=lengthchanges[0], gravity=gravity)\n",
    "\n",
    "    \n",
    "    currentlengthidx = 0\n",
    "    \n",
    "    \n",
    "    # Initialize energy-based reward function\n",
    "    energy_reward = EnergyBasedRewardFunction(\n",
    "        mass_cart=mass_cart, \n",
    "        mass_pole=mass_pole, \n",
    "        length=initial_length, \n",
    "        gravity=gravity\n",
    "    )\n",
    "    \n",
    "    # Define all reward functions to test\n",
    "    rewardfunctions = {\n",
    "        'adaptivereward': {\n",
    "            'agent': DQLearningAgent(env, 4, 2, device),\n",
    "            'updatesystem': RewardUpdateSystem(apiKey, modelName), \n",
    "            'rewardfunction': None,\n",
    "            'update_method': 'llm'\n",
    "        },\n",
    "        'pbrs': {\n",
    "            'agent': DQLearningAgent(env, 4, 2, device),\n",
    "            'updatesystem': None,\n",
    "            'rewardfunction': potentialBasedReward,\n",
    "            'update_method': None\n",
    "        },\n",
    "        'energy_based': {\n",
    "            'agent': DQLearningAgent(env, 4, 2, device),\n",
    "            'updatesystem': energy_reward,\n",
    "            'rewardfunction': energy_reward.compute_reward,\n",
    "            'update_method': 'physics'\n",
    "        },\n",
    "        'baseline': {\n",
    "            'agent': DQLearningAgent(env, 4, 2, device),\n",
    "            'updatesystem': None,\n",
    "            'rewardfunction': baselineReward,\n",
    "            'update_method': None\n",
    "        }\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # Create a specific order for testing\n",
    "    test_order = ['adaptivereward', 'energy_based', 'baseline', 'pbrs']\n",
    "    \n",
    "    # Test each reward function in the specified order\n",
    "    for rewardname in test_order:\n",
    "        rewardinfo = rewardfunctions[rewardname]\n",
    "        print(f\"\\nTesting reward function: {rewardname}\")\n",
    "    \n",
    "        env.reset()\n",
    "        if rewardname == 'adaptivereward':\n",
    "            # Initialize both components for adaptive reward\n",
    "            env.setComponentReward(1, stabilityReward)  # Set initial stability component\n",
    "            env.setComponentReward(2, efficiencyReward)  # Set initial efficiency component\n",
    "            rewardinfo['updatesystem'].lastUpdateEpisode = 0\n",
    "        else:\n",
    "            env.setRewardFunction(rewardinfo['rewardfunction'])\n",
    "        \n",
    "        episoderewards = []\n",
    "        episodebalancetimes = []\n",
    "        rewardchangeepisodes = []\n",
    "        \n",
    "        def onEpisodeEnd(env, updatesystem, episode, reward, steps):\n",
    "            nonlocal episoderewards, episodebalancetimes, rewardchangeepisodes, currentlengthidx\n",
    "            \n",
    "            # Record episode results\n",
    "            episoderewards.append(reward)\n",
    "            episodebalancetimes.append(steps)\n",
    "            \n",
    "            # Create metrics dictionary\n",
    "            metrics = {\n",
    "                'currentEpisode': episode,\n",
    "                'recentRewards': episoderewards[-100:] if len(episoderewards) > 100 else episoderewards,\n",
    "                'averageBalanceTime': np.mean(episodebalancetimes[-100:]) if episodebalancetimes else 0,\n",
    "                'balanceTimeVariance': np.var(episodebalancetimes[-100:]) if len(episodebalancetimes) > 1 else 0\n",
    "            }\n",
    "            \n",
    "            # Debug print for metrics\n",
    "            if episode % 1000 == 0:\n",
    "                print(f\"\\nMetrics Debug at Episode {episode}:\")\n",
    "                print(f\"Recent Average Reward: {np.mean(metrics['recentRewards']):.2f}\")\n",
    "                print(f\"Average Balance Time: {metrics['averageBalanceTime']:.2f}\")\n",
    "                print(f\"Balance Time Variance: {metrics['balanceTimeVariance']:.2f}\")\n",
    "                \n",
    "                if rewardname == 'adaptivereward' and hasattr(env, 'getCurrentWeights'):\n",
    "                    weights = env.getCurrentWeights()\n",
    "                    print(f\"Component Weights - Stability: {weights['stability']:.2f}, \"\n",
    "                          f\"Efficiency: {weights['efficiency']:.2f}\")\n",
    "            \n",
    "            # Handle LLM updates only for adaptive reward\n",
    "            if rewardname == 'adaptivereward' and updatesystem is not None:\n",
    "                # Only show major milestones\n",
    "                if episode % 1000 == 0:\n",
    "                    print(f\"\\nEpisode {episode} - Time Since Last Update: {episode - updatesystem.lastUpdateEpisode}\")\n",
    "                \n",
    "                for component in range(1, 3):\n",
    "                    updatesystem.targetComponent = component\n",
    "                    if updatesystem.waitingTime(f'component_{component}', metrics, updatesystem.lastUpdateEpisode):\n",
    "                        current_func = env.rewardComponents[f'rewardFunction{component}']\n",
    "                        new_function, updated = updatesystem.validateAndUpdate(current_func)\n",
    "                        \n",
    "                        if updated:\n",
    "                            env.setComponentReward(component, new_function)\n",
    "                            rewardchangeepisodes.append(episode)\n",
    "                            updatesystem.lastUpdateEpisode = episode\n",
    "                            print(f\"✓ LLM update for component {component} at episode {episode}\")\n",
    "            \n",
    "            # Handle physics-based updates\n",
    "            elif rewardinfo['update_method'] == 'physics':\n",
    "                if episode % changeInterval == 0 and episode > 0:\n",
    "                    print(f\"\\nUpdating physics-based reward at episode {episode}\")\n",
    "                    updatesystem.length = lengthchanges[currentlengthidx]\n",
    "                    env.setRewardFunction(updatesystem.compute_reward)\n",
    "                    rewardchangeepisodes.append(episode)\n",
    "                    print(\"✓ Physics-based update completed\")\n",
    "            \n",
    "            # Environment changes\n",
    "            if episode % changeInterval == 0 and episode > 0:\n",
    "                currentlengthidx = (currentlengthidx + 1) % len(lengthchanges)\n",
    "                newlength = lengthchanges[currentlengthidx]\n",
    "                env.setEnvironmentParameters(length=newlength)\n",
    "                print(f\"\\nChanged pole length to: {newlength}m at episode {episode}\")\n",
    "        \n",
    "        # Train the agent\n",
    "        agent, env, rewards = trainDQLearning(\n",
    "            agent=rewardinfo['agent'],\n",
    "            env=env,\n",
    "            numEpisodes=episodes,\n",
    "            updateSystem=rewardinfo['updatesystem'],\n",
    "            onEpisodeEnd=onEpisodeEnd\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        results[rewardname] = {\n",
    "            'rewards': episoderewards,\n",
    "            'balancetimes': episodebalancetimes,\n",
    "            'rewardChanges': rewardchangeepisodes\n",
    "        }\n",
    "        \n",
    "        # Print final performance metrics\n",
    "        print(f\"\\nCompleted testing {rewardname}\")\n",
    "        print(f\"Final average reward: {np.mean(episoderewards[-100:]):.2f}\")\n",
    "        print(f\"Final average balance time: {np.mean(episodebalancetimes[-100:]):.2f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Experiment\n",
    "\n",
    "changeInterval = 20000\n",
    "\n",
    "# results = runPerformanceComparisonTest(\n",
    "#     episodes=40000,  \n",
    "#     changeInterval=changeInterval,\n",
    "#     mass_cart=1.0,\n",
    "#     lengthchanges=[0.3, 0.9]  \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizePerformanceComparison(results, changeInterval, folder_path):\n",
    "    \"\"\"\n",
    "    Create and save performance comparison visualizations\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 15))\n",
    "    \n",
    "    # Color map for different reward functions\n",
    "    colors = ['b', 'g', 'r', 'c', 'm']\n",
    "    \n",
    "    # Plot rewards for each reward function with variance\n",
    "    for idx, (rewardname, rewardresults) in enumerate(results.items()):\n",
    "        rewards = pd.Series(rewardresults['rewards'])\n",
    "        \n",
    "        # Calculate rolling mean and standard deviation for rewards\n",
    "        window = 50\n",
    "        rolling_mean_rewards = rewards.rolling(window=window).mean()\n",
    "        rolling_std_rewards = rewards.rolling(window=window).std()\n",
    "        \n",
    "        # Plot mean line for rewards\n",
    "        ax1.plot(rolling_mean_rewards, \n",
    "                label=f'{rewardname}', \n",
    "                linewidth=2, \n",
    "                color=colors[idx])\n",
    "        \n",
    "        # Plot variance area for rewards\n",
    "        ax1.fill_between(\n",
    "            range(len(rewards)),\n",
    "            rolling_mean_rewards - rolling_std_rewards,\n",
    "            rolling_mean_rewards + rolling_std_rewards,\n",
    "            color=colors[idx],\n",
    "            alpha=0.2\n",
    "        )\n",
    "        \n",
    "        # Add vertical lines for environment changes (red)\n",
    "        change_episodes = range(changeInterval, len(rewards), changeInterval)\n",
    "        for ep in change_episodes:\n",
    "            ax1.axvline(x=ep, color='r', linestyle='--', alpha=0.3,\n",
    "                       label='Environment Change' if ep == change_episodes[0] else None)\n",
    "        \n",
    "        # Add vertical lines for reward function changes (green)\n",
    "        if 'rewardChanges' in rewardresults:\n",
    "            for ep in rewardresults['rewardChanges']:\n",
    "                ax1.axvline(x=ep, color='g', linestyle='--', alpha=0.3,\n",
    "                          label=f'{rewardname} Update' if ep == rewardresults['rewardChanges'][0] else None)\n",
    "    \n",
    "    ax1.set_title('Average Reward Over Time with Variance')\n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.set_ylabel('Reward')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot balance times\n",
    "    for idx, (rewardname, rewardresults) in enumerate(results.items()):\n",
    "        balancetimes = pd.Series(rewardresults['balancetimes'])\n",
    "        \n",
    "        rolling_mean_balance = balancetimes.rolling(window=window).mean()\n",
    "        rolling_std_balance = balancetimes.rolling(window=window).std()\n",
    "        \n",
    "        ax2.plot(rolling_mean_balance,\n",
    "                label=f'{rewardname}', \n",
    "                linewidth=2, \n",
    "                color=colors[idx])\n",
    "        \n",
    "        ax2.fill_between(\n",
    "            range(len(balancetimes)),\n",
    "            rolling_mean_balance - rolling_std_balance,\n",
    "            rolling_mean_balance + rolling_std_balance,\n",
    "            color=colors[idx],\n",
    "            alpha=0.2\n",
    "        )\n",
    "        \n",
    "        # Add vertical lines for environment changes\n",
    "        for ep in change_episodes:\n",
    "            ax2.axvline(x=ep, color='r', linestyle='--', alpha=0.3,\n",
    "                       label='Environment Change' if ep == change_episodes[0] else None)\n",
    "        \n",
    "        # Add vertical lines for reward function changes\n",
    "        if 'rewardChanges' in rewardresults:\n",
    "            for ep in rewardresults['rewardChanges']:\n",
    "                ax2.axvline(x=ep, color='g', linestyle='--', alpha=0.3,\n",
    "                          label=f'{rewardname} Update' if ep == rewardresults['rewardChanges'][0] else None)\n",
    "    \n",
    "    ax2.set_title('Average Balance Time Over Episodes with Variance')\n",
    "    ax2.set_xlabel('Episode')\n",
    "    ax2.set_ylabel('Steps')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    # Plot environment parameters\n",
    "    env_param_history = []\n",
    "    for episode in range(len(next(iter(results.values()))['rewards'])):\n",
    "        idx = (episode // changeInterval) % 2\n",
    "        length = 0.3 if idx == 0 else 0.9  # Alternating between 0.3 and 0.9\n",
    "        env_param_history.append(length)\n",
    "    \n",
    "    ax3.plot(env_param_history, label='Pole Length', color='purple')\n",
    "    ax3.set_title('Environment Parameters Over Episodes')\n",
    "    ax3.set_xlabel('Episode')\n",
    "    ax3.set_ylabel('Pole Length (m)')\n",
    "    ax3.grid(True)\n",
    "    ax3.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plots\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filepath = os.path.join(folder_path, f\"performance_comparison_{timestamp}.png\")\n",
    "    plt.savefig(filepath, bbox_inches='tight', dpi=300)\n",
    "    print(f\"Saved plot: performance_comparison_{timestamp}.png in {folder_path}\")\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "    \n",
    "def calculateStability(rewards):\n",
    "    \"\"\"\n",
    "    Calculate stability score based on reward variance in the last 100 episodes\n",
    "    Lower variance = higher stability\n",
    "    \"\"\"\n",
    "    if len(rewards) < 100:\n",
    "        return 0.0\n",
    "    \n",
    "    last_hundred = rewards[-100:]\n",
    "    mean_reward = np.mean(last_hundred)\n",
    "    if mean_reward == 0:\n",
    "        return 0.0\n",
    "        \n",
    "    # Calculate coefficient of variation (normalized standard deviation)\n",
    "    stability = 1 - (np.std(last_hundred) / mean_reward)\n",
    "    return max(0, min(1, stability))  # Normalize between 0 and 1\n",
    "\n",
    "def calculateConvergenceTime(rewards, threshold=195, window=50):\n",
    "    \"\"\"\n",
    "    Calculate the number of episodes needed to reach and maintain a certain performance\n",
    "    threshold for a given window of episodes\n",
    "    \"\"\"\n",
    "    if len(rewards) < window:\n",
    "        return len(rewards)\n",
    "    \n",
    "    rolling_mean = pd.Series(rewards).rolling(window).mean()\n",
    "    \n",
    "    for episode in range(window, len(rewards)):\n",
    "        if rolling_mean[episode] >= threshold:\n",
    "            # Check if performance is maintained\n",
    "            maintained = all(avg >= threshold * 0.9 for avg in rolling_mean[episode:episode+window])\n",
    "            if maintained:\n",
    "                return episode\n",
    "    \n",
    "    return len(rewards)  # If never converged, return total episodes\n",
    "\n",
    "def calculatePerformanceMetrics(results):\n",
    "    \"\"\"Calculate performance metrics for each reward type\"\"\"\n",
    "    metrics = {}\n",
    "    for rewardname, rewardresults in results.items():\n",
    "        metrics[rewardname] = {\n",
    "            'finalavgreward': np.mean(rewardresults['rewards'][-100:]),\n",
    "            'finalavgbalance': np.mean(rewardresults['balancetimes'][-100:]),\n",
    "            'convergencetime': calculateConvergenceTime(rewardresults['rewards']),\n",
    "            'stability': calculateStability(rewardresults['rewards'])\n",
    "        }\n",
    "    return pd.DataFrame(metrics).T\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def saveMetricsTable(metrics, filename, folder_path):\n",
    "    \"\"\"Save metrics table to specified folder\"\"\"\n",
    "    # Create figure for metrics table\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Create table with formatted metrics\n",
    "    table = ax.table(\n",
    "        cellText=metrics.values.round(3),\n",
    "        colLabels=metrics.columns,\n",
    "        rowLabels=metrics.index,\n",
    "        cellLoc='center',\n",
    "        loc='center'\n",
    "    )\n",
    "    \n",
    "    # Adjust font size and scaling\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(9)\n",
    "    table.scale(1.2, 1.5)\n",
    "    \n",
    "    plt.title(\"Performance Metrics Comparison\")\n",
    "    \n",
    "    # Save with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filepath = os.path.join(folder_path, f\"{filename}_{timestamp}.png\")\n",
    "    plt.savefig(filepath, bbox_inches='tight', dpi=300)\n",
    "    print(f\"Saved metrics table: {filename}_{timestamp}.png in {folder_path}\")\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize the results\n",
    "# visualizePerformanceComparison(results,changeInterval)\n",
    "\n",
    "\n",
    "# # Calculate and display the metrics\n",
    "# metrics = calculatePerformanceMetrics(results)\n",
    "# print(\"\\nPerformance Metrics:\")\n",
    "# print(metrics)\n",
    "    \n",
    "# Call the function\n",
    "# saveMetricsTable(metrics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple runs to generate confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createExperimentFolder():\n",
    "    \"\"\"Create a timestamped folder for experiment results\"\"\"\n",
    "    from datetime import datetime\n",
    "    import os\n",
    "    \n",
    "    # Create base experiment folder if it doesn't exist\n",
    "    if not os.path.exists(\"PerformanceExperiment\"):\n",
    "        os.makedirs(\"PerformanceExperiment\")\n",
    "    \n",
    "    # Create timestamped subfolder\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    experiment_folder = os.path.join(\"PerformanceExperiment\", f\"experiment_{timestamp}\")\n",
    "    os.makedirs(experiment_folder)\n",
    "    \n",
    "    return experiment_folder\n",
    "\n",
    "def savePlot(fig, filename, folder_path):\n",
    "    \"\"\"Save plot to specified folder with timestamp\"\"\"\n",
    "    from datetime import datetime\n",
    "    import os\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filepath = os.path.join(folder_path, f\"{filename}_{timestamp}.png\")\n",
    "    fig.savefig(filepath, bbox_inches='tight', dpi=300)\n",
    "    print(f\"Saved plot: {filename}_{timestamp}.png in {folder_path}\")\n",
    "    \n",
    "def saveMetricsTable(metrics, filename, folder_path):\n",
    "    \"\"\"Save metrics table to specified folder\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    from datetime import datetime\n",
    "    import os\n",
    "    \n",
    "    # Style the DataFrame for visualization\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    table = ax.table(cellText=metrics.values,\n",
    "                    colLabels=metrics.columns,\n",
    "                    rowLabels=metrics.index,\n",
    "                    cellLoc='center',\n",
    "                    loc='center')\n",
    "    \n",
    "    # Adjust font size and scaling\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(9)\n",
    "    table.scale(1.2, 1.5)\n",
    "    \n",
    "    plt.title(\"Performance Metrics Comparison\")\n",
    "    \n",
    "    # Save with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filepath = os.path.join(folder_path, f\"{filename}_{timestamp}.png\")\n",
    "    plt.savefig(filepath, bbox_inches='tight', dpi=300)\n",
    "    print(f\"Saved metrics table: {filename}_{timestamp}.png in {folder_path}\")\n",
    "    plt.close()\n",
    "\n",
    "def runMultipleExperiments(numRuns=4, episodes=40000, changeInterval=20000):\n",
    "    \"\"\"\n",
    "    Run multiple experiments and save results in organized folders\n",
    "    \"\"\"\n",
    "    # Create main experiment folder\n",
    "    experiment_folder = createExperimentFolder()\n",
    "    \n",
    "    allResults = []\n",
    "    allMetrics = []\n",
    "    aggregatedMetrics = {}\n",
    "    \n",
    "    # Create run folders\n",
    "    for run in range(numRuns):\n",
    "        print(f\"\\nStarting Run {run + 1}/{numRuns}\")\n",
    "        \n",
    "        # Create folder for this run\n",
    "        run_folder = os.path.join(experiment_folder, f\"run_{run + 1}\")\n",
    "        os.makedirs(run_folder)\n",
    "        \n",
    "        # Run experiment\n",
    "        results = runPerformanceComparisonTest(\n",
    "            episodes=episodes,\n",
    "            changeInterval=changeInterval,\n",
    "            mass_cart=1.0,\n",
    "            lengthchanges=[0.3, 0.9]\n",
    "        )\n",
    "        \n",
    "        # Calculate metrics for this run\n",
    "        metrics = calculatePerformanceMetrics(results)\n",
    "        \n",
    "        # Store results\n",
    "        allResults.append(results)\n",
    "        allMetrics.append(metrics)\n",
    "        \n",
    "        # Store metrics by reward type\n",
    "        for idx, row in metrics.iterrows():\n",
    "            if idx not in aggregatedMetrics:\n",
    "                aggregatedMetrics[idx] = []\n",
    "            aggregatedMetrics[idx].append(row.to_dict())\n",
    "        \n",
    "        # Visualize and save individual run results\n",
    "        visualizePerformanceComparison(results, changeInterval, run_folder)\n",
    "        saveMetricsTable(metrics, f\"metrics_run_{run + 1}\", run_folder)\n",
    "    \n",
    "    # Calculate confidence intervals (95%)\n",
    "    confidenceIntervals = {}\n",
    "    resultsTable = pd.DataFrame()\n",
    "    \n",
    "    for rewardType, metrics_list in aggregatedMetrics.items():\n",
    "        metrics_df = pd.DataFrame(metrics_list)\n",
    "        means = metrics_df.mean()\n",
    "        cis = 1.96 * metrics_df.std() / np.sqrt(numRuns)\n",
    "        \n",
    "        # Create row for this reward type\n",
    "        resultsTable.loc[rewardType, 'Average Reward'] = f\"{means['finalavgreward']:.2f} ± {cis['finalavgreward']:.2f}\"\n",
    "        resultsTable.loc[rewardType, 'Average Balance'] = f\"{means['finalavgbalance']:.2f} ± {cis['finalavgbalance']:.2f}\"\n",
    "        resultsTable.loc[rewardType, 'Convergence Time'] = f\"{means['convergencetime']:.2f} ± {cis['convergencetime']:.2f}\"\n",
    "        resultsTable.loc[rewardType, 'Stability'] = f\"{means['stability']:.2f} ± {cis['stability']:.2f}\"\n",
    "        \n",
    "        # Store the raw values for possible further analysis\n",
    "        confidenceIntervals[rewardType] = {\n",
    "            'finalavgreward': {'mean': means['finalavgreward'], 'ci': cis['finalavgreward']},\n",
    "            'finalavgbalance': {'mean': means['finalavgbalance'], 'ci': cis['finalavgbalance']},\n",
    "            'convergencetime': {'mean': means['convergencetime'], 'ci': cis['convergencetime']},\n",
    "            'stability': {'mean': means['stability'], 'ci': cis['stability']}\n",
    "        }\n",
    "    \n",
    "    # Save aggregate statistics\n",
    "    with open(os.path.join(experiment_folder, \"aggregate_statistics.txt\"), \"w\") as f:\n",
    "        f.write(\"Aggregate Statistics:\\n\")\n",
    "        for rewardType, metrics in confidenceIntervals.items():\n",
    "            f.write(f\"\\n{rewardType}:\\n\")\n",
    "            for metric, values in metrics.items():\n",
    "                f.write(f\"{metric}: {values['mean']:.2f} ± {values['ci']:.2f}\\n\")\n",
    "    \n",
    "    # Save final results table\n",
    "    saveMetricsTable(resultsTable, \"final_results\", experiment_folder)\n",
    "    \n",
    "    print(f\"\\nExperiment results saved in: {experiment_folder}\")\n",
    "    return confidenceIntervals, allResults, resultsTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Run 1/2\n",
      "Starting Performance Comparison Test with seed 42...\n",
      "Environment parameters updated: masscart=1.0, length=0.3, gravity=9.8\n",
      "\n",
      "Testing reward function: adaptivereward\n",
      "\n",
      "Metrics Debug at Episode 0:\n",
      "Recent Average Reward: 20.82\n",
      "Average Balance Time: 25.00\n",
      "Balance Time Variance: 0.00\n",
      "Component Weights - Stability: 0.60, Efficiency: 0.40\n",
      "\n",
      "Episode 0 - Time Since Last Update: 0\n",
      "\n",
      "Episode 0/10000\n",
      "Average Reward: 20.82\n",
      "Average Steps: 25.00\n",
      "\n",
      "Episode 100/10000\n",
      "Average Reward: 21.85\n",
      "Average Steps: 22.61\n",
      "\n",
      "Episode 200/10000\n",
      "Average Reward: 35.34\n",
      "Average Steps: 35.92\n",
      "\n",
      "Episode 300/10000\n",
      "Average Reward: 56.94\n",
      "Average Steps: 57.51\n",
      "\n",
      "Episode 400/10000\n",
      "Average Reward: 112.95\n",
      "Average Steps: 114.72\n",
      "\n",
      "Episode 500/10000\n",
      "Average Reward: 145.42\n",
      "Average Steps: 147.54\n",
      "\n",
      "Episode 600/10000\n",
      "Average Reward: 77.41\n",
      "Average Steps: 78.38\n",
      "\n",
      "Episode 700/10000\n",
      "Average Reward: 110.22\n",
      "Average Steps: 110.58\n",
      "\n",
      "Episode 800/10000\n",
      "Average Reward: 345.41\n",
      "Average Steps: 346.15\n",
      "\n",
      "Episode 900/10000\n",
      "Average Reward: 317.99\n",
      "Average Steps: 318.78\n",
      "\n",
      "Metrics Debug at Episode 1000:\n",
      "Recent Average Reward: 336.26\n",
      "Average Balance Time: 337.17\n",
      "Balance Time Variance: 24516.72\n",
      "Component Weights - Stability: 0.60, Efficiency: 0.40\n",
      "\n",
      "Episode 1000 - Time Since Last Update: 1000\n",
      "\n",
      "Episode 1000/10000\n",
      "Average Reward: 336.26\n",
      "Average Steps: 337.17\n",
      "\n",
      "Episode 1100/10000\n",
      "Average Reward: 226.30\n",
      "Average Steps: 227.09\n",
      "\n",
      "Episode 1200/10000\n",
      "Average Reward: 223.08\n",
      "Average Steps: 223.21\n",
      "\n",
      "Episode 1300/10000\n",
      "Average Reward: 262.78\n",
      "Average Steps: 263.00\n",
      "\n",
      "Episode 1400/10000\n",
      "Average Reward: 331.92\n",
      "Average Steps: 331.98\n",
      "\n",
      "Episode 1500/10000\n",
      "Average Reward: 485.70\n",
      "Average Steps: 485.84\n",
      "\n",
      "Performance Analysis for component_1:\n",
      "Current Performance: 312.57\n",
      "Historical Best: 644.16\n",
      "Relative Performance: 48.5%\n",
      "\n",
      "Generating new stability reward function...\n",
      "\n",
      "Proposed Function:\n",
      "Here's a modified version of the stability reward component function with small, targeted improvements and detailed inline comments:\n",
      "\n",
      "```python\n",
      "def stabilityReward(state, action, next_state):\n",
      "    \"\"\"\n",
      "    Calculate the stability reward component based on the agent's state and action.\n",
      "    \n",
      "    Args:\n",
      "    state (array): Current state of the agent\n",
      "    action (array): Action taken by the agent\n",
      "    next_state (array): Resulting state after taking the action\n",
      "    \n",
      "    Returns:\n",
      "    float: Stability reward value\n",
      "    \"\"\"\n",
      "    # Extract relevant state information\n",
      "    current_angle = state[2]  # Assuming index 2 contains the angle\n",
      "    next_angle = next_state[2]\n",
      "    \n",
      "    # Calculate change in angle\n",
      "    angle_change = abs(next_angle - current_angle)\n",
      "    \n",
      "    # Base stability reward\n",
      "    stability_reward = 1.0\n",
      "    \n",
      "    # Penalize large angle changes\n",
      "    if angle_change > 0.1:  # Slightly reduced threshold for more sensitivity\n",
      "        stability_reward -= angle_change * 2  # Increased penalty multiplier\n",
      "    \n",
      "    # Bonus for maintaining near-vertical position\n",
      "    if abs(next_angle) < 0.05:  # Tightened threshold for verticality\n",
      "        stability_reward += 0.5  # Increased bonus for better vertical stability\n",
      "    \n",
      "    # Penalize extreme angles\n",
      "    if abs(next_angle) > 0.5:  # Added penalty for more extreme angles\n",
      "        stability_reward -= 1.0\n",
      "    \n",
      "    # Reward smooth actions\n",
      "    action_magnitude = np.linalg.norm(action)\n",
      "    if action_magnitude < 0.5:  # Encourage smoother, smaller actions\n",
      "        stability_reward += 0.2\n",
      "    \n",
      "    # Ensure reward is non-negative\n",
      "    stability_reward = max(stability_reward, 0)\n",
      "    \n",
      "    return stability_reward\n",
      "```\n",
      "\n",
      "Explanation of changes:\n",
      "\n",
      "1. Slightly reduced the threshold for penalizing angle changes from 0.15 to 0.1, making the function more sensitive to smaller instabilities.\n",
      "\n",
      "2. Increased the penalty multiplier for angle changes from 1.5 to 2, providing a stronger disincentive for instability.\n",
      "\n",
      "3. Tightened the threshold for the near-vertical position bonus from 0.1 to 0.05, encouraging more precise vertical stability.\n",
      "\n",
      "4. Increased the bonus for maintaining a near-vertical position from 0.3 to 0.5, providing a stronger incentive for stability.\n",
      "\n",
      "5. Added a new penalty for extreme angles (>0.5 radians) to discourage the agent from allowing the pole to deviate too far from vertical.\n",
      "\n",
      "6. Introduced a small reward for smooth actions, encouraging the agent to make smaller, more controlled adjustments.\n",
      "\n",
      "7. Ensured the final reward is non-negative to prevent potential issues with negative rewards in the learning process.\n",
      "\n",
      "These changes aim to incrementally improve the stability reward while maintaining the core structure and successful elements of the original function. The modifications should encourage more stable behavior and smoother actions without dramatically altering the reward landscape.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sd37/.conda/envs/thesis/lib/python3.10/site-packages/numpy/_core/fromnumeric.py:3904: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/sd37/.conda/envs/thesis/lib/python3.10/site-packages/numpy/_core/_methods.py:147: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ LLM update for component 1 at episode 1505\n",
      "\n",
      "Episode 1600/10000\n",
      "Average Reward: 42.10\n",
      "Average Steps: 87.55\n",
      "\n",
      "Episode 1700/10000\n",
      "Average Reward: 4.12\n",
      "Average Steps: 10.81\n",
      "\n",
      "Episode 1800/10000\n",
      "Average Reward: 39.02\n",
      "Average Steps: 97.77\n",
      "\n",
      "Episode 1900/10000\n",
      "Average Reward: 65.32\n",
      "Average Steps: 163.52\n",
      "\n",
      "Metrics Debug at Episode 2000:\n",
      "Recent Average Reward: 35.11\n",
      "Average Balance Time: 88.00\n",
      "Balance Time Variance: 119261.88\n",
      "Component Weights - Stability: 0.60, Efficiency: 0.40\n",
      "\n",
      "Episode 2000 - Time Since Last Update: 495\n",
      "\n",
      "Episode 2000/10000\n",
      "Average Reward: 35.11\n",
      "Average Steps: 88.00\n",
      "\n",
      "Episode 2100/10000\n",
      "Average Reward: 24.41\n",
      "Average Steps: 61.17\n",
      "\n",
      "Episode 2200/10000\n",
      "Average Reward: 25.90\n",
      "Average Steps: 64.90\n",
      "\n",
      "Episode 2300/10000\n",
      "Average Reward: 11.27\n",
      "Average Steps: 28.32\n",
      "\n",
      "Episode 2400/10000\n",
      "Average Reward: 13.85\n",
      "Average Steps: 34.91\n",
      "\n",
      "Episode 2500/10000\n",
      "Average Reward: 12.47\n",
      "Average Steps: 31.49\n",
      "\n",
      "Episode 2600/10000\n",
      "Average Reward: 10.82\n",
      "Average Steps: 27.36\n",
      "\n",
      "Episode 2700/10000\n",
      "Average Reward: 11.55\n",
      "Average Steps: 29.21\n",
      "\n",
      "Episode 2800/10000\n",
      "Average Reward: 12.16\n",
      "Average Steps: 30.75\n",
      "\n",
      "Episode 2900/10000\n",
      "Average Reward: 13.76\n",
      "Average Steps: 34.76\n",
      "\n",
      "Metrics Debug at Episode 3000:\n",
      "Recent Average Reward: 78.72\n",
      "Average Balance Time: 197.01\n",
      "Balance Time Variance: 149465.37\n",
      "Component Weights - Stability: 0.60, Efficiency: 0.40\n",
      "\n",
      "Episode 3000 - Time Since Last Update: 1495\n",
      "\n",
      "Episode 3000/10000\n",
      "Average Reward: 78.72\n",
      "Average Steps: 197.01\n",
      "\n",
      "Performance Analysis for component_1:\n",
      "Current Performance: 42.82\n",
      "Historical Best: 112.68\n",
      "Relative Performance: 38.0%\n",
      "\n",
      "Generating new stability reward function...\n",
      "\n",
      "Proposed Function:\n",
      "Here's the modified function with conservative improvements and detailed inline comments:\n",
      "\n",
      "```python\n",
      "def reward_function(state, action, next_state):\n",
      "    \"\"\"\n",
      "    Calculate the stability reward component based on the agent's state and action.\n",
      "    \n",
      "    Args:\n",
      "    state (array): Current state of the agent\n",
      "    action (array): Action taken by the agent\n",
      "    next_state (array): Resulting state after taking the action\n",
      "    \n",
      "    Returns:\n",
      "    float: Stability reward value\n",
      "    \"\"\"\n",
      "    # Extract relevant state information\n",
      "    current_angle = state[2]  # Assuming index 2 contains the angle\n",
      "    next_angle = next_state[2]\n",
      "    \n",
      "    # Calculate change in angle\n",
      "    angle_change = abs(next_angle - current_angle)\n",
      "    \n",
      "    # Base stability reward\n",
      "    stability_reward = 1.0\n",
      "    \n",
      "    # Penalize large angle changes (kept the same)\n",
      "    if angle_change > 0.1:\n",
      "        stability_reward -= angle_change * 2\n",
      "    \n",
      "    # Bonus for maintaining near-vertical position (slightly adjusted)\n",
      "    if abs(next_angle) < 0.06:  # Slightly relaxed threshold for verticality\n",
      "        stability_reward += 0.4  # Slightly reduced bonus to balance with other rewards\n",
      "    \n",
      "    # Penalize extreme angles (kept the same)\n",
      "    if abs(next_angle) > 0.5:\n",
      "        stability_reward -= 1.0\n",
      "    \n",
      "    # Reward smooth actions (adjusted)\n",
      "    action_magnitude = np.linalg.norm(action)\n",
      "    if action_magnitude < 0.6:  # Slightly increased threshold for smoother actions\n",
      "        stability_reward += 0.25  # Slightly increased reward for smooth actions\n",
      "    \n",
      "    # NEW: Small penalty for rapid angle changes\n",
      "    if angle_change > 0.05:  # Introduce a smaller threshold for subtle changes\n",
      "        stability_reward -= angle_change * 0.5  # Mild penalty for quick movements\n",
      "    \n",
      "    # NEW: Slight reward for angle improvement\n",
      "    if abs(next_angle) < abs(current_angle):\n",
      "        stability_reward += 0.1  # Small reward for moving towards vertical\n",
      "    \n",
      "    # Ensure reward is non-negative\n",
      "    stability_reward = max(stability_reward, 0)\n",
      "    \n",
      "    return stability_reward\n",
      "```\n",
      "\n",
      "These modifications make small, targeted improvements while maintaining the core structure:\n",
      "\n",
      "1. Slightly relaxed the near-vertical position threshold and adjusted its reward.\n",
      "2. Increased the threshold and reward for smooth actions to encourage more stable behavior.\n",
      "3. Added a small penalty for rapid angle changes to discourage quick, unstable movements.\n",
      "4. Introduced a slight reward for improving the angle towards vertical.\n",
      "\n",
      "These changes aim to provide more nuanced feedback for stability while keeping the overall reward structure intact.\n",
      "✓ LLM update for component 1 at episode 3025\n",
      "\n",
      "Episode 3100/10000\n",
      "Average Reward: 17.19\n",
      "Average Steps: 43.22\n",
      "\n",
      "Episode 3200/10000\n",
      "Average Reward: 13.11\n",
      "Average Steps: 33.04\n",
      "\n",
      "Episode 3300/10000\n",
      "Average Reward: 20.21\n",
      "Average Steps: 50.81\n",
      "\n",
      "Episode 3400/10000\n",
      "Average Reward: 20.35\n",
      "Average Steps: 51.17\n",
      "\n",
      "Episode 3500/10000\n",
      "Average Reward: 21.59\n",
      "Average Steps: 54.27\n",
      "\n",
      "Episode 3600/10000\n",
      "Average Reward: 46.61\n",
      "Average Steps: 116.80\n",
      "\n",
      "Episode 3700/10000\n",
      "Average Reward: 109.87\n",
      "Average Steps: 274.88\n",
      "\n",
      "Episode 3800/10000\n",
      "Average Reward: 13.34\n",
      "Average Steps: 33.64\n",
      "\n",
      "Episode 3900/10000\n",
      "Average Reward: 603.70\n",
      "Average Steps: 1509.27\n",
      "\n",
      "Metrics Debug at Episode 4000:\n",
      "Recent Average Reward: 85.98\n",
      "Average Balance Time: 215.15\n",
      "Balance Time Variance: 143672.71\n",
      "Component Weights - Stability: 0.60, Efficiency: 0.40\n",
      "\n",
      "Episode 4000 - Time Since Last Update: 975\n",
      "\n",
      "Episode 4000/10000\n",
      "Average Reward: 85.98\n",
      "Average Steps: 215.15\n",
      "\n",
      "Episode 4100/10000\n",
      "Average Reward: 11.37\n",
      "Average Steps: 28.69\n",
      "\n",
      "Episode 4200/10000\n",
      "Average Reward: 46.93\n",
      "Average Steps: 117.59\n",
      "\n",
      "Episode 4300/10000\n",
      "Average Reward: 16.35\n",
      "Average Steps: 41.12\n",
      "\n",
      "Episode 4400/10000\n",
      "Average Reward: 12.53\n",
      "Average Steps: 31.42\n",
      "\n",
      "Episode 4500/10000\n",
      "Average Reward: 32.35\n",
      "Average Steps: 81.19\n",
      "\n",
      "Episode 4600/10000\n",
      "Average Reward: 36.31\n",
      "Average Steps: 91.54\n",
      "\n",
      "Episode 4700/10000\n",
      "Average Reward: 39.72\n",
      "Average Steps: 99.55\n",
      "\n",
      "Performance Analysis for component_1:\n",
      "Current Performance: 25.74\n",
      "Historical Best: 52.73\n",
      "Relative Performance: 48.8%\n",
      "\n",
      "Generating new stability reward function...\n",
      "\n",
      "Proposed Function:\n",
      "Here's the modified function with conservative improvements and detailed inline comments:\n",
      "\n",
      "```python\n",
      "def reward_function(state, action, next_state):\n",
      "    \"\"\"\n",
      "    Calculate the stability reward component based on the agent's state and action.\n",
      "    \n",
      "    Args:\n",
      "    state (array): Current state of the agent\n",
      "    action (array): Action taken by the agent\n",
      "    next_state (array): Resulting state after taking the action\n",
      "    \n",
      "    Returns:\n",
      "    float: Stability reward value\n",
      "    \"\"\"\n",
      "    # Extract relevant state information\n",
      "    current_angle = state[2]  # Assuming index 2 contains the angle\n",
      "    next_angle = next_state[2]\n",
      "    \n",
      "    # Calculate change in angle\n",
      "    angle_change = abs(next_angle - current_angle)\n",
      "    \n",
      "    # Base stability reward\n",
      "    stability_reward = 1.0\n",
      "    \n",
      "    # Penalize large angle changes (kept the same)\n",
      "    if angle_change > 0.1:\n",
      "        stability_reward -= angle_change * 2\n",
      "    \n",
      "    # Bonus for maintaining near-vertical position (slightly adjusted)\n",
      "    if abs(next_angle) < 0.07:  # Slightly increased threshold for more lenient verticality\n",
      "        stability_reward += 0.35  # Slightly reduced bonus to balance with other rewards\n",
      "    \n",
      "    # Penalize extreme angles (kept the same)\n",
      "    if abs(next_angle) > 0.5:\n",
      "        stability_reward -= 1.0\n",
      "    \n",
      "    # Reward smooth actions (adjusted)\n",
      "    action_magnitude = np.linalg.norm(action)\n",
      "    if action_magnitude < 0.65:  # Slightly increased threshold for smoother actions\n",
      "        stability_reward += 0.3  # Slightly increased reward for smooth actions\n",
      "    \n",
      "    # Small penalty for rapid angle changes (adjusted)\n",
      "    if angle_change > 0.04:  # Slightly reduced threshold to catch more subtle changes\n",
      "        stability_reward -= angle_change * 0.6  # Slightly increased penalty for quick movements\n",
      "    \n",
      "    # Slight reward for angle improvement (adjusted)\n",
      "    if abs(next_angle) < abs(current_angle):\n",
      "        stability_reward += 0.15  # Slightly increased reward for moving towards vertical\n",
      "    \n",
      "    # NEW: Gradual penalty for increasing angles\n",
      "    angle_difference = abs(next_angle) - abs(current_angle)\n",
      "    if angle_difference > 0:\n",
      "        stability_reward -= angle_difference * 0.2  # Small penalty for moving away from vertical\n",
      "    \n",
      "    # Ensure reward is non-negative\n",
      "    stability_reward = max(stability_reward, 0)\n",
      "    \n",
      "    return stability_reward\n",
      "```\n",
      "\n",
      "Explanation of changes:\n",
      "1. Slightly increased the threshold for the near-vertical position bonus from 0.06 to 0.07, allowing for a bit more leniency.\n",
      "2. Reduced the near-vertical bonus from 0.4 to 0.35 to maintain balance with other rewards.\n",
      "3. Increased the threshold for smooth actions from 0.6 to 0.65, encouraging slightly smoother actions.\n",
      "4. Increased the reward for smooth actions from 0.25 to 0.3 to emphasize their importance.\n",
      "5. Reduced the threshold for rapid angle changes from 0.05 to 0.04 to catch more subtle movements.\n",
      "6. Increased the penalty for rapid angle changes from 0.5 to 0.6 to discourage quick movements more strongly.\n",
      "7. Increased the reward for angle improvement from 0.1 to 0.15 to encourage moving towards vertical more.\n",
      "8. Added a new gradual penalty for increasing angles, providing a small disincentive for moving away from vertical.\n",
      "\n",
      "These changes are conservative and aim to fine-tune the existing reward structure without dramatically altering its core components. The focus remains on stability, with slight adjustments to encourage smoother actions and more consistent vertical positioning.\n",
      "✓ LLM update for component 1 at episode 4702\n",
      "\n",
      "Episode 4800/10000\n",
      "Average Reward: 19.61\n",
      "Average Steps: 49.16\n",
      "\n",
      "Episode 4900/10000\n",
      "Average Reward: 69.98\n",
      "Average Steps: 175.17\n",
      "\n",
      "Metrics Debug at Episode 5000:\n",
      "Recent Average Reward: 131.83\n",
      "Average Balance Time: 329.78\n",
      "Balance Time Variance: 8080.23\n",
      "Component Weights - Stability: 0.60, Efficiency: 0.40\n",
      "\n",
      "Episode 5000 - Time Since Last Update: 298\n",
      "Environment parameters updated: masscart=1.0, length=0.9, gravity=9.8\n",
      "\n",
      "Changed pole length to: 0.9m at episode 5000\n",
      "\n",
      "Episode 5000/10000\n",
      "Average Reward: 131.83\n",
      "Average Steps: 329.78\n",
      "\n",
      "Episode 5100/10000\n",
      "Average Reward: 99.64\n",
      "Average Steps: 249.32\n",
      "\n",
      "Episode 5200/10000\n",
      "Average Reward: 68.44\n",
      "Average Steps: 171.32\n",
      "\n",
      "Episode 5300/10000\n",
      "Average Reward: 66.43\n",
      "Average Steps: 166.32\n",
      "\n",
      "Episode 5400/10000\n",
      "Average Reward: 60.79\n",
      "Average Steps: 152.23\n",
      "\n",
      "Episode 5500/10000\n",
      "Average Reward: 57.08\n",
      "Average Steps: 142.94\n",
      "\n",
      "Episode 5600/10000\n",
      "Average Reward: 48.03\n",
      "Average Steps: 120.27\n",
      "\n",
      "Episode 5700/10000\n",
      "Average Reward: 38.74\n",
      "Average Steps: 97.04\n",
      "\n",
      "Episode 5800/10000\n",
      "Average Reward: 33.96\n",
      "Average Steps: 85.07\n",
      "\n",
      "Episode 5900/10000\n",
      "Average Reward: 30.52\n",
      "Average Steps: 76.47\n",
      "\n",
      "Metrics Debug at Episode 6000:\n",
      "Recent Average Reward: 28.80\n",
      "Average Balance Time: 72.18\n",
      "Balance Time Variance: 25.33\n",
      "Component Weights - Stability: 0.60, Efficiency: 0.40\n",
      "\n",
      "Episode 6000 - Time Since Last Update: 1298\n",
      "\n",
      "Episode 6000/10000\n",
      "Average Reward: 28.80\n",
      "Average Steps: 72.18\n",
      "\n",
      "Episode 6100/10000\n",
      "Average Reward: 31.48\n",
      "Average Steps: 78.90\n",
      "\n",
      "Episode 6200/10000\n",
      "Average Reward: 17.85\n",
      "Average Steps: 45.07\n",
      "\n",
      "Performance Analysis for component_1:\n",
      "Current Performance: 3.54\n",
      "Historical Best: 30.66\n",
      "Relative Performance: 11.6%\n",
      "\n",
      "Generating new stability reward function...\n",
      "\n",
      "Proposed Function:\n",
      "Here's the modified function with conservative improvements and detailed inline comments:\n",
      "\n",
      "```python\n",
      "def reward_function(state, action, next_state):\n",
      "    \"\"\"\n",
      "    Calculate the stability reward component based on the agent's state and action.\n",
      "    \n",
      "    Args:\n",
      "    state (array): Current state of the agent\n",
      "    action (array): Action taken by the agent\n",
      "    next_state (array): Resulting state after taking the action\n",
      "    \n",
      "    Returns:\n",
      "    float: Stability reward value\n",
      "    \"\"\"\n",
      "    # Extract relevant state information\n",
      "    current_angle = state[2]  # Assuming index 2 contains the angle\n",
      "    next_angle = next_state[2]\n",
      "    \n",
      "    # Calculate change in angle\n",
      "    angle_change = abs(next_angle - current_angle)\n",
      "    \n",
      "    # Base stability reward\n",
      "    stability_reward = 1.0\n",
      "    \n",
      "    # Penalize large angle changes (kept the same)\n",
      "    if angle_change > 0.1:\n",
      "        stability_reward -= angle_change * 2\n",
      "    \n",
      "    # Bonus for maintaining near-vertical position (slightly adjusted)\n",
      "    if abs(next_angle) < 0.08:  # Slightly increased threshold for more lenient verticality\n",
      "        stability_reward += 0.4  # Slightly increased bonus to encourage vertical stability\n",
      "    \n",
      "    # Penalize extreme angles (kept the same)\n",
      "    if abs(next_angle) > 0.5:\n",
      "        stability_reward -= 1.0\n",
      "    \n",
      "    # Reward smooth actions (adjusted)\n",
      "    action_magnitude = np.linalg.norm(action)\n",
      "    if action_magnitude < 0.6:  # Slightly decreased threshold to encourage even smoother actions\n",
      "        stability_reward += 0.35  # Slightly increased reward for smooth actions\n",
      "    \n",
      "    # Small penalty for rapid angle changes (adjusted)\n",
      "    if angle_change > 0.035:  # Slightly reduced threshold to catch more subtle changes\n",
      "        stability_reward -= angle_change * 0.65  # Slightly increased penalty for quick movements\n",
      "    \n",
      "    # Slight reward for angle improvement (adjusted)\n",
      "    if abs(next_angle) < abs(current_angle):\n",
      "        stability_reward += 0.2  # Slightly increased reward for moving towards vertical\n",
      "    \n",
      "    # Gradual penalty for increasing angles (adjusted)\n",
      "    angle_difference = abs(next_angle) - abs(current_angle)\n",
      "    if angle_difference > 0:\n",
      "        stability_reward -= angle_difference * 0.25  # Slightly increased penalty for moving away from vertical\n",
      "    \n",
      "    # NEW: Small reward for maintaining low angular velocity\n",
      "    angular_velocity = state[5]  # Assuming index 5 contains angular velocity\n",
      "    if abs(angular_velocity) < 0.1:\n",
      "        stability_reward += 0.1  # Small reward for low angular velocity\n",
      "    \n",
      "    # Ensure reward is non-negative\n",
      "    stability_reward = max(stability_reward, 0)\n",
      "    \n",
      "    return stability_reward\n",
      "```\n",
      "\n",
      "Changes and explanations:\n",
      "\n",
      "1. Slightly increased the threshold for near-vertical position bonus from 0.07 to 0.08, and increased the bonus from 0.35 to 0.4. This provides a bit more leniency for vertical stability while still encouraging it.\n",
      "\n",
      "2. Adjusted the smooth actions threshold from 0.65 to 0.6, making it slightly stricter, and increased the reward from 0.3 to 0.35. This encourages even smoother actions.\n",
      "\n",
      "3. Further reduced the threshold for rapid angle changes from 0.04 to 0.035, and slightly increased the penalty multiplier from 0.6 to 0.65. This aims to catch and discourage more subtle quick movements.\n",
      "\n",
      "4. Increased the reward for angle improvement from 0.15 to 0.2, providing a stronger incentive for moving towards vertical.\n",
      "\n",
      "5. Slightly increased the penalty for increasing angles from 0.2 to 0.25, discouraging movement away from vertical more strongly.\n",
      "\n",
      "6. Added a new small reward for maintaining low angular velocity. This encourages the agent to keep rotational speed low, which contributes to overall stability.\n",
      "\n",
      "These changes are conservative and aim to fine-tune the existing reward structure, focusing on encouraging smoother actions, vertical stability, and penalizing undesirable movements slightly more.\n",
      "✓ LLM update for component 1 at episode 6202\n",
      "\n",
      "Episode 6300/10000\n",
      "Average Reward: 12.18\n",
      "Average Steps: 30.64\n",
      "\n",
      "Episode 6400/10000\n",
      "Average Reward: 23.03\n",
      "Average Steps: 57.87\n",
      "\n",
      "Episode 6500/10000\n",
      "Average Reward: 43.30\n",
      "Average Steps: 108.45\n",
      "\n",
      "Episode 6600/10000\n",
      "Average Reward: 24.53\n",
      "Average Steps: 61.49\n",
      "\n",
      "Episode 6700/10000\n",
      "Average Reward: 26.65\n",
      "Average Steps: 66.81\n",
      "\n",
      "Episode 6800/10000\n",
      "Average Reward: 29.56\n",
      "Average Steps: 74.18\n",
      "\n",
      "Episode 6900/10000\n",
      "Average Reward: 26.75\n",
      "Average Steps: 67.09\n",
      "\n",
      "Metrics Debug at Episode 7000:\n",
      "Recent Average Reward: 28.35\n",
      "Average Balance Time: 71.05\n",
      "Balance Time Variance: 81.93\n",
      "Component Weights - Stability: 0.60, Efficiency: 0.40\n",
      "\n",
      "Episode 7000 - Time Since Last Update: 798\n",
      "\n",
      "Episode 7000/10000\n",
      "Average Reward: 28.35\n",
      "Average Steps: 71.05\n",
      "\n",
      "Episode 7100/10000\n",
      "Average Reward: 28.07\n",
      "Average Steps: 70.38\n",
      "\n",
      "Episode 7200/10000\n",
      "Average Reward: 28.42\n",
      "Average Steps: 71.26\n",
      "\n",
      "Episode 7300/10000\n",
      "Average Reward: 31.44\n",
      "Average Steps: 78.83\n",
      "\n",
      "Episode 7400/10000\n",
      "Average Reward: 35.75\n",
      "Average Steps: 89.63\n",
      "\n",
      "Episode 7500/10000\n",
      "Average Reward: 33.86\n",
      "Average Steps: 84.96\n",
      "\n",
      "Episode 7600/10000\n",
      "Average Reward: 34.62\n",
      "Average Steps: 86.83\n",
      "\n",
      "Episode 7700/10000\n",
      "Average Reward: 14.34\n",
      "Average Steps: 36.34\n",
      "\n",
      "Performance Analysis for component_1:\n",
      "Current Performance: 3.62\n",
      "Historical Best: 23.96\n",
      "Relative Performance: 15.1%\n",
      "\n",
      "Generating new stability reward function...\n",
      "\n",
      "Proposed Function:\n",
      "Here's the modified function with conservative improvements and detailed inline comments:\n",
      "\n",
      "```python\n",
      "def reward_function(state, action, next_state):\n",
      "    \"\"\"\n",
      "    Calculate the stability reward component based on the agent's state and action.\n",
      "    \n",
      "    Args:\n",
      "    state (array): Current state of the agent\n",
      "    action (array): Action taken by the agent\n",
      "    next_state (array): Resulting state after taking the action\n",
      "    \n",
      "    Returns:\n",
      "    float: Stability reward value\n",
      "    \"\"\"\n",
      "    # Extract relevant state information\n",
      "    current_angle = state[2]  # Assuming index 2 contains the angle\n",
      "    next_angle = next_state[2]\n",
      "    \n",
      "    # Calculate change in angle\n",
      "    angle_change = abs(next_angle - current_angle)\n",
      "    \n",
      "    # Base stability reward\n",
      "    stability_reward = 1.0\n",
      "    \n",
      "    # Penalize large angle changes (kept the same)\n",
      "    if angle_change > 0.1:\n",
      "        stability_reward -= angle_change * 2\n",
      "    \n",
      "    # Bonus for maintaining near-vertical position (slightly adjusted)\n",
      "    if abs(next_angle) < 0.085:  # Slightly increased threshold for more lenient verticality\n",
      "        stability_reward += 0.45  # Slightly increased bonus to encourage vertical stability\n",
      "    \n",
      "    # Penalize extreme angles (kept the same)\n",
      "    if abs(next_angle) > 0.5:\n",
      "        stability_reward -= 1.0\n",
      "    \n",
      "    # Reward smooth actions (slightly adjusted)\n",
      "    action_magnitude = np.linalg.norm(action)\n",
      "    if action_magnitude < 0.62:  # Slightly increased threshold to allow for more action range\n",
      "        stability_reward += 0.38  # Slightly increased reward for smooth actions\n",
      "    \n",
      "    # Small penalty for rapid angle changes (adjusted)\n",
      "    if angle_change > 0.033:  # Slightly reduced threshold to catch more subtle changes\n",
      "        stability_reward -= angle_change * 0.68  # Slightly increased penalty for quick movements\n",
      "    \n",
      "    # Slight reward for angle improvement (adjusted)\n",
      "    if abs(next_angle) < abs(current_angle):\n",
      "        stability_reward += 0.22  # Slightly increased reward for moving towards vertical\n",
      "    \n",
      "    # Gradual penalty for increasing angles (adjusted)\n",
      "    angle_difference = abs(next_angle) - abs(current_angle)\n",
      "    if angle_difference > 0:\n",
      "        stability_reward -= angle_difference * 0.27  # Slightly increased penalty for moving away from vertical\n",
      "    \n",
      "    # Small reward for maintaining low angular velocity (adjusted)\n",
      "    angular_velocity = state[5]  # Assuming index 5 contains angular velocity\n",
      "    if abs(angular_velocity) < 0.095:  # Slightly reduced threshold for stricter control\n",
      "        stability_reward += 0.12  # Slightly increased reward for low angular velocity\n",
      "    \n",
      "    # NEW: Tiny reward for consistent actions\n",
      "    if len(state) > 6:  # Assuming previous action is stored in state\n",
      "        prev_action = state[6:]\n",
      "        action_consistency = np.linalg.norm(np.array(action) - np.array(prev_action))\n",
      "        if action_consistency < 0.1:\n",
      "            stability_reward += 0.05  # Small reward for consistent actions\n",
      "    \n",
      "    # Ensure reward is non-negative\n",
      "    stability_reward = max(stability_reward, 0)\n",
      "    \n",
      "    return stability_reward\n",
      "```\n",
      "\n",
      "The modifications made are conservative and incremental, focusing on fine-tuning the existing reward structure:\n",
      "\n",
      "1. Slightly adjusted the near-vertical position threshold and reward to encourage more precise control.\n",
      "2. Marginally increased the smooth action threshold to allow for a bit more action range while still promoting smooth movements.\n",
      "3. Fine-tuned the rapid angle change penalty to be slightly more sensitive to subtle changes.\n",
      "4. Incrementally increased rewards and penalties for moving towards or away from vertical to emphasize this behavior.\n",
      "5. Adjusted the low angular velocity reward to be slightly stricter and more rewarding.\n",
      "6. Added a new, small reward for consistent actions to promote stability over time.\n",
      "\n",
      "These changes aim to maintain the core reward structure while making small improvements to encourage more stable behavior. The adjustments are minimal to avoid drastic changes in performance.\n",
      "✓ LLM update for component 1 at episode 7702\n",
      "\n",
      "Episode 7800/10000\n",
      "Average Reward: 16.97\n",
      "Average Steps: 42.72\n",
      "\n",
      "Episode 7900/10000\n",
      "Average Reward: 15.59\n",
      "Average Steps: 39.13\n",
      "\n",
      "Metrics Debug at Episode 8000:\n",
      "Recent Average Reward: 26.66\n",
      "Average Balance Time: 66.84\n",
      "Balance Time Variance: 435.99\n",
      "Component Weights - Stability: 0.60, Efficiency: 0.40\n",
      "\n",
      "Episode 8000 - Time Since Last Update: 298\n",
      "\n",
      "Episode 8000/10000\n",
      "Average Reward: 26.66\n",
      "Average Steps: 66.84\n",
      "\n",
      "Episode 8100/10000\n",
      "Average Reward: 18.68\n",
      "Average Steps: 46.84\n",
      "\n",
      "Episode 8200/10000\n",
      "Average Reward: 18.52\n",
      "Average Steps: 46.46\n",
      "\n",
      "Episode 8300/10000\n",
      "Average Reward: 8.23\n",
      "Average Steps: 20.91\n",
      "\n",
      "Episode 8400/10000\n",
      "Average Reward: 49.47\n",
      "Average Steps: 123.90\n",
      "\n",
      "Episode 8500/10000\n",
      "Average Reward: 94.53\n",
      "Average Steps: 236.89\n",
      "\n",
      "Episode 8600/10000\n",
      "Average Reward: 99.31\n",
      "Average Steps: 248.62\n",
      "\n",
      "Episode 8700/10000\n",
      "Average Reward: 67.47\n",
      "Average Steps: 168.84\n",
      "\n",
      "Episode 8800/10000\n",
      "Average Reward: 45.22\n",
      "Average Steps: 113.26\n",
      "\n",
      "Episode 8900/10000\n",
      "Average Reward: 32.38\n",
      "Average Steps: 81.15\n",
      "\n",
      "Metrics Debug at Episode 9000:\n",
      "Recent Average Reward: 435.73\n",
      "Average Balance Time: 1089.47\n",
      "Balance Time Variance: 758973.29\n",
      "Component Weights - Stability: 0.60, Efficiency: 0.40\n",
      "\n",
      "Episode 9000 - Time Since Last Update: 1298\n",
      "\n",
      "Episode 9000/10000\n",
      "Average Reward: 435.73\n",
      "Average Steps: 1089.47\n",
      "\n",
      "Episode 9100/10000\n",
      "Average Reward: 411.14\n",
      "Average Steps: 1028.08\n",
      "\n",
      "Episode 9200/10000\n",
      "Average Reward: 341.93\n",
      "Average Steps: 855.00\n",
      "\n",
      "Episode 9300/10000\n",
      "Average Reward: 618.42\n",
      "Average Steps: 1546.11\n",
      "\n",
      "Performance Analysis for component_1:\n",
      "Current Performance: 391.80\n",
      "Historical Best: 800.00\n",
      "Relative Performance: 49.0%\n",
      "\n",
      "Generating new stability reward function...\n",
      "\n",
      "Proposed Function:\n",
      "Here's the modified function with small, targeted improvements and detailed inline comments:\n",
      "\n",
      "```python\n",
      "def reward_function(state, action, next_state):\n",
      "    \"\"\"\n",
      "    Calculate the stability reward component based on the agent's state and action.\n",
      "    \n",
      "    Args:\n",
      "    state (array): Current state of the agent\n",
      "    action (array): Action taken by the agent\n",
      "    next_state (array): Resulting state after taking the action\n",
      "    \n",
      "    Returns:\n",
      "    float: Stability reward value\n",
      "    \"\"\"\n",
      "    # Extract relevant state information\n",
      "    current_angle = state[2]  # Assuming index 2 contains the angle\n",
      "    next_angle = next_state[2]\n",
      "    \n",
      "    # Calculate change in angle\n",
      "    angle_change = abs(next_angle - current_angle)\n",
      "    \n",
      "    # Base stability reward\n",
      "    stability_reward = 1.0\n",
      "    \n",
      "    # Penalize large angle changes (kept the same)\n",
      "    if angle_change > 0.1:\n",
      "        stability_reward -= angle_change * 2\n",
      "    \n",
      "    # Bonus for maintaining near-vertical position (slightly adjusted)\n",
      "    if abs(next_angle) < 0.083:  # Slightly decreased threshold for stricter verticality\n",
      "        stability_reward += 0.47  # Slightly increased bonus to further encourage vertical stability\n",
      "    \n",
      "    # Penalize extreme angles (kept the same)\n",
      "    if abs(next_angle) > 0.5:\n",
      "        stability_reward -= 1.0\n",
      "    \n",
      "    # Reward smooth actions (slightly adjusted)\n",
      "    action_magnitude = np.linalg.norm(action)\n",
      "    if action_magnitude < 0.61:  # Slightly decreased threshold to encourage even smoother actions\n",
      "        stability_reward += 0.39  # Slightly increased reward for smooth actions\n",
      "    \n",
      "    # Small penalty for rapid angle changes (adjusted)\n",
      "    if angle_change > 0.032:  # Slightly decreased threshold to catch more subtle changes\n",
      "        stability_reward -= angle_change * 0.7  # Slightly increased penalty for quick movements\n",
      "    \n",
      "    # Slight reward for angle improvement (adjusted)\n",
      "    if abs(next_angle) < abs(current_angle):\n",
      "        stability_reward += 0.23  # Slightly increased reward for moving towards vertical\n",
      "    \n",
      "    # Gradual penalty for increasing angles (adjusted)\n",
      "    angle_difference = abs(next_angle) - abs(current_angle)\n",
      "    if angle_difference > 0:\n",
      "        stability_reward -= angle_difference * 0.28  # Slightly increased penalty for moving away from vertical\n",
      "    \n",
      "    # Small reward for maintaining low angular velocity (adjusted)\n",
      "    angular_velocity = state[5]  # Assuming index 5 contains angular velocity\n",
      "    if abs(angular_velocity) < 0.093:  # Slightly decreased threshold for stricter control\n",
      "        stability_reward += 0.13  # Slightly increased reward for low angular velocity\n",
      "    \n",
      "    # Reward for consistent actions (slightly adjusted)\n",
      "    if len(state) > 6:  # Assuming previous action is stored in state\n",
      "        prev_action = state[6:]\n",
      "        action_consistency = np.linalg.norm(np.array(action) - np.array(prev_action))\n",
      "        if action_consistency < 0.09:  # Slightly decreased threshold for stricter consistency\n",
      "            stability_reward += 0.06  # Slightly increased reward for consistent actions\n",
      "    \n",
      "    # NEW: Small penalty for oscillating actions\n",
      "    if len(state) > 6:\n",
      "        prev_action = state[6:]\n",
      "        if np.dot(action, prev_action) < 0:  # Check if actions are in opposite directions\n",
      "            stability_reward -= 0.05  # Small penalty for oscillating actions\n",
      "    \n",
      "    # Ensure reward is non-negative\n",
      "    stability_reward = max(stability_reward, 0)\n",
      "    \n",
      "    return stability_reward\n",
      "```\n",
      "\n",
      "The changes made are conservative and incremental, focusing on fine-tuning the existing reward structure:\n",
      "\n",
      "1. Slightly adjusted thresholds for verticality, smooth actions, and angle changes to be more strict.\n",
      "2. Marginally increased rewards and penalties to reinforce desired behaviors.\n",
      "3. Adjusted the action consistency reward to be slightly more impactful.\n",
      "4. Added a small penalty for oscillating actions to discourage rapid back-and-forth movements.\n",
      "\n",
      "These changes aim to further refine the stability aspects of the reward function while maintaining its core structure and successful elements.\n",
      "✓ LLM update for component 1 at episode 9303\n",
      "\n",
      "Episode 9400/10000\n",
      "Average Reward: 288.05\n",
      "Average Steps: 720.26\n",
      "\n",
      "Episode 9500/10000\n",
      "Average Reward: 766.19\n",
      "Average Steps: 1915.46\n",
      "\n",
      "Episode 9600/10000\n",
      "Average Reward: 340.09\n",
      "Average Steps: 850.36\n",
      "\n",
      "Episode 9700/10000\n",
      "Average Reward: 611.52\n",
      "Average Steps: 1528.83\n",
      "\n",
      "Episode 9800/10000\n",
      "Average Reward: 683.66\n",
      "Average Steps: 1709.19\n",
      "\n",
      "Episode 9900/10000\n",
      "Average Reward: 599.41\n",
      "Average Steps: 1498.57\n",
      "\n",
      "Completed testing adaptivereward\n",
      "Final average reward: 397.42\n",
      "Final average balance time: 993.67\n",
      "\n",
      "Testing reward function: energy_based\n",
      "\n",
      "Metrics Debug at Episode 0:\n",
      "Recent Average Reward: 10.08\n",
      "Average Balance Time: 26.00\n",
      "Balance Time Variance: 0.00\n",
      "\n",
      "Episode 0/10000\n",
      "Average Reward: 10.08\n",
      "Average Steps: 26.00\n",
      "\n",
      "Episode 100/10000\n",
      "Average Reward: 9.59\n",
      "Average Steps: 24.21\n",
      "\n",
      "Episode 200/10000\n",
      "Average Reward: 19.53\n",
      "Average Steps: 48.94\n",
      "\n",
      "Episode 300/10000\n",
      "Average Reward: 28.65\n",
      "Average Steps: 71.80\n",
      "\n",
      "Episode 400/10000\n",
      "Average Reward: 56.26\n",
      "Average Steps: 140.84\n",
      "\n",
      "Episode 500/10000\n",
      "Average Reward: 268.91\n",
      "Average Steps: 672.39\n",
      "\n",
      "Episode 600/10000\n",
      "Average Reward: 214.78\n",
      "Average Steps: 537.07\n",
      "\n",
      "Episode 700/10000\n",
      "Average Reward: 53.47\n",
      "Average Steps: 133.83\n",
      "\n",
      "Episode 800/10000\n",
      "Average Reward: 352.26\n",
      "Average Steps: 880.78\n",
      "\n",
      "Episode 900/10000\n",
      "Average Reward: 459.73\n",
      "Average Steps: 1149.41\n",
      "\n",
      "Metrics Debug at Episode 1000:\n",
      "Recent Average Reward: 553.24\n",
      "Average Balance Time: 1383.16\n",
      "Balance Time Variance: 562933.69\n",
      "\n",
      "Episode 1000/10000\n",
      "Average Reward: 553.24\n",
      "Average Steps: 1383.16\n",
      "\n",
      "Episode 1100/10000\n",
      "Average Reward: 617.34\n",
      "Average Steps: 1543.41\n",
      "\n",
      "Episode 1200/10000\n",
      "Average Reward: 668.50\n",
      "Average Steps: 1671.29\n",
      "\n",
      "Episode 1300/10000\n",
      "Average Reward: 129.96\n",
      "Average Steps: 325.35\n",
      "\n",
      "Episode 1400/10000\n",
      "Average Reward: 3.57\n",
      "Average Steps: 9.45\n",
      "\n",
      "Episode 1500/10000\n",
      "Average Reward: 3.57\n",
      "Average Steps: 9.44\n",
      "\n",
      "Episode 1600/10000\n",
      "Average Reward: 3.52\n",
      "Average Steps: 9.31\n",
      "\n",
      "Episode 1700/10000\n",
      "Average Reward: 5.23\n",
      "Average Steps: 13.42\n",
      "\n",
      "Episode 1800/10000\n",
      "Average Reward: 22.52\n",
      "Average Steps: 56.42\n",
      "\n",
      "Episode 1900/10000\n",
      "Average Reward: 24.23\n",
      "Average Steps: 60.68\n",
      "\n",
      "Metrics Debug at Episode 2000:\n",
      "Recent Average Reward: 13.05\n",
      "Average Balance Time: 32.72\n",
      "Balance Time Variance: 77.28\n",
      "\n",
      "Episode 2000/10000\n",
      "Average Reward: 13.05\n",
      "Average Steps: 32.72\n",
      "\n",
      "Episode 2100/10000\n",
      "Average Reward: 10.44\n",
      "Average Steps: 26.20\n",
      "\n",
      "Episode 2200/10000\n",
      "Average Reward: 9.31\n",
      "Average Steps: 23.38\n",
      "\n",
      "Episode 2300/10000\n",
      "Average Reward: 9.68\n",
      "Average Steps: 24.27\n",
      "\n",
      "Episode 2400/10000\n",
      "Average Reward: 9.14\n",
      "Average Steps: 22.94\n",
      "\n",
      "Episode 2500/10000\n",
      "Average Reward: 9.02\n",
      "Average Steps: 22.64\n",
      "\n",
      "Episode 2600/10000\n",
      "Average Reward: 9.38\n",
      "Average Steps: 23.54\n",
      "\n",
      "Episode 2700/10000\n",
      "Average Reward: 8.93\n",
      "Average Steps: 22.42\n",
      "\n",
      "Episode 2800/10000\n",
      "Average Reward: 9.06\n",
      "Average Steps: 22.75\n",
      "\n",
      "Episode 2900/10000\n",
      "Average Reward: 9.00\n",
      "Average Steps: 22.58\n",
      "\n",
      "Metrics Debug at Episode 3000:\n",
      "Recent Average Reward: 9.13\n",
      "Average Balance Time: 22.92\n",
      "Balance Time Variance: 15.71\n",
      "\n",
      "Episode 3000/10000\n",
      "Average Reward: 9.13\n",
      "Average Steps: 22.92\n",
      "\n",
      "Episode 3100/10000\n",
      "Average Reward: 9.56\n",
      "Average Steps: 23.98\n",
      "\n",
      "Episode 3200/10000\n",
      "Average Reward: 9.65\n",
      "Average Steps: 24.22\n",
      "\n",
      "Episode 3300/10000\n",
      "Average Reward: 10.07\n",
      "Average Steps: 25.26\n",
      "\n",
      "Episode 3400/10000\n",
      "Average Reward: 10.45\n",
      "Average Steps: 26.21\n",
      "\n",
      "Episode 3500/10000\n",
      "Average Reward: 10.08\n",
      "Average Steps: 25.29\n",
      "\n",
      "Episode 3600/10000\n",
      "Average Reward: 10.79\n",
      "Average Steps: 27.07\n",
      "\n",
      "Episode 3700/10000\n",
      "Average Reward: 10.21\n",
      "Average Steps: 25.61\n",
      "\n",
      "Episode 3800/10000\n",
      "Average Reward: 10.74\n",
      "Average Steps: 26.93\n",
      "\n",
      "Episode 3900/10000\n",
      "Average Reward: 10.28\n",
      "Average Steps: 25.80\n",
      "\n",
      "Metrics Debug at Episode 4000:\n",
      "Recent Average Reward: 10.24\n",
      "Average Balance Time: 25.69\n",
      "Balance Time Variance: 19.61\n",
      "\n",
      "Episode 4000/10000\n",
      "Average Reward: 10.24\n",
      "Average Steps: 25.69\n",
      "\n",
      "Episode 4100/10000\n",
      "Average Reward: 10.93\n",
      "Average Steps: 27.41\n",
      "\n",
      "Episode 4200/10000\n",
      "Average Reward: 10.86\n",
      "Average Steps: 27.23\n",
      "\n",
      "Episode 4300/10000\n",
      "Average Reward: 11.00\n",
      "Average Steps: 27.60\n",
      "\n",
      "Episode 4400/10000\n",
      "Average Reward: 12.02\n",
      "Average Steps: 30.14\n",
      "\n",
      "Episode 4500/10000\n",
      "Average Reward: 12.36\n",
      "Average Steps: 31.01\n",
      "\n",
      "Episode 4600/10000\n",
      "Average Reward: 12.83\n",
      "Average Steps: 32.16\n",
      "\n",
      "Episode 4700/10000\n",
      "Average Reward: 14.17\n",
      "Average Steps: 35.52\n",
      "\n",
      "Episode 4800/10000\n",
      "Average Reward: 18.28\n",
      "Average Steps: 45.81\n",
      "\n",
      "Episode 4900/10000\n",
      "Average Reward: 22.66\n",
      "Average Steps: 56.76\n",
      "\n",
      "Metrics Debug at Episode 5000:\n",
      "Recent Average Reward: 27.68\n",
      "Average Balance Time: 69.32\n",
      "Balance Time Variance: 3961.26\n",
      "\n",
      "Updating physics-based reward at episode 5000\n",
      "✓ Physics-based update completed\n",
      "Environment parameters updated: masscart=1.0, length=0.3, gravity=9.8\n",
      "\n",
      "Changed pole length to: 0.3m at episode 5000\n",
      "\n",
      "Episode 5000/10000\n",
      "Average Reward: 27.68\n",
      "Average Steps: 69.32\n",
      "\n",
      "Episode 5100/10000\n",
      "Average Reward: 32.07\n",
      "Average Steps: 80.29\n",
      "\n",
      "Episode 5200/10000\n",
      "Average Reward: 69.16\n",
      "Average Steps: 173.04\n",
      "\n",
      "Episode 5300/10000\n",
      "Average Reward: 81.82\n",
      "Average Steps: 204.68\n",
      "\n",
      "Episode 5400/10000\n",
      "Average Reward: 51.36\n",
      "Average Steps: 128.52\n",
      "\n",
      "Episode 5500/10000\n",
      "Average Reward: 37.73\n",
      "Average Steps: 94.46\n",
      "\n",
      "Episode 5600/10000\n",
      "Average Reward: 52.33\n",
      "Average Steps: 130.96\n",
      "\n",
      "Episode 5700/10000\n",
      "Average Reward: 27.28\n",
      "Average Steps: 68.46\n",
      "\n",
      "Episode 5800/10000\n",
      "Average Reward: 3.59\n",
      "Average Steps: 9.49\n",
      "\n",
      "Episode 5900/10000\n",
      "Average Reward: 14.22\n",
      "Average Steps: 35.86\n",
      "\n",
      "Metrics Debug at Episode 6000:\n",
      "Recent Average Reward: 3.54\n",
      "Average Balance Time: 9.36\n",
      "Balance Time Variance: 0.63\n",
      "\n",
      "Episode 6000/10000\n",
      "Average Reward: 3.54\n",
      "Average Steps: 9.36\n",
      "\n",
      "Episode 6100/10000\n",
      "Average Reward: 21.97\n",
      "Average Steps: 55.23\n",
      "\n",
      "Episode 6200/10000\n",
      "Average Reward: 28.96\n",
      "Average Steps: 72.52\n",
      "\n",
      "Episode 6300/10000\n",
      "Average Reward: 13.37\n",
      "Average Steps: 33.54\n",
      "\n",
      "Episode 6400/10000\n",
      "Average Reward: 12.93\n",
      "Average Steps: 32.42\n",
      "\n",
      "Episode 6500/10000\n",
      "Average Reward: 13.24\n",
      "Average Steps: 33.19\n",
      "\n",
      "Episode 6600/10000\n",
      "Average Reward: 14.37\n",
      "Average Steps: 36.03\n",
      "\n",
      "Episode 6700/10000\n",
      "Average Reward: 13.41\n",
      "Average Steps: 33.62\n",
      "\n",
      "Episode 6800/10000\n",
      "Average Reward: 15.48\n",
      "Average Steps: 38.81\n",
      "\n",
      "Episode 6900/10000\n",
      "Average Reward: 15.47\n",
      "Average Steps: 38.78\n",
      "\n",
      "Metrics Debug at Episode 7000:\n",
      "Recent Average Reward: 20.41\n",
      "Average Balance Time: 51.14\n",
      "Balance Time Variance: 3126.12\n",
      "\n",
      "Episode 7000/10000\n",
      "Average Reward: 20.41\n",
      "Average Steps: 51.14\n",
      "\n",
      "Episode 7100/10000\n",
      "Average Reward: 19.87\n",
      "Average Steps: 49.79\n",
      "\n",
      "Episode 7200/10000\n",
      "Average Reward: 19.65\n",
      "Average Steps: 49.24\n",
      "\n",
      "Episode 7300/10000\n",
      "Average Reward: 30.32\n",
      "Average Steps: 75.90\n",
      "\n",
      "Episode 7400/10000\n",
      "Average Reward: 24.51\n",
      "Average Steps: 61.39\n",
      "\n",
      "Episode 7500/10000\n",
      "Average Reward: 36.36\n",
      "Average Steps: 91.03\n",
      "\n",
      "Episode 7600/10000\n",
      "Average Reward: 34.63\n",
      "Average Steps: 86.68\n",
      "\n",
      "Episode 7700/10000\n",
      "Average Reward: 38.04\n",
      "Average Steps: 95.21\n",
      "\n",
      "Episode 7800/10000\n",
      "Average Reward: 30.09\n",
      "Average Steps: 75.33\n",
      "\n",
      "Episode 7900/10000\n",
      "Average Reward: 36.49\n",
      "Average Steps: 91.35\n",
      "\n",
      "Metrics Debug at Episode 8000:\n",
      "Recent Average Reward: 41.73\n",
      "Average Balance Time: 104.43\n",
      "Balance Time Variance: 23492.31\n",
      "\n",
      "Episode 8000/10000\n",
      "Average Reward: 41.73\n",
      "Average Steps: 104.43\n",
      "\n",
      "Episode 8100/10000\n",
      "Average Reward: 32.91\n",
      "Average Steps: 82.39\n",
      "\n",
      "Episode 8200/10000\n",
      "Average Reward: 34.49\n",
      "Average Steps: 86.34\n",
      "\n",
      "Episode 8300/10000\n",
      "Average Reward: 30.61\n",
      "Average Steps: 76.63\n",
      "\n",
      "Episode 8400/10000\n",
      "Average Reward: 36.85\n",
      "Average Steps: 92.23\n",
      "\n",
      "Episode 8500/10000\n",
      "Average Reward: 39.78\n",
      "Average Steps: 99.56\n",
      "\n",
      "Episode 8600/10000\n",
      "Average Reward: 34.47\n",
      "Average Steps: 86.28\n",
      "\n",
      "Episode 8700/10000\n",
      "Average Reward: 45.07\n",
      "Average Steps: 112.78\n",
      "\n",
      "Episode 8800/10000\n",
      "Average Reward: 43.36\n",
      "Average Steps: 108.52\n",
      "\n",
      "Episode 8900/10000\n",
      "Average Reward: 51.85\n",
      "Average Steps: 129.74\n",
      "\n",
      "Metrics Debug at Episode 9000:\n",
      "Recent Average Reward: 50.77\n",
      "Average Balance Time: 127.05\n",
      "Balance Time Variance: 31198.03\n",
      "\n",
      "Episode 9000/10000\n",
      "Average Reward: 50.77\n",
      "Average Steps: 127.05\n",
      "\n",
      "Episode 9100/10000\n",
      "Average Reward: 33.62\n",
      "Average Steps: 84.17\n",
      "\n",
      "Episode 9200/10000\n",
      "Average Reward: 72.89\n",
      "Average Steps: 182.35\n",
      "\n",
      "Episode 9300/10000\n",
      "Average Reward: 134.66\n",
      "Average Steps: 336.76\n",
      "\n",
      "Episode 9400/10000\n",
      "Average Reward: 137.71\n",
      "Average Steps: 344.41\n",
      "\n",
      "Episode 9500/10000\n",
      "Average Reward: 91.96\n",
      "Average Steps: 230.17\n",
      "\n",
      "Episode 9600/10000\n",
      "Average Reward: 3.78\n",
      "Average Steps: 9.90\n",
      "\n",
      "Episode 9700/10000\n",
      "Average Reward: 15.09\n",
      "Average Steps: 38.00\n",
      "\n",
      "Episode 9800/10000\n",
      "Average Reward: 3.47\n",
      "Average Steps: 9.21\n",
      "\n",
      "Episode 9900/10000\n",
      "Average Reward: 4.50\n",
      "Average Steps: 11.57\n",
      "\n",
      "Completed testing energy_based\n",
      "Final average reward: 13.82\n",
      "Final average balance time: 34.64\n",
      "\n",
      "Testing reward function: baseline\n",
      "\n",
      "Metrics Debug at Episode 0:\n",
      "Recent Average Reward: 8.51\n",
      "Average Balance Time: 22.00\n",
      "Balance Time Variance: 0.00\n",
      "\n",
      "Episode 0/10000\n",
      "Average Reward: 8.51\n",
      "Average Steps: 22.00\n",
      "\n",
      "Episode 100/10000\n",
      "Average Reward: 8.11\n",
      "Average Steps: 20.56\n",
      "\n",
      "Episode 200/10000\n",
      "Average Reward: 5.37\n",
      "Average Steps: 13.81\n",
      "\n",
      "Episode 300/10000\n",
      "Average Reward: 8.51\n",
      "Average Steps: 21.52\n",
      "\n",
      "Episode 400/10000\n",
      "Average Reward: 24.43\n",
      "Average Steps: 61.19\n",
      "\n",
      "Episode 500/10000\n",
      "Average Reward: 26.32\n",
      "Average Steps: 65.92\n",
      "\n",
      "Episode 600/10000\n",
      "Average Reward: 35.11\n",
      "Average Steps: 87.90\n",
      "\n",
      "Episode 700/10000\n",
      "Average Reward: 32.52\n",
      "Average Steps: 81.44\n",
      "\n",
      "Episode 800/10000\n",
      "Average Reward: 35.58\n",
      "Average Steps: 89.08\n",
      "\n",
      "Episode 900/10000\n",
      "Average Reward: 32.27\n",
      "Average Steps: 80.79\n",
      "\n",
      "Metrics Debug at Episode 1000:\n",
      "Recent Average Reward: 52.00\n",
      "Average Balance Time: 130.22\n",
      "Balance Time Variance: 22240.83\n",
      "\n",
      "Episode 1000/10000\n",
      "Average Reward: 52.00\n",
      "Average Steps: 130.22\n",
      "\n",
      "Episode 1100/10000\n",
      "Average Reward: 3.56\n",
      "Average Steps: 9.41\n",
      "\n",
      "Episode 1200/10000\n",
      "Average Reward: 3.58\n",
      "Average Steps: 9.48\n",
      "\n",
      "Episode 1300/10000\n",
      "Average Reward: 4.55\n",
      "Average Steps: 11.79\n",
      "\n",
      "Episode 1400/10000\n",
      "Average Reward: 21.69\n",
      "Average Steps: 54.43\n",
      "\n",
      "Episode 1500/10000\n",
      "Average Reward: 39.94\n",
      "Average Steps: 100.02\n",
      "\n",
      "Episode 1600/10000\n",
      "Average Reward: 188.91\n",
      "Average Steps: 472.36\n",
      "\n",
      "Episode 1700/10000\n",
      "Average Reward: 52.49\n",
      "Average Steps: 131.42\n",
      "\n",
      "Episode 1800/10000\n",
      "Average Reward: 14.29\n",
      "Average Steps: 35.90\n",
      "\n",
      "Episode 1900/10000\n",
      "Average Reward: 41.03\n",
      "Average Steps: 102.82\n",
      "\n",
      "Metrics Debug at Episode 2000:\n",
      "Recent Average Reward: 37.67\n",
      "Average Balance Time: 94.45\n",
      "Balance Time Variance: 486.57\n",
      "\n",
      "Episode 2000/10000\n",
      "Average Reward: 37.67\n",
      "Average Steps: 94.45\n",
      "\n",
      "Episode 2100/10000\n",
      "Average Reward: 27.92\n",
      "Average Steps: 70.00\n",
      "\n",
      "Episode 2200/10000\n",
      "Average Reward: 57.16\n",
      "Average Steps: 143.05\n",
      "\n",
      "Episode 2300/10000\n",
      "Average Reward: 44.68\n",
      "Average Steps: 111.86\n",
      "\n",
      "Episode 2400/10000\n",
      "Average Reward: 598.82\n",
      "Average Steps: 1497.07\n",
      "\n",
      "Episode 2500/10000\n",
      "Average Reward: 343.68\n",
      "Average Steps: 859.30\n",
      "\n",
      "Episode 2600/10000\n",
      "Average Reward: 671.28\n",
      "Average Steps: 1678.22\n",
      "\n",
      "Episode 2700/10000\n",
      "Average Reward: 165.71\n",
      "Average Steps: 414.45\n",
      "\n",
      "Episode 2800/10000\n",
      "Average Reward: 342.46\n",
      "Average Steps: 856.19\n",
      "\n",
      "Episode 2900/10000\n",
      "Average Reward: 323.46\n",
      "Average Steps: 808.74\n",
      "\n",
      "Metrics Debug at Episode 3000:\n",
      "Recent Average Reward: 220.14\n",
      "Average Balance Time: 550.50\n",
      "Balance Time Variance: 396674.27\n",
      "\n",
      "Episode 3000/10000\n",
      "Average Reward: 220.14\n",
      "Average Steps: 550.50\n",
      "\n",
      "Episode 3100/10000\n",
      "Average Reward: 514.54\n",
      "Average Steps: 1286.41\n",
      "\n",
      "Episode 3200/10000\n",
      "Average Reward: 32.76\n",
      "Average Steps: 82.22\n",
      "\n",
      "Episode 3300/10000\n",
      "Average Reward: 123.45\n",
      "Average Steps: 308.77\n",
      "\n",
      "Episode 3400/10000\n",
      "Average Reward: 65.77\n",
      "Average Steps: 164.58\n",
      "\n",
      "Episode 3500/10000\n",
      "Average Reward: 282.57\n",
      "Average Steps: 706.49\n",
      "\n",
      "Episode 3600/10000\n",
      "Average Reward: 256.20\n",
      "Average Steps: 640.61\n",
      "\n",
      "Episode 3700/10000\n",
      "Average Reward: 164.69\n",
      "Average Steps: 411.87\n",
      "\n",
      "Episode 3800/10000\n",
      "Average Reward: 118.77\n",
      "Average Steps: 297.05\n",
      "\n",
      "Episode 3900/10000\n",
      "Average Reward: 3.59\n",
      "Average Steps: 9.51\n",
      "\n",
      "Metrics Debug at Episode 4000:\n",
      "Recent Average Reward: 3.59\n",
      "Average Balance Time: 9.50\n",
      "Balance Time Variance: 1.13\n",
      "\n",
      "Episode 4000/10000\n",
      "Average Reward: 3.59\n",
      "Average Steps: 9.50\n",
      "\n",
      "Episode 4100/10000\n",
      "Average Reward: 3.52\n",
      "Average Steps: 9.30\n",
      "\n",
      "Episode 4200/10000\n",
      "Average Reward: 3.61\n",
      "Average Steps: 9.54\n",
      "\n",
      "Episode 4300/10000\n",
      "Average Reward: 3.54\n",
      "Average Steps: 9.37\n",
      "\n",
      "Episode 4400/10000\n",
      "Average Reward: 3.85\n",
      "Average Steps: 9.99\n",
      "\n",
      "Episode 4500/10000\n",
      "Average Reward: 7.43\n",
      "Average Steps: 18.82\n",
      "\n",
      "Episode 4600/10000\n",
      "Average Reward: 3.53\n",
      "Average Steps: 9.35\n",
      "\n",
      "Episode 4700/10000\n",
      "Average Reward: 3.60\n",
      "Average Steps: 9.52\n",
      "\n",
      "Episode 4800/10000\n",
      "Average Reward: 4.66\n",
      "Average Steps: 12.00\n",
      "\n",
      "Episode 4900/10000\n",
      "Average Reward: 8.17\n",
      "Average Steps: 20.61\n",
      "\n",
      "Metrics Debug at Episode 5000:\n",
      "Recent Average Reward: 9.61\n",
      "Average Balance Time: 24.15\n",
      "Balance Time Variance: 74.79\n",
      "Environment parameters updated: masscart=1.0, length=0.9, gravity=9.8\n",
      "\n",
      "Changed pole length to: 0.9m at episode 5000\n",
      "\n",
      "Episode 5000/10000\n",
      "Average Reward: 9.61\n",
      "Average Steps: 24.15\n",
      "\n",
      "Episode 5100/10000\n",
      "Average Reward: 8.52\n",
      "Average Steps: 21.43\n",
      "\n",
      "Episode 5200/10000\n",
      "Average Reward: 10.11\n",
      "Average Steps: 25.47\n",
      "\n",
      "Episode 5300/10000\n",
      "Average Reward: 10.73\n",
      "Average Steps: 26.97\n",
      "\n",
      "Episode 5400/10000\n",
      "Average Reward: 9.83\n",
      "Average Steps: 24.77\n",
      "\n",
      "Episode 5500/10000\n",
      "Average Reward: 3.57\n",
      "Average Steps: 9.43\n",
      "\n",
      "Episode 5600/10000\n",
      "Average Reward: 3.61\n",
      "Average Steps: 9.56\n",
      "\n",
      "Episode 5700/10000\n",
      "Average Reward: 3.60\n",
      "Average Steps: 9.51\n",
      "\n",
      "Episode 5800/10000\n",
      "Average Reward: 4.46\n",
      "Average Steps: 11.48\n",
      "\n",
      "Episode 5900/10000\n",
      "Average Reward: 7.86\n",
      "Average Steps: 19.80\n",
      "\n",
      "Metrics Debug at Episode 6000:\n",
      "Recent Average Reward: 31.34\n",
      "Average Balance Time: 78.45\n",
      "Balance Time Variance: 15530.95\n",
      "\n",
      "Episode 6000/10000\n",
      "Average Reward: 31.34\n",
      "Average Steps: 78.45\n",
      "\n",
      "Episode 6100/10000\n",
      "Average Reward: 14.33\n",
      "Average Steps: 35.93\n",
      "\n",
      "Episode 6200/10000\n",
      "Average Reward: 15.05\n",
      "Average Steps: 37.74\n",
      "\n",
      "Episode 6300/10000\n",
      "Average Reward: 14.32\n",
      "Average Steps: 35.91\n",
      "\n",
      "Episode 6400/10000\n",
      "Average Reward: 15.74\n",
      "Average Steps: 39.46\n",
      "\n",
      "Episode 6500/10000\n",
      "Average Reward: 16.15\n",
      "Average Steps: 40.50\n",
      "\n",
      "Episode 6600/10000\n",
      "Average Reward: 17.51\n",
      "Average Steps: 43.87\n",
      "\n",
      "Episode 6700/10000\n",
      "Average Reward: 24.38\n",
      "Average Steps: 61.05\n",
      "\n",
      "Episode 6800/10000\n",
      "Average Reward: 27.39\n",
      "Average Steps: 68.59\n",
      "\n",
      "Episode 6900/10000\n",
      "Average Reward: 31.26\n",
      "Average Steps: 78.27\n",
      "\n",
      "Metrics Debug at Episode 7000:\n",
      "Recent Average Reward: 36.14\n",
      "Average Balance Time: 90.47\n",
      "Balance Time Variance: 8178.31\n",
      "\n",
      "Episode 7000/10000\n",
      "Average Reward: 36.14\n",
      "Average Steps: 90.47\n",
      "\n",
      "Episode 7100/10000\n",
      "Average Reward: 67.32\n",
      "Average Steps: 168.43\n",
      "\n",
      "Episode 7200/10000\n",
      "Average Reward: 118.18\n",
      "Average Steps: 295.58\n",
      "\n",
      "Episode 7300/10000\n",
      "Average Reward: 132.86\n",
      "Average Steps: 332.30\n",
      "\n",
      "Episode 7400/10000\n",
      "Average Reward: 199.22\n",
      "Average Steps: 498.19\n",
      "\n",
      "Episode 7500/10000\n",
      "Average Reward: 180.78\n",
      "Average Steps: 452.09\n",
      "\n",
      "Episode 7600/10000\n",
      "Average Reward: 144.90\n",
      "Average Steps: 362.38\n",
      "\n",
      "Episode 7700/10000\n",
      "Average Reward: 79.75\n",
      "Average Steps: 199.51\n",
      "\n",
      "Episode 7800/10000\n",
      "Average Reward: 48.80\n",
      "Average Steps: 122.12\n",
      "\n",
      "Episode 7900/10000\n",
      "Average Reward: 50.20\n",
      "Average Steps: 125.64\n",
      "\n",
      "Metrics Debug at Episode 8000:\n",
      "Recent Average Reward: 60.77\n",
      "Average Balance Time: 152.04\n",
      "Balance Time Variance: 35491.14\n",
      "\n",
      "Episode 8000/10000\n",
      "Average Reward: 60.77\n",
      "Average Steps: 152.04\n",
      "\n",
      "Episode 8100/10000\n",
      "Average Reward: 99.03\n",
      "Average Steps: 247.71\n",
      "\n",
      "Episode 8200/10000\n",
      "Average Reward: 170.39\n",
      "Average Steps: 426.12\n",
      "\n",
      "Episode 8300/10000\n",
      "Average Reward: 29.76\n",
      "Average Steps: 74.81\n",
      "\n",
      "Episode 8400/10000\n",
      "Average Reward: 7.26\n",
      "Average Steps: 18.60\n",
      "\n",
      "Episode 8500/10000\n",
      "Average Reward: 3.53\n",
      "Average Steps: 9.34\n",
      "\n",
      "Episode 8600/10000\n",
      "Average Reward: 3.65\n",
      "Average Steps: 9.56\n",
      "\n",
      "Episode 8700/10000\n",
      "Average Reward: 32.48\n",
      "Average Steps: 81.32\n",
      "\n",
      "Episode 8800/10000\n",
      "Average Reward: 32.27\n",
      "Average Steps: 80.77\n",
      "\n",
      "Episode 8900/10000\n",
      "Average Reward: 16.83\n",
      "Average Steps: 42.20\n",
      "\n",
      "Metrics Debug at Episode 9000:\n",
      "Recent Average Reward: 10.52\n",
      "Average Balance Time: 26.39\n",
      "Balance Time Variance: 20.16\n",
      "\n",
      "Episode 9000/10000\n",
      "Average Reward: 10.52\n",
      "Average Steps: 26.39\n",
      "\n",
      "Episode 9100/10000\n",
      "Average Reward: 11.22\n",
      "Average Steps: 28.14\n",
      "\n",
      "Episode 9200/10000\n",
      "Average Reward: 13.80\n",
      "Average Steps: 34.61\n",
      "\n",
      "Episode 9300/10000\n",
      "Average Reward: 13.01\n",
      "Average Steps: 32.62\n",
      "\n",
      "Episode 9400/10000\n",
      "Average Reward: 37.93\n",
      "Average Steps: 94.94\n",
      "\n",
      "Episode 9500/10000\n",
      "Average Reward: 211.51\n",
      "Average Steps: 528.91\n",
      "\n",
      "Episode 9600/10000\n",
      "Average Reward: 124.61\n",
      "Average Steps: 311.68\n",
      "\n",
      "Episode 9700/10000\n",
      "Average Reward: 141.32\n",
      "Average Steps: 353.46\n",
      "\n",
      "Episode 9800/10000\n",
      "Average Reward: 336.97\n",
      "Average Steps: 842.53\n",
      "\n",
      "Episode 9900/10000\n",
      "Average Reward: 150.78\n",
      "Average Steps: 377.08\n",
      "\n",
      "Completed testing baseline\n",
      "Final average reward: 299.44\n",
      "Final average balance time: 748.72\n",
      "\n",
      "Testing reward function: pbrs\n",
      "\n",
      "Metrics Debug at Episode 0:\n",
      "Recent Average Reward: 6.48\n",
      "Average Balance Time: 17.00\n",
      "Balance Time Variance: 0.00\n",
      "\n",
      "Episode 0/10000\n",
      "Average Reward: 6.48\n",
      "Average Steps: 17.00\n",
      "\n",
      "Episode 100/10000\n",
      "Average Reward: 8.72\n",
      "Average Steps: 22.05\n",
      "\n",
      "Episode 200/10000\n",
      "Average Reward: 12.74\n",
      "Average Steps: 32.01\n",
      "\n",
      "Episode 300/10000\n",
      "Average Reward: 15.46\n",
      "Average Steps: 38.78\n",
      "\n",
      "Episode 400/10000\n",
      "Average Reward: 24.36\n",
      "Average Steps: 61.00\n",
      "\n",
      "Episode 500/10000\n",
      "Average Reward: 37.50\n",
      "Average Steps: 93.95\n",
      "\n",
      "Episode 600/10000\n",
      "Average Reward: 38.53\n",
      "Average Steps: 96.52\n",
      "\n",
      "Episode 700/10000\n",
      "Average Reward: 76.97\n",
      "Average Steps: 192.58\n",
      "\n",
      "Episode 800/10000\n",
      "Average Reward: 33.79\n",
      "Average Steps: 84.73\n",
      "\n",
      "Episode 900/10000\n",
      "Average Reward: 23.36\n",
      "Average Steps: 58.72\n",
      "\n",
      "Metrics Debug at Episode 1000:\n",
      "Recent Average Reward: 26.97\n",
      "Average Balance Time: 67.73\n",
      "Balance Time Variance: 5126.56\n",
      "\n",
      "Episode 1000/10000\n",
      "Average Reward: 26.97\n",
      "Average Steps: 67.73\n",
      "\n",
      "Episode 1100/10000\n",
      "Average Reward: 16.35\n",
      "Average Steps: 40.99\n",
      "\n",
      "Episode 1200/10000\n",
      "Average Reward: 62.29\n",
      "Average Steps: 155.91\n",
      "\n",
      "Episode 1300/10000\n",
      "Average Reward: 81.57\n",
      "Average Steps: 204.14\n",
      "\n",
      "Episode 1400/10000\n",
      "Average Reward: 65.84\n",
      "Average Steps: 164.85\n",
      "\n",
      "Episode 1500/10000\n",
      "Average Reward: 40.87\n",
      "Average Steps: 102.47\n",
      "\n",
      "Episode 1600/10000\n",
      "Average Reward: 49.38\n",
      "Average Steps: 123.78\n",
      "\n",
      "Episode 1700/10000\n",
      "Average Reward: 321.35\n",
      "Average Steps: 803.49\n",
      "\n",
      "Episode 1800/10000\n",
      "Average Reward: 366.85\n",
      "Average Steps: 917.17\n",
      "\n",
      "Episode 1900/10000\n",
      "Average Reward: 620.38\n",
      "Average Steps: 1550.98\n",
      "\n",
      "Metrics Debug at Episode 2000:\n",
      "Recent Average Reward: 104.51\n",
      "Average Balance Time: 261.50\n",
      "Balance Time Variance: 270482.69\n",
      "\n",
      "Episode 2000/10000\n",
      "Average Reward: 104.51\n",
      "Average Steps: 261.50\n",
      "\n",
      "Episode 2100/10000\n",
      "Average Reward: 22.58\n",
      "Average Steps: 56.59\n",
      "\n",
      "Episode 2200/10000\n",
      "Average Reward: 19.20\n",
      "Average Steps: 48.11\n",
      "\n",
      "Episode 2300/10000\n",
      "Average Reward: 18.95\n",
      "Average Steps: 47.47\n",
      "\n",
      "Episode 2400/10000\n",
      "Average Reward: 21.83\n",
      "Average Steps: 54.71\n",
      "\n",
      "Episode 2500/10000\n",
      "Average Reward: 25.31\n",
      "Average Steps: 63.72\n",
      "\n",
      "Episode 2600/10000\n",
      "Average Reward: 3.53\n",
      "Average Steps: 9.35\n",
      "\n",
      "Episode 2700/10000\n",
      "Average Reward: 3.89\n",
      "Average Steps: 10.22\n",
      "\n",
      "Episode 2800/10000\n",
      "Average Reward: 10.16\n",
      "Average Steps: 25.62\n",
      "\n",
      "Episode 2900/10000\n",
      "Average Reward: 18.58\n",
      "Average Steps: 46.55\n",
      "\n",
      "Metrics Debug at Episode 3000:\n",
      "Recent Average Reward: 27.74\n",
      "Average Balance Time: 69.47\n",
      "Balance Time Variance: 1228.05\n",
      "\n",
      "Episode 3000/10000\n",
      "Average Reward: 27.74\n",
      "Average Steps: 69.47\n",
      "\n",
      "Episode 3100/10000\n",
      "Average Reward: 25.65\n",
      "Average Steps: 64.25\n",
      "\n",
      "Episode 3200/10000\n",
      "Average Reward: 27.27\n",
      "Average Steps: 68.29\n",
      "\n",
      "Episode 3300/10000\n",
      "Average Reward: 22.48\n",
      "Average Steps: 56.31\n",
      "\n",
      "Episode 3400/10000\n",
      "Average Reward: 29.92\n",
      "Average Steps: 74.91\n",
      "\n",
      "Episode 3500/10000\n",
      "Average Reward: 33.42\n",
      "Average Steps: 83.67\n",
      "\n",
      "Episode 3600/10000\n",
      "Average Reward: 32.40\n",
      "Average Steps: 81.11\n",
      "\n",
      "Episode 3700/10000\n",
      "Average Reward: 24.23\n",
      "Average Steps: 60.84\n",
      "\n",
      "Episode 3800/10000\n",
      "Average Reward: 6.05\n",
      "Average Steps: 15.43\n",
      "\n",
      "Episode 3900/10000\n",
      "Average Reward: 6.89\n",
      "Average Steps: 17.54\n",
      "\n",
      "Metrics Debug at Episode 4000:\n",
      "Recent Average Reward: 14.40\n",
      "Average Balance Time: 36.22\n",
      "Balance Time Variance: 998.47\n",
      "\n",
      "Episode 4000/10000\n",
      "Average Reward: 14.40\n",
      "Average Steps: 36.22\n",
      "\n",
      "Episode 4100/10000\n",
      "Average Reward: 27.89\n",
      "Average Steps: 69.84\n",
      "\n",
      "Episode 4200/10000\n",
      "Average Reward: 13.77\n",
      "Average Steps: 34.52\n",
      "\n",
      "Episode 4300/10000\n",
      "Average Reward: 35.93\n",
      "Average Steps: 89.94\n",
      "\n",
      "Episode 4400/10000\n",
      "Average Reward: 48.49\n",
      "Average Steps: 121.37\n",
      "\n",
      "Episode 4500/10000\n",
      "Average Reward: 55.46\n",
      "Average Steps: 138.79\n",
      "\n",
      "Episode 4600/10000\n",
      "Average Reward: 47.90\n",
      "Average Steps: 119.88\n",
      "\n",
      "Episode 4700/10000\n",
      "Average Reward: 27.58\n",
      "Average Steps: 69.23\n",
      "\n",
      "Episode 4800/10000\n",
      "Average Reward: 82.44\n",
      "Average Steps: 206.24\n",
      "\n",
      "Episode 4900/10000\n",
      "Average Reward: 97.56\n",
      "Average Steps: 244.25\n",
      "\n",
      "Metrics Debug at Episode 5000:\n",
      "Recent Average Reward: 191.34\n",
      "Average Balance Time: 478.60\n",
      "Balance Time Variance: 608128.08\n",
      "Environment parameters updated: masscart=1.0, length=0.3, gravity=9.8\n",
      "\n",
      "Changed pole length to: 0.3m at episode 5000\n",
      "\n",
      "Episode 5000/10000\n",
      "Average Reward: 191.34\n",
      "Average Steps: 478.60\n",
      "\n",
      "Episode 5100/10000\n",
      "Average Reward: 38.89\n",
      "Average Steps: 97.41\n",
      "\n",
      "Episode 5200/10000\n",
      "Average Reward: 86.64\n",
      "Average Steps: 216.73\n",
      "\n",
      "Episode 5300/10000\n",
      "Average Reward: 67.19\n",
      "Average Steps: 168.10\n",
      "\n",
      "Episode 5400/10000\n",
      "Average Reward: 130.40\n",
      "Average Steps: 326.11\n",
      "\n",
      "Episode 5500/10000\n",
      "Average Reward: 37.61\n",
      "Average Steps: 94.17\n",
      "\n",
      "Episode 5600/10000\n",
      "Average Reward: 43.39\n",
      "Average Steps: 108.63\n",
      "\n",
      "Episode 5700/10000\n",
      "Average Reward: 49.46\n",
      "Average Steps: 123.81\n",
      "\n",
      "Episode 5800/10000\n",
      "Average Reward: 53.40\n",
      "Average Steps: 133.75\n",
      "\n",
      "Episode 5900/10000\n",
      "Average Reward: 80.55\n",
      "Average Steps: 201.61\n",
      "\n",
      "Metrics Debug at Episode 6000:\n",
      "Recent Average Reward: 357.16\n",
      "Average Balance Time: 893.00\n",
      "Balance Time Variance: 477651.46\n",
      "\n",
      "Episode 6000/10000\n",
      "Average Reward: 357.16\n",
      "Average Steps: 893.00\n",
      "\n",
      "Episode 6100/10000\n",
      "Average Reward: 211.50\n",
      "Average Steps: 529.03\n",
      "\n",
      "Episode 6200/10000\n",
      "Average Reward: 3.55\n",
      "Average Steps: 9.41\n",
      "\n",
      "Episode 6300/10000\n",
      "Average Reward: 7.45\n",
      "Average Steps: 19.13\n",
      "\n",
      "Episode 6400/10000\n",
      "Average Reward: 3.57\n",
      "Average Steps: 9.45\n",
      "\n",
      "Episode 6500/10000\n",
      "Average Reward: 3.55\n",
      "Average Steps: 9.39\n",
      "\n",
      "Episode 6600/10000\n",
      "Average Reward: 34.80\n",
      "Average Steps: 87.30\n",
      "\n",
      "Episode 6700/10000\n",
      "Average Reward: 23.56\n",
      "Average Steps: 59.02\n",
      "\n",
      "Episode 6800/10000\n",
      "Average Reward: 27.95\n",
      "Average Steps: 70.00\n",
      "\n",
      "Episode 6900/10000\n",
      "Average Reward: 65.98\n",
      "Average Steps: 165.11\n",
      "\n",
      "Metrics Debug at Episode 7000:\n",
      "Recent Average Reward: 159.54\n",
      "Average Balance Time: 399.09\n",
      "Balance Time Variance: 136011.66\n",
      "\n",
      "Episode 7000/10000\n",
      "Average Reward: 159.54\n",
      "Average Steps: 399.09\n",
      "\n",
      "Episode 7100/10000\n",
      "Average Reward: 62.61\n",
      "Average Steps: 156.72\n",
      "\n",
      "Episode 7200/10000\n",
      "Average Reward: 37.17\n",
      "Average Steps: 93.11\n",
      "\n",
      "Episode 7300/10000\n",
      "Average Reward: 102.57\n",
      "Average Steps: 256.60\n",
      "\n",
      "Episode 7400/10000\n",
      "Average Reward: 147.18\n",
      "Average Steps: 368.20\n",
      "\n",
      "Episode 7500/10000\n",
      "Average Reward: 3.52\n",
      "Average Steps: 9.33\n",
      "\n",
      "Episode 7600/10000\n",
      "Average Reward: 3.57\n",
      "Average Steps: 9.44\n",
      "\n",
      "Episode 7700/10000\n",
      "Average Reward: 3.57\n",
      "Average Steps: 9.46\n",
      "\n",
      "Episode 7800/10000\n",
      "Average Reward: 4.88\n",
      "Average Steps: 12.54\n",
      "\n",
      "Episode 7900/10000\n",
      "Average Reward: 33.45\n",
      "Average Steps: 83.77\n",
      "\n",
      "Metrics Debug at Episode 8000:\n",
      "Recent Average Reward: 32.38\n",
      "Average Balance Time: 81.10\n",
      "Balance Time Variance: 1775.07\n",
      "\n",
      "Episode 8000/10000\n",
      "Average Reward: 32.38\n",
      "Average Steps: 81.10\n",
      "\n",
      "Episode 8100/10000\n",
      "Average Reward: 32.36\n",
      "Average Steps: 81.07\n",
      "\n",
      "Episode 8200/10000\n",
      "Average Reward: 47.18\n",
      "Average Steps: 118.11\n",
      "\n",
      "Episode 8300/10000\n",
      "Average Reward: 62.61\n",
      "Average Steps: 156.77\n",
      "\n",
      "Episode 8400/10000\n",
      "Average Reward: 4.42\n",
      "Average Steps: 11.50\n",
      "\n",
      "Episode 8500/10000\n",
      "Average Reward: 3.71\n",
      "Average Steps: 9.76\n",
      "\n",
      "Episode 8600/10000\n",
      "Average Reward: 34.75\n",
      "Average Steps: 86.97\n",
      "\n",
      "Episode 8700/10000\n",
      "Average Reward: 42.48\n",
      "Average Steps: 106.33\n",
      "\n",
      "Episode 8800/10000\n",
      "Average Reward: 34.96\n",
      "Average Steps: 87.55\n",
      "\n",
      "Episode 8900/10000\n",
      "Average Reward: 25.51\n",
      "Average Steps: 63.91\n",
      "\n",
      "Metrics Debug at Episode 9000:\n",
      "Recent Average Reward: 16.40\n",
      "Average Balance Time: 41.10\n",
      "Balance Time Variance: 107.71\n",
      "\n",
      "Episode 9000/10000\n",
      "Average Reward: 16.40\n",
      "Average Steps: 41.10\n",
      "\n",
      "Episode 9100/10000\n",
      "Average Reward: 13.02\n",
      "Average Steps: 32.65\n",
      "\n",
      "Episode 9200/10000\n",
      "Average Reward: 13.24\n",
      "Average Steps: 33.21\n",
      "\n",
      "Episode 9300/10000\n",
      "Average Reward: 13.69\n",
      "Average Steps: 34.31\n",
      "\n",
      "Episode 9400/10000\n",
      "Average Reward: 16.17\n",
      "Average Steps: 40.53\n",
      "\n",
      "Episode 9500/10000\n",
      "Average Reward: 19.00\n",
      "Average Steps: 47.63\n",
      "\n",
      "Episode 9600/10000\n",
      "Average Reward: 35.20\n",
      "Average Steps: 88.15\n",
      "\n",
      "Episode 9700/10000\n",
      "Average Reward: 56.24\n",
      "Average Steps: 140.77\n",
      "\n",
      "Episode 9800/10000\n",
      "Average Reward: 45.60\n",
      "Average Steps: 114.15\n",
      "\n",
      "Episode 9900/10000\n",
      "Average Reward: 24.33\n",
      "Average Steps: 60.99\n",
      "\n",
      "Completed testing pbrs\n",
      "Final average reward: 23.58\n",
      "Final average balance time: 59.11\n",
      "Saved plot: performance_comparison_20250221_151117.png in PerformanceExperiment/experiment_20250221_144741/run_1\n",
      "Saved metrics table: metrics_run_1_20250221_151118.png in PerformanceExperiment/experiment_20250221_144741/run_1\n",
      "\n",
      "Starting Run 2/2\n",
      "Starting Performance Comparison Test with seed 42...\n",
      "Environment parameters updated: masscart=1.0, length=0.3, gravity=9.8\n",
      "\n",
      "Testing reward function: adaptivereward\n",
      "\n",
      "Metrics Debug at Episode 0:\n",
      "Recent Average Reward: 20.82\n",
      "Average Balance Time: 25.00\n",
      "Balance Time Variance: 0.00\n",
      "Component Weights - Stability: 0.60, Efficiency: 0.40\n",
      "\n",
      "Episode 0 - Time Since Last Update: 0\n",
      "\n",
      "Episode 0/10000\n",
      "Average Reward: 20.82\n",
      "Average Steps: 25.00\n",
      "\n",
      "Episode 100/10000\n",
      "Average Reward: 21.85\n",
      "Average Steps: 22.61\n",
      "\n",
      "Episode 200/10000\n",
      "Average Reward: 35.34\n",
      "Average Steps: 35.92\n",
      "\n",
      "Episode 300/10000\n",
      "Average Reward: 56.94\n",
      "Average Steps: 57.51\n",
      "\n",
      "Episode 400/10000\n",
      "Average Reward: 112.95\n",
      "Average Steps: 114.72\n",
      "\n",
      "Episode 500/10000\n",
      "Average Reward: 145.42\n",
      "Average Steps: 147.54\n",
      "\n",
      "Episode 600/10000\n",
      "Average Reward: 77.41\n",
      "Average Steps: 78.38\n",
      "\n",
      "Episode 700/10000\n",
      "Average Reward: 110.22\n",
      "Average Steps: 110.58\n",
      "\n",
      "Episode 800/10000\n",
      "Average Reward: 345.41\n",
      "Average Steps: 346.15\n",
      "\n",
      "Episode 900/10000\n",
      "Average Reward: 317.99\n",
      "Average Steps: 318.78\n",
      "\n",
      "Metrics Debug at Episode 1000:\n",
      "Recent Average Reward: 336.26\n",
      "Average Balance Time: 337.17\n",
      "Balance Time Variance: 24516.72\n",
      "Component Weights - Stability: 0.60, Efficiency: 0.40\n",
      "\n",
      "Episode 1000 - Time Since Last Update: 1000\n",
      "\n",
      "Episode 1000/10000\n",
      "Average Reward: 336.26\n",
      "Average Steps: 337.17\n",
      "\n",
      "Episode 1100/10000\n",
      "Average Reward: 226.30\n",
      "Average Steps: 227.09\n",
      "\n",
      "Episode 1200/10000\n",
      "Average Reward: 223.08\n",
      "Average Steps: 223.21\n",
      "\n",
      "Episode 1300/10000\n",
      "Average Reward: 262.78\n",
      "Average Steps: 263.00\n",
      "\n",
      "Episode 1400/10000\n",
      "Average Reward: 331.92\n",
      "Average Steps: 331.98\n",
      "\n",
      "Episode 1500/10000\n",
      "Average Reward: 485.70\n",
      "Average Steps: 485.84\n",
      "\n",
      "Performance Analysis for component_1:\n",
      "Current Performance: 312.57\n",
      "Historical Best: 644.16\n",
      "Relative Performance: 48.5%\n",
      "\n",
      "Generating new stability reward function...\n",
      "\n",
      "Proposed Function:\n",
      "Here's a modified version of the stability reward function with some conservative improvements and detailed inline comments:\n",
      "\n",
      "```python\n",
      "def stabilityReward(state, action, next_state):\n",
      "    # Extract relevant state variables\n",
      "    angle = state[2]\n",
      "    angular_velocity = state[3]\n",
      "    next_angle = next_state[2]\n",
      "    next_angular_velocity = next_state[3]\n",
      "\n",
      "    # Base reward for staying upright\n",
      "    base_reward = 1.0\n",
      "\n",
      "    # Angle penalty (unchanged)\n",
      "    angle_penalty = abs(angle) * 0.5\n",
      "\n",
      "    # Angular velocity penalty (slightly increased weight)\n",
      "    # Increased from 0.1 to 0.15 to emphasize smoother motion\n",
      "    angular_velocity_penalty = abs(angular_velocity) * 0.15\n",
      "\n",
      "    # Reward for reducing angle (new)\n",
      "    # Small incentive for moving towards vertical position\n",
      "    angle_improvement = max(0, abs(angle) - abs(next_angle)) * 0.2\n",
      "\n",
      "    # Reward for reducing angular velocity (new)\n",
      "    # Small incentive for reducing rotation speed\n",
      "    velocity_improvement = max(0, abs(angular_velocity) - abs(next_angular_velocity)) * 0.1\n",
      "\n",
      "    # Calculate total reward\n",
      "    total_reward = (\n",
      "        base_reward\n",
      "        - angle_penalty\n",
      "        - angular_velocity_penalty\n",
      "        + angle_improvement\n",
      "        + velocity_improvement\n",
      "    )\n",
      "\n",
      "    # Clip reward to ensure it stays within a reasonable range\n",
      "    return max(-1.0, min(total_reward, 1.0))\n",
      "```\n",
      "\n",
      "Explanation of changes:\n",
      "\n",
      "1. Kept the successful elements: The base reward and angle penalty remain unchanged, as they seem to work well in the current implementation.\n",
      "\n",
      "2. Slightly increased angular velocity penalty: The weight for the angular velocity penalty is increased from 0.1 to 0.15. This change aims to emphasize smoother motion and discourage rapid rotations more strongly.\n",
      "\n",
      "3. Added angle improvement reward: A small reward is given for reducing the absolute angle between the current and next state. This encourages the agent to move towards a more vertical position.\n",
      "\n",
      "4. Added velocity improvement reward: A small reward is given for reducing the absolute angular velocity between the current and next state. This encourages the agent to stabilize its rotation.\n",
      "\n",
      "5. Implemented reward clipping: The final reward is clipped between -1.0 and 1.0 to ensure it stays within a reasonable range and prevent extreme values from destabilizing the learning process.\n",
      "\n",
      "These changes are conservative and maintain the core reward structure while providing additional incentives for stability. The new components (angle and velocity improvement rewards) have small weights to avoid dramatically altering the existing behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sd37/.conda/envs/thesis/lib/python3.10/site-packages/numpy/_core/fromnumeric.py:3904: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/sd37/.conda/envs/thesis/lib/python3.10/site-packages/numpy/_core/_methods.py:147: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ LLM update for component 1 at episode 1505\n",
      "\n",
      "Episode 1600/10000\n",
      "Average Reward: 42.10\n",
      "Average Steps: 87.55\n",
      "\n",
      "Episode 1700/10000\n",
      "Average Reward: 4.12\n",
      "Average Steps: 10.81\n",
      "\n",
      "Episode 1800/10000\n",
      "Average Reward: 39.02\n",
      "Average Steps: 97.77\n",
      "\n",
      "Episode 1900/10000\n",
      "Average Reward: 65.32\n",
      "Average Steps: 163.52\n",
      "\n",
      "Metrics Debug at Episode 2000:\n",
      "Recent Average Reward: 35.11\n",
      "Average Balance Time: 88.00\n",
      "Balance Time Variance: 119261.88\n",
      "Component Weights - Stability: 0.60, Efficiency: 0.40\n",
      "\n",
      "Episode 2000 - Time Since Last Update: 495\n",
      "\n",
      "Episode 2000/10000\n",
      "Average Reward: 35.11\n",
      "Average Steps: 88.00\n",
      "\n",
      "Episode 2100/10000\n",
      "Average Reward: 24.41\n",
      "Average Steps: 61.17\n",
      "\n",
      "Episode 2200/10000\n",
      "Average Reward: 25.90\n",
      "Average Steps: 64.90\n",
      "\n",
      "Episode 2300/10000\n",
      "Average Reward: 11.27\n",
      "Average Steps: 28.32\n",
      "\n",
      "Episode 2400/10000\n",
      "Average Reward: 13.85\n",
      "Average Steps: 34.91\n",
      "\n",
      "Episode 2500/10000\n",
      "Average Reward: 12.47\n",
      "Average Steps: 31.49\n",
      "\n",
      "Episode 2600/10000\n",
      "Average Reward: 10.82\n",
      "Average Steps: 27.36\n",
      "\n",
      "Episode 2700/10000\n",
      "Average Reward: 11.55\n",
      "Average Steps: 29.21\n",
      "\n",
      "Episode 2800/10000\n",
      "Average Reward: 12.16\n",
      "Average Steps: 30.75\n",
      "\n",
      "Episode 2900/10000\n",
      "Average Reward: 13.76\n",
      "Average Steps: 34.76\n",
      "\n",
      "Metrics Debug at Episode 3000:\n",
      "Recent Average Reward: 78.72\n",
      "Average Balance Time: 197.01\n",
      "Balance Time Variance: 149465.37\n",
      "Component Weights - Stability: 0.60, Efficiency: 0.40\n",
      "\n",
      "Episode 3000 - Time Since Last Update: 1495\n",
      "\n",
      "Episode 3000/10000\n",
      "Average Reward: 78.72\n",
      "Average Steps: 197.01\n",
      "\n",
      "Performance Analysis for component_1:\n",
      "Current Performance: 42.82\n",
      "Historical Best: 112.68\n",
      "Relative Performance: 38.0%\n",
      "\n",
      "Generating new stability reward function...\n",
      "\n",
      "Proposed Function:\n",
      "Here's the modified function with targeted improvements and detailed inline comments:\n",
      "\n",
      "```python\n",
      "def reward_function(state, action, next_state):\n",
      "    # Extract relevant state variables\n",
      "    angle = state[2]\n",
      "    angular_velocity = state[3]\n",
      "    next_angle = next_state[2]\n",
      "    next_angular_velocity = next_state[3]\n",
      "\n",
      "    # Base reward for staying upright (unchanged)\n",
      "    base_reward = 1.0\n",
      "\n",
      "    # Angle penalty (slightly increased weight)\n",
      "    # Increased from 0.5 to 0.6 to emphasize staying more vertical\n",
      "    angle_penalty = abs(angle) * 0.6\n",
      "\n",
      "    # Angular velocity penalty (unchanged)\n",
      "    angular_velocity_penalty = abs(angular_velocity) * 0.15\n",
      "\n",
      "    # Reward for reducing angle (slightly increased weight)\n",
      "    # Increased from 0.2 to 0.25 to provide more incentive for vertical alignment\n",
      "    angle_improvement = max(0, abs(angle) - abs(next_angle)) * 0.25\n",
      "\n",
      "    # Reward for reducing angular velocity (unchanged)\n",
      "    velocity_improvement = max(0, abs(angular_velocity) - abs(next_angular_velocity)) * 0.1\n",
      "\n",
      "    # New: Small penalty for rapid changes in angular velocity\n",
      "    # This encourages smoother transitions and discourages erratic movements\n",
      "    angular_acceleration_penalty = abs(next_angular_velocity - angular_velocity) * 0.05\n",
      "\n",
      "    # Calculate total reward\n",
      "    total_reward = (\n",
      "        base_reward\n",
      "        - angle_penalty\n",
      "        - angular_velocity_penalty\n",
      "        + angle_improvement\n",
      "        + velocity_improvement\n",
      "        - angular_acceleration_penalty  # New penalty added to total reward\n",
      "    )\n",
      "\n",
      "    # Clip reward to ensure it stays within a reasonable range (unchanged)\n",
      "    return max(-1.0, min(total_reward, 1.0))\n",
      "```\n",
      "\n",
      "The modifications made are:\n",
      "\n",
      "1. Slightly increased the angle penalty weight from 0.5 to 0.6 to put more emphasis on maintaining a vertical position.\n",
      "2. Increased the angle improvement reward weight from 0.2 to 0.25 to provide a stronger incentive for moving towards and maintaining a vertical alignment.\n",
      "3. Added a new small penalty for rapid changes in angular velocity (angular acceleration). This discourages erratic movements and encourages smoother transitions, which should contribute to overall stability.\n",
      "\n",
      "These changes are conservative and incremental, maintaining the core structure of the reward function while making targeted improvements to enhance stability and smooth motion.\n",
      "✓ LLM update for component 1 at episode 3025\n",
      "\n",
      "Episode 3100/10000\n",
      "Average Reward: 17.19\n",
      "Average Steps: 43.22\n",
      "\n",
      "Episode 3200/10000\n",
      "Average Reward: 13.11\n",
      "Average Steps: 33.04\n",
      "\n",
      "Episode 3300/10000\n",
      "Average Reward: 20.21\n",
      "Average Steps: 50.81\n",
      "\n",
      "Episode 3400/10000\n",
      "Average Reward: 20.35\n",
      "Average Steps: 51.17\n",
      "\n",
      "Episode 3500/10000\n",
      "Average Reward: 21.59\n",
      "Average Steps: 54.27\n",
      "\n",
      "Episode 3600/10000\n",
      "Average Reward: 46.61\n",
      "Average Steps: 116.80\n",
      "\n",
      "Episode 3700/10000\n",
      "Average Reward: 109.87\n",
      "Average Steps: 274.88\n",
      "\n",
      "Episode 3800/10000\n",
      "Average Reward: 13.34\n",
      "Average Steps: 33.64\n",
      "\n",
      "Episode 3900/10000\n",
      "Average Reward: 603.70\n",
      "Average Steps: 1509.27\n",
      "\n",
      "Metrics Debug at Episode 4000:\n",
      "Recent Average Reward: 85.98\n",
      "Average Balance Time: 215.15\n",
      "Balance Time Variance: 143672.71\n",
      "Component Weights - Stability: 0.60, Efficiency: 0.40\n",
      "\n",
      "Episode 4000 - Time Since Last Update: 975\n",
      "\n",
      "Episode 4000/10000\n",
      "Average Reward: 85.98\n",
      "Average Steps: 215.15\n",
      "\n",
      "Episode 4100/10000\n",
      "Average Reward: 11.37\n",
      "Average Steps: 28.69\n",
      "\n",
      "Episode 4200/10000\n",
      "Average Reward: 46.93\n",
      "Average Steps: 117.59\n",
      "\n",
      "Episode 4300/10000\n",
      "Average Reward: 16.35\n",
      "Average Steps: 41.12\n",
      "\n",
      "Episode 4400/10000\n",
      "Average Reward: 12.53\n",
      "Average Steps: 31.42\n",
      "\n",
      "Episode 4500/10000\n",
      "Average Reward: 32.35\n",
      "Average Steps: 81.19\n",
      "\n",
      "Episode 4600/10000\n",
      "Average Reward: 36.31\n",
      "Average Steps: 91.54\n",
      "\n",
      "Episode 4700/10000\n",
      "Average Reward: 39.72\n",
      "Average Steps: 99.55\n",
      "\n",
      "Performance Analysis for component_1:\n",
      "Current Performance: 25.74\n",
      "Historical Best: 52.73\n",
      "Relative Performance: 48.8%\n",
      "\n",
      "Generating new stability reward function...\n",
      "\n",
      "Proposed Function:\n",
      "Here's the modified function with small, targeted improvements and detailed inline comments:\n",
      "\n",
      "```python\n",
      "def reward_function(state, action, next_state):\n",
      "    # Extract relevant state variables\n",
      "    angle = state[2]\n",
      "    angular_velocity = state[3]\n",
      "    next_angle = next_state[2]\n",
      "    next_angular_velocity = next_state[3]\n",
      "\n",
      "    # Base reward for staying upright (unchanged)\n",
      "    base_reward = 1.0\n",
      "\n",
      "    # Angle penalty (slightly increased weight)\n",
      "    # Increased from 0.6 to 0.65 to further emphasize staying vertical\n",
      "    angle_penalty = abs(angle) * 0.65\n",
      "\n",
      "    # Angular velocity penalty (slightly increased)\n",
      "    # Increased from 0.15 to 0.18 to discourage excessive rotation\n",
      "    angular_velocity_penalty = abs(angular_velocity) * 0.18\n",
      "\n",
      "    # Reward for reducing angle (unchanged)\n",
      "    angle_improvement = max(0, abs(angle) - abs(next_angle)) * 0.25\n",
      "\n",
      "    # Reward for reducing angular velocity (slightly increased)\n",
      "    # Increased from 0.1 to 0.12 to encourage more stability\n",
      "    velocity_improvement = max(0, abs(angular_velocity) - abs(next_angular_velocity)) * 0.12\n",
      "\n",
      "    # Penalty for rapid changes in angular velocity (slightly adjusted)\n",
      "    # Decreased from 0.05 to 0.04 to fine-tune its impact\n",
      "    angular_acceleration_penalty = abs(next_angular_velocity - angular_velocity) * 0.04\n",
      "\n",
      "    # New: Small reward for maintaining near-zero angle\n",
      "    # Encourages the pole to stay very close to vertical\n",
      "    near_zero_angle_reward = 0.1 if abs(angle) < 0.05 else 0\n",
      "\n",
      "    # Calculate total reward\n",
      "    total_reward = (\n",
      "        base_reward\n",
      "        - angle_penalty\n",
      "        - angular_velocity_penalty\n",
      "        + angle_improvement\n",
      "        + velocity_improvement\n",
      "        - angular_acceleration_penalty\n",
      "        + near_zero_angle_reward  # New reward added to total\n",
      "    )\n",
      "\n",
      "    # Clip reward to ensure it stays within a reasonable range (unchanged)\n",
      "    return max(-1.0, min(total_reward, 1.0))\n",
      "```\n",
      "\n",
      "The changes made are conservative and incremental, focusing on fine-tuning the existing components and adding a small new element to encourage stability:\n",
      "\n",
      "1. Slightly increased the angle penalty weight from 0.6 to 0.65 to further emphasize the importance of staying vertical.\n",
      "2. Increased the angular velocity penalty from 0.15 to 0.18 to discourage excessive rotation more strongly.\n",
      "3. Slightly increased the velocity improvement reward from 0.1 to 0.12 to encourage more stability in angular velocity.\n",
      "4. Fine-tuned the angular acceleration penalty by decreasing it from 0.05 to 0.04, maintaining its purpose while slightly reducing its impact.\n",
      "5. Added a small new reward (near_zero_angle_reward) to encourage the pole to stay very close to vertical, providing an extra incentive for perfect balance.\n",
      "\n",
      "These changes aim to incrementally improve the stability aspects of the reward function while maintaining its core structure and successful elements.\n",
      "✓ LLM update for component 1 at episode 4702\n",
      "\n",
      "Episode 4800/10000\n",
      "Average Reward: 19.61\n",
      "Average Steps: 49.16\n",
      "\n",
      "Episode 4900/10000\n",
      "Average Reward: 69.98\n",
      "Average Steps: 175.17\n",
      "\n",
      "Metrics Debug at Episode 5000:\n",
      "Recent Average Reward: 131.83\n",
      "Average Balance Time: 329.78\n",
      "Balance Time Variance: 8080.23\n",
      "Component Weights - Stability: 0.60, Efficiency: 0.40\n",
      "\n",
      "Episode 5000 - Time Since Last Update: 298\n",
      "Environment parameters updated: masscart=1.0, length=0.9, gravity=9.8\n",
      "\n",
      "Changed pole length to: 0.9m at episode 5000\n",
      "\n",
      "Episode 5000/10000\n",
      "Average Reward: 131.83\n",
      "Average Steps: 329.78\n",
      "\n",
      "Episode 5100/10000\n",
      "Average Reward: 99.64\n",
      "Average Steps: 249.32\n",
      "\n",
      "Episode 5200/10000\n",
      "Average Reward: 68.44\n",
      "Average Steps: 171.32\n",
      "\n",
      "Episode 5300/10000\n",
      "Average Reward: 66.43\n",
      "Average Steps: 166.32\n",
      "\n",
      "Episode 5400/10000\n",
      "Average Reward: 60.79\n",
      "Average Steps: 152.23\n",
      "\n",
      "Episode 5500/10000\n",
      "Average Reward: 57.08\n",
      "Average Steps: 142.94\n",
      "\n",
      "Episode 5600/10000\n",
      "Average Reward: 48.03\n",
      "Average Steps: 120.27\n",
      "\n",
      "Episode 5700/10000\n",
      "Average Reward: 38.74\n",
      "Average Steps: 97.04\n",
      "\n",
      "Episode 5800/10000\n",
      "Average Reward: 33.96\n",
      "Average Steps: 85.07\n",
      "\n",
      "Episode 5900/10000\n",
      "Average Reward: 30.52\n",
      "Average Steps: 76.47\n",
      "\n",
      "Metrics Debug at Episode 6000:\n",
      "Recent Average Reward: 28.80\n",
      "Average Balance Time: 72.18\n",
      "Balance Time Variance: 25.33\n",
      "Component Weights - Stability: 0.60, Efficiency: 0.40\n",
      "\n",
      "Episode 6000 - Time Since Last Update: 1298\n",
      "\n",
      "Episode 6000/10000\n",
      "Average Reward: 28.80\n",
      "Average Steps: 72.18\n",
      "\n",
      "Episode 6100/10000\n",
      "Average Reward: 31.48\n",
      "Average Steps: 78.90\n",
      "\n",
      "Episode 6200/10000\n",
      "Average Reward: 17.85\n",
      "Average Steps: 45.07\n",
      "\n",
      "Performance Analysis for component_1:\n",
      "Current Performance: 3.54\n",
      "Historical Best: 30.66\n",
      "Relative Performance: 11.6%\n",
      "\n",
      "Generating new stability reward function...\n",
      "\n",
      "Proposed Function:\n",
      "Here's the modified function with small, targeted improvements and detailed inline comments:\n",
      "\n",
      "```python\n",
      "def reward_function(state, action, next_state):\n",
      "    # Extract relevant state variables\n",
      "    angle = state[2]\n",
      "    angular_velocity = state[3]\n",
      "    next_angle = next_state[2]\n",
      "    next_angular_velocity = next_state[3]\n",
      "\n",
      "    # Base reward for staying upright (unchanged)\n",
      "    base_reward = 1.0\n",
      "\n",
      "    # Angle penalty (slightly adjusted)\n",
      "    # Decreased from 0.65 to 0.62 to fine-tune balance between stability and flexibility\n",
      "    angle_penalty = abs(angle) * 0.62\n",
      "\n",
      "    # Angular velocity penalty (unchanged)\n",
      "    angular_velocity_penalty = abs(angular_velocity) * 0.18\n",
      "\n",
      "    # Reward for reducing angle (slightly increased)\n",
      "    # Increased from 0.25 to 0.27 to encourage more emphasis on angle correction\n",
      "    angle_improvement = max(0, abs(angle) - abs(next_angle)) * 0.27\n",
      "\n",
      "    # Reward for reducing angular velocity (unchanged)\n",
      "    velocity_improvement = max(0, abs(angular_velocity) - abs(next_angular_velocity)) * 0.12\n",
      "\n",
      "    # Penalty for rapid changes in angular velocity (unchanged)\n",
      "    angular_acceleration_penalty = abs(next_angular_velocity - angular_velocity) * 0.04\n",
      "\n",
      "    # Small reward for maintaining near-zero angle (slightly adjusted)\n",
      "    # Increased range from 0.05 to 0.08 to allow for a bit more flexibility\n",
      "    near_zero_angle_reward = 0.12 if abs(angle) < 0.08 else 0\n",
      "\n",
      "    # New: Small penalty for extreme angles to prevent tipping over\n",
      "    # Adds an additional safeguard against extreme angles\n",
      "    extreme_angle_penalty = 0.2 if abs(angle) > 0.3 else 0\n",
      "\n",
      "    # Calculate total reward\n",
      "    total_reward = (\n",
      "        base_reward\n",
      "        - angle_penalty\n",
      "        - angular_velocity_penalty\n",
      "        + angle_improvement\n",
      "        + velocity_improvement\n",
      "        - angular_acceleration_penalty\n",
      "        + near_zero_angle_reward\n",
      "        - extreme_angle_penalty  # New penalty added to total\n",
      "    )\n",
      "\n",
      "    # Clip reward to ensure it stays within a reasonable range (unchanged)\n",
      "    return max(-1.0, min(total_reward, 1.0))\n",
      "```\n",
      "\n",
      "These changes are conservative and incremental, focusing on fine-tuning the existing successful elements while adding a small new component to further emphasize stability:\n",
      "\n",
      "1. Slightly reduced the angle penalty weight from 0.65 to 0.62 to allow for a bit more flexibility while still maintaining a strong emphasis on staying vertical.\n",
      "2. Increased the angle improvement reward from 0.25 to 0.27 to encourage more emphasis on correcting the angle towards vertical.\n",
      "3. Adjusted the near-zero angle reward by increasing the angle range from 0.05 to 0.08 and the reward from 0.1 to 0.12. This allows for a slightly wider \"sweet spot\" near vertical while still encouraging precise balance.\n",
      "4. Added a new small penalty for extreme angles (> 0.3 radians) to provide an additional safeguard against tipping over, without significantly altering the overall reward structure.\n",
      "\n",
      "These modifications aim to maintain the core reward structure while making targeted improvements to enhance stability and performance.\n",
      "\n",
      "Error during update: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}\n",
      "\n",
      "Performance Analysis for component_2:\n",
      "Current Performance: 3.54\n",
      "Historical Best: 30.66\n",
      "Relative Performance: 11.6%\n",
      "\n",
      "Generating new efficiency reward function...\n",
      "\n",
      "Error during update: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}\n",
      "\n",
      "Performance Analysis for component_1:\n",
      "Current Performance: 3.53\n",
      "Historical Best: 29.97\n",
      "Relative Performance: 11.8%\n",
      "\n",
      "Generating new stability reward function...\n",
      "\n",
      "Error during update: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}\n",
      "\n",
      "Performance Analysis for component_2:\n",
      "Current Performance: 3.53\n",
      "Historical Best: 29.97\n",
      "Relative Performance: 11.8%\n",
      "\n",
      "Generating new efficiency reward function...\n",
      "\n",
      "Proposed Function:\n",
      "Here's a conservatively modified version of the energy efficiency reward function with detailed inline comments explaining the changes:\n",
      "\n",
      "```python\n",
      "def energyEfficiencyReward(state, action, next_state):\n",
      "    # Retrieve relevant state variables\n",
      "    current_temp = state[0]\n",
      "    target_temp = state[1]\n",
      "    outside_temp = state[2]\n",
      "    current_energy = state[3]\n",
      "\n",
      "    # Calculate temperature difference\n",
      "    temp_diff = abs(current_temp - target_temp)\n",
      "    \n",
      "    # Base reward for being close to target temperature\n",
      "    base_reward = 10 - temp_diff\n",
      "    \n",
      "    # Small improvement: Slightly increase reward for being very close to target\n",
      "    if temp_diff < 0.5:\n",
      "        base_reward += 1  # Additional bonus for precision\n",
      "\n",
      "    # Penalize energy usage\n",
      "    energy_penalty = -0.1 * current_energy\n",
      "    \n",
      "    # Small improvement: Graduated energy penalty\n",
      "    if current_energy > 50:\n",
      "        energy_penalty -= 0.05 * (current_energy - 50)  # Additional penalty for high energy use\n",
      "\n",
      "    # Reward for moving towards target temperature\n",
      "    temp_progress = abs(state[0] - target_temp) - abs(next_state[0] - target_temp)\n",
      "    progress_reward = 5 * temp_progress\n",
      "    \n",
      "    # Small improvement: Bonus for quick adjustments\n",
      "    if temp_progress > 1:\n",
      "        progress_reward += 1  # Bonus for significant temperature changes\n",
      "\n",
      "    # Efficiency bonus for maintaining temperature with less energy\n",
      "    efficiency_bonus = 0\n",
      "    if temp_diff < 1 and current_energy < 30:\n",
      "        efficiency_bonus = 5\n",
      "    \n",
      "    # Small improvement: Graduated efficiency bonus\n",
      "    elif temp_diff < 2 and current_energy < 40:\n",
      "        efficiency_bonus = 3  # Smaller bonus for good but not optimal efficiency\n",
      "\n",
      "    # Combine all components\n",
      "    total_reward = base_reward + energy_penalty + progress_reward + efficiency_bonus\n",
      "\n",
      "    # Small improvement: Clip reward to prevent extreme values\n",
      "    return max(min(total_reward, 50), -50)  # Limit reward between -50 and 50\n",
      "```\n",
      "\n",
      "This modified function maintains the core structure and successful elements of the original while making small, targeted improvements:\n",
      "\n",
      "1. Added a small bonus for very precise temperature control.\n",
      "2. Introduced a graduated energy penalty for high energy usage.\n",
      "3. Included a bonus for quick temperature adjustments.\n",
      "4. Implemented a graduated efficiency bonus.\n",
      "5. Added reward clipping to prevent extreme values.\n",
      "\n",
      "These changes aim to slightly refine the reward structure, encouraging more efficient behavior without dramatically altering the function's core logic.\n",
      "✓ LLM update for component 2 at episode 6203\n",
      "\n",
      "Episode 6300/10000\n",
      "Average Reward: 34.90\n",
      "Average Steps: 35.30\n",
      "\n",
      "Episode 6400/10000\n",
      "Average Reward: 33.77\n",
      "Average Steps: 34.00\n",
      "\n",
      "Episode 6500/10000\n",
      "Average Reward: 116.66\n",
      "Average Steps: 117.06\n",
      "\n",
      "Episode 6600/10000\n",
      "Average Reward: 60.74\n",
      "Average Steps: 61.07\n",
      "\n",
      "Episode 6700/10000\n",
      "Average Reward: 56.83\n",
      "Average Steps: 57.12\n",
      "\n",
      "Episode 6800/10000\n",
      "Average Reward: 55.53\n",
      "Average Steps: 55.76\n",
      "\n",
      "Episode 6900/10000\n",
      "Average Reward: 63.05\n",
      "Average Steps: 63.31\n",
      "\n",
      "Metrics Debug at Episode 7000:\n",
      "Recent Average Reward: 69.17\n",
      "Average Balance Time: 69.46\n",
      "Balance Time Variance: 134.49\n",
      "Component Weights - Stability: 0.60, Efficiency: 0.40\n",
      "\n",
      "Episode 7000 - Time Since Last Update: 797\n",
      "\n",
      "Episode 7000/10000\n",
      "Average Reward: 69.17\n",
      "Average Steps: 69.46\n",
      "\n",
      "Episode 7100/10000\n",
      "Average Reward: 78.08\n",
      "Average Steps: 78.33\n",
      "\n",
      "Episode 7200/10000\n",
      "Average Reward: 88.39\n",
      "Average Steps: 88.62\n",
      "\n",
      "Episode 7300/10000\n",
      "Average Reward: 117.30\n",
      "Average Steps: 117.55\n",
      "\n",
      "Episode 7400/10000\n",
      "Average Reward: 181.46\n",
      "Average Steps: 181.81\n",
      "\n",
      "Episode 7500/10000\n",
      "Average Reward: 199.74\n",
      "Average Steps: 200.09\n",
      "\n",
      "Episode 7600/10000\n",
      "Average Reward: 215.42\n",
      "Average Steps: 215.72\n",
      "\n",
      "Episode 7700/10000\n",
      "Average Reward: 217.17\n",
      "Average Steps: 217.40\n",
      "\n",
      "Episode 7800/10000\n",
      "Average Reward: 235.51\n",
      "Average Steps: 235.74\n",
      "\n",
      "Episode 7900/10000\n",
      "Average Reward: 304.95\n",
      "Average Steps: 305.20\n",
      "\n",
      "Metrics Debug at Episode 8000:\n",
      "Recent Average Reward: 289.55\n",
      "Average Balance Time: 289.76\n",
      "Balance Time Variance: 21473.04\n",
      "Component Weights - Stability: 0.60, Efficiency: 0.40\n",
      "\n",
      "Episode 8000 - Time Since Last Update: 1797\n",
      "\n",
      "Episode 8000/10000\n",
      "Average Reward: 289.55\n",
      "Average Steps: 289.76\n",
      "\n",
      "Episode 8100/10000\n",
      "Average Reward: 265.05\n",
      "Average Steps: 265.25\n",
      "\n",
      "Episode 8200/10000\n",
      "Average Reward: 284.92\n",
      "Average Steps: 285.10\n",
      "\n",
      "Episode 8300/10000\n",
      "Average Reward: 291.29\n",
      "Average Steps: 291.48\n",
      "\n",
      "Episode 8400/10000\n",
      "Average Reward: 310.30\n",
      "Average Steps: 310.46\n",
      "\n",
      "Episode 8500/10000\n",
      "Average Reward: 306.51\n",
      "Average Steps: 306.66\n",
      "\n",
      "Episode 8600/10000\n",
      "Average Reward: 357.68\n",
      "Average Steps: 357.83\n",
      "\n",
      "Episode 8700/10000\n",
      "Average Reward: 385.26\n",
      "Average Steps: 385.42\n",
      "\n",
      "Episode 8800/10000\n",
      "Average Reward: 395.37\n",
      "Average Steps: 395.51\n",
      "\n",
      "Episode 8900/10000\n",
      "Average Reward: 464.99\n",
      "Average Steps: 465.12\n",
      "\n",
      "Metrics Debug at Episode 9000:\n",
      "Recent Average Reward: 459.23\n",
      "Average Balance Time: 459.38\n",
      "Balance Time Variance: 52254.10\n",
      "Component Weights - Stability: 0.60, Efficiency: 0.40\n",
      "\n",
      "Episode 9000 - Time Since Last Update: 2797\n",
      "\n",
      "Episode 9000/10000\n",
      "Average Reward: 459.23\n",
      "Average Steps: 459.38\n",
      "\n",
      "Episode 9100/10000\n",
      "Average Reward: 545.43\n",
      "Average Steps: 545.56\n",
      "\n",
      "Episode 9200/10000\n",
      "Average Reward: 602.46\n",
      "Average Steps: 602.60\n",
      "\n",
      "Episode 9300/10000\n",
      "Average Reward: 683.77\n",
      "Average Steps: 683.89\n",
      "\n",
      "Episode 9400/10000\n",
      "Average Reward: 856.70\n",
      "Average Steps: 856.85\n",
      "\n",
      "Episode 9500/10000\n",
      "Average Reward: 1145.61\n",
      "Average Steps: 1145.78\n",
      "\n",
      "Episode 9600/10000\n",
      "Average Reward: 1165.32\n",
      "Average Steps: 1165.49\n",
      "\n",
      "Performance Analysis for component_1:\n",
      "Current Performance: 586.91\n",
      "Historical Best: 1203.88\n",
      "Relative Performance: 48.8%\n",
      "\n",
      "Generating new stability reward function...\n",
      "\n",
      "Proposed Function:\n",
      "Here's the modified function with small, targeted improvements and detailed inline comments:\n",
      "\n",
      "```python\n",
      "def reward_function(state, action, next_state):\n",
      "    # Extract relevant state variables\n",
      "    angle = state[2]\n",
      "    angular_velocity = state[3]\n",
      "    next_angle = next_state[2]\n",
      "    next_angular_velocity = next_state[3]\n",
      "\n",
      "    # Base reward for staying upright (unchanged)\n",
      "    base_reward = 1.0\n",
      "\n",
      "    # Angle penalty (slightly increased weight)\n",
      "    # Further increased from 0.65 to 0.68 to emphasize staying vertical even more\n",
      "    angle_penalty = abs(angle) * 0.68\n",
      "\n",
      "    # Angular velocity penalty (unchanged)\n",
      "    angular_velocity_penalty = abs(angular_velocity) * 0.18\n",
      "\n",
      "    # Reward for reducing angle (slightly increased)\n",
      "    # Increased from 0.25 to 0.28 to encourage more active angle correction\n",
      "    angle_improvement = max(0, abs(angle) - abs(next_angle)) * 0.28\n",
      "\n",
      "    # Reward for reducing angular velocity (unchanged)\n",
      "    velocity_improvement = max(0, abs(angular_velocity) - abs(next_angular_velocity)) * 0.12\n",
      "\n",
      "    # Penalty for rapid changes in angular velocity (unchanged)\n",
      "    angular_acceleration_penalty = abs(next_angular_velocity - angular_velocity) * 0.04\n",
      "\n",
      "    # Reward for maintaining near-zero angle (slightly adjusted)\n",
      "    # Increased reward from 0.1 to 0.12 and tightened threshold from 0.05 to 0.03\n",
      "    near_zero_angle_reward = 0.12 if abs(angle) < 0.03 else 0\n",
      "\n",
      "    # New: Small penalty for extreme angles to prevent tipping over\n",
      "    # Adds an additional disincentive for allowing the pole to lean too far\n",
      "    extreme_angle_penalty = 0.1 if abs(angle) > 0.3 else 0\n",
      "\n",
      "    # Calculate total reward\n",
      "    total_reward = (\n",
      "        base_reward\n",
      "        - angle_penalty\n",
      "        - angular_velocity_penalty\n",
      "        + angle_improvement\n",
      "        + velocity_improvement\n",
      "        - angular_acceleration_penalty\n",
      "        + near_zero_angle_reward\n",
      "        - extreme_angle_penalty  # New penalty added to total\n",
      "    )\n",
      "\n",
      "    # Clip reward to ensure it stays within a reasonable range (unchanged)\n",
      "    return max(-1.0, min(total_reward, 1.0))\n",
      "```\n",
      "\n",
      "These modifications make conservative, incremental improvements while maintaining the core reward structure and focusing on stability aspects:\n",
      "\n",
      "1. Slightly increased the angle penalty to further emphasize staying vertical.\n",
      "2. Increased the reward for reducing angle to encourage more active angle correction.\n",
      "3. Adjusted the near-zero angle reward by increasing the reward value and tightening the threshold, promoting more precise vertical positioning.\n",
      "4. Added a small penalty for extreme angles to help prevent tipping over.\n",
      "\n",
      "These changes aim to fine-tune the agent's behavior towards maintaining a more stable and vertical position without dramatically altering the existing reward structure.\n",
      "✓ LLM update for component 1 at episode 9679\n",
      "\n",
      "Episode 9700/10000\n",
      "Average Reward: 789.34\n",
      "Average Steps: 789.56\n",
      "\n",
      "Episode 9800/10000\n",
      "Average Reward: 478.62\n",
      "Average Steps: 478.91\n",
      "\n",
      "Episode 9900/10000\n",
      "Average Reward: 353.70\n",
      "Average Steps: 354.01\n",
      "\n",
      "Completed testing adaptivereward\n",
      "Final average reward: 424.72\n",
      "Final average balance time: 425.05\n",
      "\n",
      "Testing reward function: energy_based\n",
      "\n",
      "Metrics Debug at Episode 0:\n",
      "Recent Average Reward: 12.89\n",
      "Average Balance Time: 13.00\n",
      "Balance Time Variance: 0.00\n",
      "\n",
      "Episode 0/10000\n",
      "Average Reward: 12.89\n",
      "Average Steps: 13.00\n",
      "\n",
      "Episode 100/10000\n",
      "Average Reward: 20.76\n",
      "Average Steps: 20.86\n",
      "\n",
      "Episode 200/10000\n",
      "Average Reward: 26.80\n",
      "Average Steps: 26.96\n",
      "\n",
      "Episode 300/10000\n",
      "Average Reward: 44.64\n",
      "Average Steps: 44.86\n",
      "\n",
      "Episode 400/10000\n",
      "Average Reward: 64.72\n",
      "Average Steps: 65.00\n",
      "\n",
      "Episode 500/10000\n",
      "Average Reward: 105.71\n",
      "Average Steps: 106.06\n",
      "\n",
      "Episode 600/10000\n",
      "Average Reward: 85.66\n",
      "Average Steps: 86.16\n",
      "\n",
      "Episode 700/10000\n",
      "Average Reward: 92.46\n",
      "Average Steps: 92.81\n",
      "\n",
      "Episode 800/10000\n",
      "Average Reward: 107.89\n",
      "Average Steps: 108.29\n",
      "\n",
      "Episode 900/10000\n",
      "Average Reward: 145.31\n",
      "Average Steps: 145.82\n",
      "\n",
      "Metrics Debug at Episode 1000:\n",
      "Recent Average Reward: 358.67\n",
      "Average Balance Time: 358.94\n",
      "Balance Time Variance: 141556.68\n",
      "\n",
      "Episode 1000/10000\n",
      "Average Reward: 358.67\n",
      "Average Steps: 358.94\n",
      "\n",
      "Episode 1100/10000\n",
      "Average Reward: 443.10\n",
      "Average Steps: 443.25\n",
      "\n",
      "Episode 1200/10000\n",
      "Average Reward: 372.99\n",
      "Average Steps: 373.15\n",
      "\n",
      "Episode 1300/10000\n",
      "Average Reward: 562.06\n",
      "Average Steps: 562.17\n",
      "\n",
      "Episode 1400/10000\n",
      "Average Reward: 386.39\n",
      "Average Steps: 386.48\n",
      "\n",
      "Episode 1500/10000\n",
      "Average Reward: 208.72\n",
      "Average Steps: 209.00\n",
      "\n",
      "Episode 1600/10000\n",
      "Average Reward: 226.48\n",
      "Average Steps: 226.67\n",
      "\n",
      "Episode 1700/10000\n",
      "Average Reward: 42.03\n",
      "Average Steps: 42.15\n",
      "\n",
      "Episode 1800/10000\n",
      "Average Reward: 240.80\n",
      "Average Steps: 241.07\n",
      "\n",
      "Episode 1900/10000\n",
      "Average Reward: 384.33\n",
      "Average Steps: 384.68\n",
      "\n",
      "Metrics Debug at Episode 2000:\n",
      "Recent Average Reward: 444.51\n",
      "Average Balance Time: 444.59\n",
      "Balance Time Variance: 77239.20\n",
      "\n",
      "Episode 2000/10000\n",
      "Average Reward: 444.51\n",
      "Average Steps: 444.59\n",
      "\n",
      "Episode 2100/10000\n",
      "Average Reward: 439.95\n",
      "Average Steps: 440.03\n",
      "\n",
      "Episode 2200/10000\n",
      "Average Reward: 428.24\n",
      "Average Steps: 428.33\n",
      "\n",
      "Episode 2300/10000\n",
      "Average Reward: 423.28\n",
      "Average Steps: 423.35\n",
      "\n",
      "Episode 2400/10000\n",
      "Average Reward: 420.73\n",
      "Average Steps: 420.80\n",
      "\n",
      "Episode 2500/10000\n",
      "Average Reward: 485.95\n",
      "Average Steps: 485.99\n",
      "\n",
      "Episode 2600/10000\n",
      "Average Reward: 496.93\n",
      "Average Steps: 496.98\n",
      "\n",
      "Episode 2700/10000\n",
      "Average Reward: 613.00\n",
      "Average Steps: 613.03\n",
      "\n",
      "Episode 2800/10000\n",
      "Average Reward: 49.55\n",
      "Average Steps: 49.64\n",
      "\n",
      "Episode 2900/10000\n",
      "Average Reward: 220.05\n",
      "Average Steps: 220.23\n",
      "\n",
      "Metrics Debug at Episode 3000:\n",
      "Recent Average Reward: 468.73\n",
      "Average Balance Time: 468.83\n",
      "Balance Time Variance: 80635.28\n",
      "\n",
      "Episode 3000/10000\n",
      "Average Reward: 468.73\n",
      "Average Steps: 468.83\n",
      "\n",
      "Episode 3100/10000\n",
      "Average Reward: 412.40\n",
      "Average Steps: 412.49\n",
      "\n",
      "Episode 3200/10000\n",
      "Average Reward: 452.75\n",
      "Average Steps: 452.83\n",
      "\n",
      "Episode 3300/10000\n",
      "Average Reward: 937.07\n",
      "Average Steps: 937.12\n",
      "\n",
      "Episode 3400/10000\n",
      "Average Reward: 636.00\n",
      "Average Steps: 636.05\n",
      "\n",
      "Episode 3500/10000\n",
      "Average Reward: 580.59\n",
      "Average Steps: 580.69\n",
      "\n",
      "Episode 3600/10000\n",
      "Average Reward: 534.24\n",
      "Average Steps: 534.35\n",
      "\n",
      "Episode 3700/10000\n",
      "Average Reward: 441.40\n",
      "Average Steps: 441.46\n",
      "\n",
      "Episode 3800/10000\n",
      "Average Reward: 442.61\n",
      "Average Steps: 442.68\n",
      "\n",
      "Episode 3900/10000\n",
      "Average Reward: 423.77\n",
      "Average Steps: 423.82\n",
      "\n",
      "Metrics Debug at Episode 4000:\n",
      "Recent Average Reward: 470.47\n",
      "Average Balance Time: 470.51\n",
      "Balance Time Variance: 59791.27\n",
      "\n",
      "Episode 4000/10000\n",
      "Average Reward: 470.47\n",
      "Average Steps: 470.51\n",
      "\n",
      "Episode 4100/10000\n",
      "Average Reward: 399.25\n",
      "Average Steps: 399.33\n",
      "\n",
      "Episode 4200/10000\n",
      "Average Reward: 49.00\n",
      "Average Steps: 49.07\n",
      "\n",
      "Episode 4300/10000\n",
      "Average Reward: 9.24\n",
      "Average Steps: 9.30\n",
      "\n",
      "Episode 4400/10000\n",
      "Average Reward: 93.79\n",
      "Average Steps: 94.15\n",
      "\n",
      "Episode 4500/10000\n",
      "Average Reward: 87.61\n",
      "Average Steps: 88.36\n",
      "\n",
      "Episode 4600/10000\n",
      "Average Reward: 165.05\n",
      "Average Steps: 165.58\n",
      "\n",
      "Episode 4700/10000\n",
      "Average Reward: 181.51\n",
      "Average Steps: 181.67\n",
      "\n",
      "Episode 4800/10000\n",
      "Average Reward: 355.06\n",
      "Average Steps: 355.56\n",
      "\n",
      "Episode 4900/10000\n",
      "Average Reward: 272.61\n",
      "Average Steps: 273.04\n",
      "\n",
      "Metrics Debug at Episode 5000:\n",
      "Recent Average Reward: 165.52\n",
      "Average Balance Time: 165.88\n",
      "Balance Time Variance: 8275.87\n",
      "\n",
      "Updating physics-based reward at episode 5000\n",
      "✓ Physics-based update completed\n",
      "Environment parameters updated: masscart=1.0, length=0.3, gravity=9.8\n",
      "\n",
      "Changed pole length to: 0.3m at episode 5000\n",
      "\n",
      "Episode 5000/10000\n",
      "Average Reward: 165.52\n",
      "Average Steps: 165.88\n",
      "\n",
      "Episode 5100/10000\n",
      "Average Reward: 128.27\n",
      "Average Steps: 128.55\n",
      "\n",
      "Episode 5200/10000\n",
      "Average Reward: 175.18\n",
      "Average Steps: 175.47\n",
      "\n",
      "Episode 5300/10000\n",
      "Average Reward: 274.54\n",
      "Average Steps: 274.78\n",
      "\n",
      "Episode 5400/10000\n",
      "Average Reward: 106.74\n",
      "Average Steps: 107.20\n",
      "\n",
      "Episode 5500/10000\n",
      "Average Reward: 433.86\n",
      "Average Steps: 434.30\n",
      "\n",
      "Episode 5600/10000\n",
      "Average Reward: 429.77\n",
      "Average Steps: 430.03\n",
      "\n",
      "Episode 5700/10000\n",
      "Average Reward: 343.00\n",
      "Average Steps: 343.23\n",
      "\n",
      "Episode 5800/10000\n",
      "Average Reward: 376.95\n",
      "Average Steps: 377.39\n",
      "\n",
      "Episode 5900/10000\n",
      "Average Reward: 354.70\n",
      "Average Steps: 355.02\n",
      "\n",
      "Metrics Debug at Episode 6000:\n",
      "Recent Average Reward: 248.89\n",
      "Average Balance Time: 249.16\n",
      "Balance Time Variance: 8196.67\n",
      "\n",
      "Episode 6000/10000\n",
      "Average Reward: 248.89\n",
      "Average Steps: 249.16\n",
      "\n",
      "Episode 6100/10000\n",
      "Average Reward: 200.54\n",
      "Average Steps: 200.94\n",
      "\n",
      "Episode 6200/10000\n",
      "Average Reward: 246.36\n",
      "Average Steps: 246.87\n",
      "\n",
      "Episode 6300/10000\n",
      "Average Reward: 268.19\n",
      "Average Steps: 268.62\n",
      "\n",
      "Episode 6400/10000\n",
      "Average Reward: 77.60\n",
      "Average Steps: 77.78\n",
      "\n",
      "Episode 6500/10000\n",
      "Average Reward: 10.26\n",
      "Average Steps: 10.33\n",
      "\n",
      "Episode 6600/10000\n",
      "Average Reward: 9.26\n",
      "Average Steps: 9.32\n",
      "\n",
      "Episode 6700/10000\n",
      "Average Reward: 56.62\n",
      "Average Steps: 56.85\n",
      "\n",
      "Episode 6800/10000\n",
      "Average Reward: 155.82\n",
      "Average Steps: 156.08\n",
      "\n",
      "Episode 6900/10000\n",
      "Average Reward: 42.60\n",
      "Average Steps: 42.78\n",
      "\n",
      "Metrics Debug at Episode 7000:\n",
      "Recent Average Reward: 21.44\n",
      "Average Balance Time: 21.55\n",
      "Balance Time Variance: 58.41\n",
      "\n",
      "Episode 7000/10000\n",
      "Average Reward: 21.44\n",
      "Average Steps: 21.55\n",
      "\n",
      "Episode 7100/10000\n",
      "Average Reward: 22.58\n",
      "Average Steps: 22.68\n",
      "\n",
      "Episode 7200/10000\n",
      "Average Reward: 16.34\n",
      "Average Steps: 16.42\n",
      "\n",
      "Episode 7300/10000\n",
      "Average Reward: 24.01\n",
      "Average Steps: 24.12\n",
      "\n",
      "Episode 7400/10000\n",
      "Average Reward: 23.91\n",
      "Average Steps: 24.03\n",
      "\n",
      "Episode 7500/10000\n",
      "Average Reward: 16.55\n",
      "Average Steps: 16.65\n",
      "\n",
      "Episode 7600/10000\n",
      "Average Reward: 55.41\n",
      "Average Steps: 55.77\n",
      "\n",
      "Episode 7700/10000\n",
      "Average Reward: 24.53\n",
      "Average Steps: 24.62\n",
      "\n",
      "Episode 7800/10000\n",
      "Average Reward: 18.57\n",
      "Average Steps: 18.65\n",
      "\n",
      "Episode 7900/10000\n",
      "Average Reward: 171.61\n",
      "Average Steps: 171.74\n",
      "\n",
      "Metrics Debug at Episode 8000:\n",
      "Recent Average Reward: 1133.18\n",
      "Average Balance Time: 1133.24\n",
      "Balance Time Variance: 550434.86\n",
      "\n",
      "Episode 8000/10000\n",
      "Average Reward: 1133.18\n",
      "Average Steps: 1133.24\n",
      "\n",
      "Episode 8100/10000\n",
      "Average Reward: 1777.81\n",
      "Average Steps: 1777.84\n",
      "\n",
      "Episode 8200/10000\n",
      "Average Reward: 1575.32\n",
      "Average Steps: 1575.36\n",
      "\n",
      "Episode 8300/10000\n",
      "Average Reward: 809.96\n",
      "Average Steps: 810.03\n",
      "\n",
      "Episode 8400/10000\n",
      "Average Reward: 1037.21\n",
      "Average Steps: 1037.27\n",
      "\n",
      "Episode 8500/10000\n",
      "Average Reward: 1855.21\n",
      "Average Steps: 1855.23\n",
      "\n",
      "Episode 8600/10000\n",
      "Average Reward: 1453.43\n",
      "Average Steps: 1453.47\n",
      "\n",
      "Episode 8700/10000\n",
      "Average Reward: 1836.85\n",
      "Average Steps: 1836.89\n",
      "\n",
      "Episode 8800/10000\n",
      "Average Reward: 1999.98\n",
      "Average Steps: 2000.00\n",
      "\n",
      "Episode 8900/10000\n",
      "Average Reward: 1925.14\n",
      "Average Steps: 1925.16\n",
      "\n",
      "Metrics Debug at Episode 9000:\n",
      "Recent Average Reward: 1075.47\n",
      "Average Balance Time: 1075.54\n",
      "Balance Time Variance: 443800.13\n",
      "\n",
      "Episode 9000/10000\n",
      "Average Reward: 1075.47\n",
      "Average Steps: 1075.54\n",
      "\n",
      "Episode 9100/10000\n",
      "Average Reward: 1801.74\n",
      "Average Steps: 1801.77\n",
      "\n",
      "Episode 9200/10000\n",
      "Average Reward: 1601.04\n",
      "Average Steps: 1601.08\n",
      "\n",
      "Episode 9300/10000\n",
      "Average Reward: 1973.75\n",
      "Average Steps: 1973.77\n",
      "\n",
      "Episode 9400/10000\n",
      "Average Reward: 1999.98\n",
      "Average Steps: 2000.00\n",
      "\n",
      "Episode 9500/10000\n",
      "Average Reward: 1999.99\n",
      "Average Steps: 2000.00\n",
      "\n",
      "Episode 9600/10000\n",
      "Average Reward: 1999.99\n",
      "Average Steps: 2000.00\n",
      "\n",
      "Episode 9700/10000\n",
      "Average Reward: 1986.76\n",
      "Average Steps: 1986.78\n",
      "\n",
      "Episode 9800/10000\n",
      "Average Reward: 1981.56\n",
      "Average Steps: 1981.58\n",
      "\n",
      "Episode 9900/10000\n",
      "Average Reward: 1728.39\n",
      "Average Steps: 1728.42\n",
      "\n",
      "Completed testing energy_based\n",
      "Final average reward: 1335.00\n",
      "Final average balance time: 1335.06\n",
      "\n",
      "Testing reward function: baseline\n",
      "\n",
      "Metrics Debug at Episode 0:\n",
      "Recent Average Reward: 25.92\n",
      "Average Balance Time: 26.00\n",
      "Balance Time Variance: 0.00\n",
      "\n",
      "Episode 0/10000\n",
      "Average Reward: 25.92\n",
      "Average Steps: 26.00\n",
      "\n",
      "Episode 100/10000\n",
      "Average Reward: 19.34\n",
      "Average Steps: 19.44\n",
      "\n",
      "Episode 200/10000\n",
      "Average Reward: 41.74\n",
      "Average Steps: 41.94\n",
      "\n",
      "Episode 300/10000\n",
      "Average Reward: 50.01\n",
      "Average Steps: 50.26\n",
      "\n",
      "Episode 400/10000\n",
      "Average Reward: 80.23\n",
      "Average Steps: 80.76\n",
      "\n",
      "Episode 500/10000\n",
      "Average Reward: 126.46\n",
      "Average Steps: 127.23\n",
      "\n",
      "Episode 600/10000\n",
      "Average Reward: 210.32\n",
      "Average Steps: 210.82\n",
      "\n",
      "Episode 700/10000\n",
      "Average Reward: 297.14\n",
      "Average Steps: 297.29\n",
      "\n",
      "Episode 800/10000\n",
      "Average Reward: 10.08\n",
      "Average Steps: 10.15\n",
      "\n",
      "Episode 900/10000\n",
      "Average Reward: 18.95\n",
      "Average Steps: 19.06\n",
      "\n",
      "Metrics Debug at Episode 1000:\n",
      "Recent Average Reward: 9.36\n",
      "Average Balance Time: 9.42\n",
      "Balance Time Variance: 0.56\n",
      "\n",
      "Episode 1000/10000\n",
      "Average Reward: 9.36\n",
      "Average Steps: 9.42\n",
      "\n",
      "Episode 1100/10000\n",
      "Average Reward: 12.58\n",
      "Average Steps: 12.68\n",
      "\n",
      "Episode 1200/10000\n",
      "Average Reward: 17.33\n",
      "Average Steps: 17.45\n",
      "\n",
      "Episode 1300/10000\n",
      "Average Reward: 40.85\n",
      "Average Steps: 41.09\n",
      "\n",
      "Episode 1400/10000\n",
      "Average Reward: 67.75\n",
      "Average Steps: 68.02\n",
      "\n",
      "Episode 1500/10000\n",
      "Average Reward: 53.84\n",
      "Average Steps: 54.13\n",
      "\n",
      "Episode 1600/10000\n",
      "Average Reward: 32.61\n",
      "Average Steps: 32.85\n",
      "\n",
      "Episode 1700/10000\n",
      "Average Reward: 34.07\n",
      "Average Steps: 34.28\n",
      "\n",
      "Episode 1800/10000\n",
      "Average Reward: 36.91\n",
      "Average Steps: 37.10\n",
      "\n",
      "Episode 1900/10000\n",
      "Average Reward: 28.51\n",
      "Average Steps: 28.67\n",
      "\n",
      "Metrics Debug at Episode 2000:\n",
      "Recent Average Reward: 22.05\n",
      "Average Balance Time: 22.20\n",
      "Balance Time Variance: 451.46\n",
      "\n",
      "Episode 2000/10000\n",
      "Average Reward: 22.05\n",
      "Average Steps: 22.20\n",
      "\n",
      "Episode 2100/10000\n",
      "Average Reward: 148.18\n",
      "Average Steps: 148.50\n",
      "\n",
      "Episode 2200/10000\n",
      "Average Reward: 72.43\n",
      "Average Steps: 72.88\n",
      "\n",
      "Episode 2300/10000\n",
      "Average Reward: 884.89\n",
      "Average Steps: 885.16\n",
      "\n",
      "Episode 2400/10000\n",
      "Average Reward: 836.89\n",
      "Average Steps: 837.15\n",
      "\n",
      "Episode 2500/10000\n",
      "Average Reward: 587.16\n",
      "Average Steps: 587.50\n",
      "\n",
      "Episode 2600/10000\n",
      "Average Reward: 743.87\n",
      "Average Steps: 744.18\n",
      "\n",
      "Episode 2700/10000\n",
      "Average Reward: 215.95\n",
      "Average Steps: 216.30\n",
      "\n",
      "Episode 2800/10000\n",
      "Average Reward: 468.98\n",
      "Average Steps: 469.33\n",
      "\n",
      "Episode 2900/10000\n",
      "Average Reward: 330.33\n",
      "Average Steps: 330.89\n",
      "\n",
      "Metrics Debug at Episode 3000:\n",
      "Recent Average Reward: 130.44\n",
      "Average Balance Time: 130.69\n",
      "Balance Time Variance: 125971.75\n",
      "\n",
      "Episode 3000/10000\n",
      "Average Reward: 130.44\n",
      "Average Steps: 130.69\n",
      "\n",
      "Episode 3100/10000\n",
      "Average Reward: 305.40\n",
      "Average Steps: 305.72\n",
      "\n",
      "Episode 3200/10000\n",
      "Average Reward: 203.78\n",
      "Average Steps: 204.05\n",
      "\n",
      "Episode 3300/10000\n",
      "Average Reward: 321.83\n",
      "Average Steps: 322.21\n",
      "\n",
      "Episode 3400/10000\n",
      "Average Reward: 282.76\n",
      "Average Steps: 283.07\n",
      "\n",
      "Episode 3500/10000\n",
      "Average Reward: 186.82\n",
      "Average Steps: 187.16\n",
      "\n",
      "Episode 3600/10000\n",
      "Average Reward: 34.78\n",
      "Average Steps: 34.95\n",
      "\n",
      "Episode 3700/10000\n",
      "Average Reward: 44.53\n",
      "Average Steps: 44.71\n",
      "\n",
      "Episode 3800/10000\n",
      "Average Reward: 83.68\n",
      "Average Steps: 83.96\n",
      "\n",
      "Episode 3900/10000\n",
      "Average Reward: 136.25\n",
      "Average Steps: 136.54\n",
      "\n",
      "Metrics Debug at Episode 4000:\n",
      "Recent Average Reward: 145.77\n",
      "Average Balance Time: 146.25\n",
      "Balance Time Variance: 52263.57\n",
      "\n",
      "Episode 4000/10000\n",
      "Average Reward: 145.77\n",
      "Average Steps: 146.25\n",
      "\n",
      "Episode 4100/10000\n",
      "Average Reward: 203.84\n",
      "Average Steps: 204.22\n",
      "\n",
      "Episode 4200/10000\n",
      "Average Reward: 159.66\n",
      "Average Steps: 159.97\n",
      "\n",
      "Episode 4300/10000\n",
      "Average Reward: 23.99\n",
      "Average Steps: 24.13\n",
      "\n",
      "Episode 4400/10000\n",
      "Average Reward: 48.69\n",
      "Average Steps: 48.86\n",
      "\n",
      "Episode 4500/10000\n",
      "Average Reward: 27.77\n",
      "Average Steps: 27.95\n",
      "\n",
      "Episode 4600/10000\n",
      "Average Reward: 242.03\n",
      "Average Steps: 242.37\n",
      "\n",
      "Episode 4700/10000\n",
      "Average Reward: 195.35\n",
      "Average Steps: 195.75\n",
      "\n",
      "Episode 4800/10000\n",
      "Average Reward: 400.89\n",
      "Average Steps: 401.26\n",
      "\n",
      "Episode 4900/10000\n",
      "Average Reward: 283.73\n",
      "Average Steps: 284.10\n",
      "\n",
      "Metrics Debug at Episode 5000:\n",
      "Recent Average Reward: 165.92\n",
      "Average Balance Time: 166.34\n",
      "Balance Time Variance: 84140.60\n",
      "Environment parameters updated: masscart=1.0, length=0.9, gravity=9.8\n",
      "\n",
      "Changed pole length to: 0.9m at episode 5000\n",
      "\n",
      "Episode 5000/10000\n",
      "Average Reward: 165.92\n",
      "Average Steps: 166.34\n",
      "\n",
      "Episode 5100/10000\n",
      "Average Reward: 210.19\n",
      "Average Steps: 210.59\n",
      "\n",
      "Episode 5200/10000\n",
      "Average Reward: 27.71\n",
      "Average Steps: 27.89\n",
      "\n",
      "Episode 5300/10000\n",
      "Average Reward: 174.41\n",
      "Average Steps: 174.73\n",
      "\n",
      "Episode 5400/10000\n",
      "Average Reward: 249.23\n",
      "Average Steps: 249.54\n",
      "\n",
      "Episode 5500/10000\n",
      "Average Reward: 108.45\n",
      "Average Steps: 108.79\n",
      "\n",
      "Episode 5600/10000\n",
      "Average Reward: 129.17\n",
      "Average Steps: 129.45\n",
      "\n",
      "Episode 5700/10000\n",
      "Average Reward: 86.40\n",
      "Average Steps: 86.73\n",
      "\n",
      "Episode 5800/10000\n",
      "Average Reward: 76.72\n",
      "Average Steps: 76.99\n",
      "\n",
      "Episode 5900/10000\n",
      "Average Reward: 160.56\n",
      "Average Steps: 160.86\n",
      "\n",
      "Metrics Debug at Episode 6000:\n",
      "Recent Average Reward: 162.66\n",
      "Average Balance Time: 163.15\n",
      "Balance Time Variance: 99599.69\n",
      "\n",
      "Episode 6000/10000\n",
      "Average Reward: 162.66\n",
      "Average Steps: 163.15\n",
      "\n",
      "Episode 6100/10000\n",
      "Average Reward: 147.29\n",
      "Average Steps: 147.64\n",
      "\n",
      "Episode 6200/10000\n",
      "Average Reward: 116.72\n",
      "Average Steps: 117.24\n",
      "\n",
      "Episode 6300/10000\n",
      "Average Reward: 84.55\n",
      "Average Steps: 84.79\n",
      "\n",
      "Episode 6400/10000\n",
      "Average Reward: 192.48\n",
      "Average Steps: 192.77\n",
      "\n",
      "Episode 6500/10000\n",
      "Average Reward: 204.72\n",
      "Average Steps: 205.01\n",
      "\n",
      "Episode 6600/10000\n",
      "Average Reward: 83.26\n",
      "Average Steps: 83.53\n",
      "\n",
      "Episode 6700/10000\n",
      "Average Reward: 108.80\n",
      "Average Steps: 109.03\n",
      "\n",
      "Episode 6800/10000\n",
      "Average Reward: 207.71\n",
      "Average Steps: 208.00\n",
      "\n",
      "Episode 6900/10000\n",
      "Average Reward: 111.16\n",
      "Average Steps: 111.70\n",
      "\n",
      "Metrics Debug at Episode 7000:\n",
      "Recent Average Reward: 102.64\n",
      "Average Balance Time: 102.99\n",
      "Balance Time Variance: 46969.25\n",
      "\n",
      "Episode 7000/10000\n",
      "Average Reward: 102.64\n",
      "Average Steps: 102.99\n",
      "\n",
      "Episode 7100/10000\n",
      "Average Reward: 181.47\n",
      "Average Steps: 181.97\n",
      "\n",
      "Episode 7200/10000\n",
      "Average Reward: 108.37\n",
      "Average Steps: 108.72\n",
      "\n",
      "Episode 7300/10000\n",
      "Average Reward: 86.29\n",
      "Average Steps: 86.51\n",
      "\n",
      "Episode 7400/10000\n",
      "Average Reward: 175.34\n",
      "Average Steps: 175.64\n",
      "\n",
      "Episode 7500/10000\n",
      "Average Reward: 142.44\n",
      "Average Steps: 142.82\n",
      "\n",
      "Episode 7600/10000\n",
      "Average Reward: 124.19\n",
      "Average Steps: 124.48\n",
      "\n",
      "Episode 7700/10000\n",
      "Average Reward: 105.68\n",
      "Average Steps: 105.99\n",
      "\n",
      "Episode 7800/10000\n",
      "Average Reward: 149.88\n",
      "Average Steps: 150.15\n",
      "\n",
      "Episode 7900/10000\n",
      "Average Reward: 129.90\n",
      "Average Steps: 130.22\n",
      "\n",
      "Metrics Debug at Episode 8000:\n",
      "Recent Average Reward: 144.33\n",
      "Average Balance Time: 144.64\n",
      "Balance Time Variance: 21517.51\n",
      "\n",
      "Episode 8000/10000\n",
      "Average Reward: 144.33\n",
      "Average Steps: 144.64\n",
      "\n",
      "Episode 8100/10000\n",
      "Average Reward: 145.39\n",
      "Average Steps: 145.61\n",
      "\n",
      "Episode 8200/10000\n",
      "Average Reward: 81.93\n",
      "Average Steps: 82.22\n",
      "\n",
      "Episode 8300/10000\n",
      "Average Reward: 39.55\n",
      "Average Steps: 39.79\n",
      "\n",
      "Episode 8400/10000\n",
      "Average Reward: 67.97\n",
      "Average Steps: 68.23\n",
      "\n",
      "Episode 8500/10000\n",
      "Average Reward: 75.16\n",
      "Average Steps: 75.40\n",
      "\n",
      "Episode 8600/10000\n",
      "Average Reward: 31.58\n",
      "Average Steps: 31.71\n",
      "\n",
      "Episode 8700/10000\n",
      "Average Reward: 52.69\n",
      "Average Steps: 52.88\n",
      "\n",
      "Episode 8800/10000\n",
      "Average Reward: 81.95\n",
      "Average Steps: 82.31\n",
      "\n",
      "Episode 8900/10000\n",
      "Average Reward: 44.99\n",
      "Average Steps: 45.27\n",
      "\n",
      "Metrics Debug at Episode 9000:\n",
      "Recent Average Reward: 33.72\n",
      "Average Balance Time: 33.89\n",
      "Balance Time Variance: 1090.84\n",
      "\n",
      "Episode 9000/10000\n",
      "Average Reward: 33.72\n",
      "Average Steps: 33.89\n",
      "\n",
      "Episode 9100/10000\n",
      "Average Reward: 55.69\n",
      "Average Steps: 55.92\n",
      "\n",
      "Episode 9200/10000\n",
      "Average Reward: 139.62\n",
      "Average Steps: 139.76\n",
      "\n",
      "Episode 9300/10000\n",
      "Average Reward: 102.97\n",
      "Average Steps: 103.16\n",
      "\n",
      "Episode 9400/10000\n",
      "Average Reward: 337.51\n",
      "Average Steps: 337.74\n",
      "\n",
      "Episode 9500/10000\n",
      "Average Reward: 118.48\n",
      "Average Steps: 118.79\n",
      "\n",
      "Episode 9600/10000\n",
      "Average Reward: 463.53\n",
      "Average Steps: 463.60\n",
      "\n",
      "Episode 9700/10000\n",
      "Average Reward: 190.65\n",
      "Average Steps: 190.84\n",
      "\n",
      "Episode 9800/10000\n",
      "Average Reward: 177.23\n",
      "Average Steps: 177.61\n",
      "\n",
      "Episode 9900/10000\n",
      "Average Reward: 480.49\n",
      "Average Steps: 480.69\n",
      "\n",
      "Completed testing baseline\n",
      "Final average reward: 122.95\n",
      "Final average balance time: 123.13\n",
      "\n",
      "Testing reward function: pbrs\n",
      "\n",
      "Metrics Debug at Episode 0:\n",
      "Recent Average Reward: 10.92\n",
      "Average Balance Time: 11.00\n",
      "Balance Time Variance: 0.00\n",
      "\n",
      "Episode 0/10000\n",
      "Average Reward: 10.92\n",
      "Average Steps: 11.00\n",
      "\n",
      "Episode 100/10000\n",
      "Average Reward: 19.53\n",
      "Average Steps: 19.63\n",
      "\n",
      "Episode 200/10000\n",
      "Average Reward: 16.43\n",
      "Average Steps: 16.53\n",
      "\n",
      "Episode 300/10000\n",
      "Average Reward: 25.54\n",
      "Average Steps: 25.70\n",
      "\n",
      "Episode 400/10000\n",
      "Average Reward: 47.49\n",
      "Average Steps: 47.75\n",
      "\n",
      "Episode 500/10000\n",
      "Average Reward: 66.24\n",
      "Average Steps: 66.54\n",
      "\n",
      "Episode 600/10000\n",
      "Average Reward: 82.96\n",
      "Average Steps: 83.33\n",
      "\n",
      "Episode 700/10000\n",
      "Average Reward: 188.27\n",
      "Average Steps: 188.70\n",
      "\n",
      "Episode 800/10000\n",
      "Average Reward: 157.83\n",
      "Average Steps: 158.27\n",
      "\n",
      "Episode 900/10000\n",
      "Average Reward: 151.18\n",
      "Average Steps: 151.66\n",
      "\n",
      "Metrics Debug at Episode 1000:\n",
      "Recent Average Reward: 154.43\n",
      "Average Balance Time: 154.89\n",
      "Balance Time Variance: 7725.28\n",
      "\n",
      "Episode 1000/10000\n",
      "Average Reward: 154.43\n",
      "Average Steps: 154.89\n",
      "\n",
      "Episode 1100/10000\n",
      "Average Reward: 234.67\n",
      "Average Steps: 235.05\n",
      "\n",
      "Episode 1200/10000\n",
      "Average Reward: 366.05\n",
      "Average Steps: 366.28\n",
      "\n",
      "Episode 1300/10000\n",
      "Average Reward: 718.18\n",
      "Average Steps: 718.38\n",
      "\n",
      "Episode 1400/10000\n",
      "Average Reward: 850.33\n",
      "Average Steps: 850.55\n",
      "\n",
      "Episode 1500/10000\n",
      "Average Reward: 658.27\n",
      "Average Steps: 658.66\n",
      "\n",
      "Episode 1600/10000\n",
      "Average Reward: 1040.23\n",
      "Average Steps: 1040.50\n",
      "\n",
      "Episode 1700/10000\n",
      "Average Reward: 799.80\n",
      "Average Steps: 800.16\n",
      "\n",
      "Episode 1800/10000\n",
      "Average Reward: 1012.73\n",
      "Average Steps: 1012.97\n",
      "\n",
      "Episode 1900/10000\n",
      "Average Reward: 1281.56\n",
      "Average Steps: 1281.75\n",
      "\n",
      "Metrics Debug at Episode 2000:\n",
      "Recent Average Reward: 810.71\n",
      "Average Balance Time: 810.97\n",
      "Balance Time Variance: 452307.49\n",
      "\n",
      "Episode 2000/10000\n",
      "Average Reward: 810.71\n",
      "Average Steps: 810.97\n",
      "\n",
      "Episode 2100/10000\n",
      "Average Reward: 809.67\n",
      "Average Steps: 809.87\n",
      "\n",
      "Episode 2200/10000\n",
      "Average Reward: 1345.05\n",
      "Average Steps: 1345.18\n",
      "\n",
      "Episode 2300/10000\n",
      "Average Reward: 962.35\n",
      "Average Steps: 962.52\n",
      "\n",
      "Episode 2400/10000\n",
      "Average Reward: 214.29\n",
      "Average Steps: 214.52\n",
      "\n",
      "Episode 2500/10000\n",
      "Average Reward: 73.97\n",
      "Average Steps: 74.18\n",
      "\n",
      "Episode 2600/10000\n",
      "Average Reward: 626.44\n",
      "Average Steps: 626.62\n",
      "\n",
      "Episode 2700/10000\n",
      "Average Reward: 816.11\n",
      "Average Steps: 816.29\n",
      "\n",
      "Episode 2800/10000\n",
      "Average Reward: 1073.24\n",
      "Average Steps: 1073.38\n",
      "\n",
      "Episode 2900/10000\n",
      "Average Reward: 467.79\n",
      "Average Steps: 468.15\n",
      "\n",
      "Metrics Debug at Episode 3000:\n",
      "Recent Average Reward: 174.26\n",
      "Average Balance Time: 174.92\n",
      "Balance Time Variance: 64505.79\n",
      "\n",
      "Episode 3000/10000\n",
      "Average Reward: 174.26\n",
      "Average Steps: 174.92\n",
      "\n",
      "Episode 3100/10000\n",
      "Average Reward: 747.46\n",
      "Average Steps: 747.70\n",
      "\n",
      "Episode 3200/10000\n",
      "Average Reward: 1667.50\n",
      "Average Steps: 1667.55\n",
      "\n",
      "Episode 3300/10000\n",
      "Average Reward: 755.36\n",
      "Average Steps: 755.48\n",
      "\n",
      "Episode 3400/10000\n",
      "Average Reward: 909.23\n",
      "Average Steps: 909.37\n",
      "\n",
      "Episode 3500/10000\n",
      "Average Reward: 1623.15\n",
      "Average Steps: 1623.23\n",
      "\n",
      "Episode 3600/10000\n",
      "Average Reward: 1710.68\n",
      "Average Steps: 1710.73\n",
      "\n",
      "Episode 3700/10000\n",
      "Average Reward: 766.22\n",
      "Average Steps: 766.46\n",
      "\n",
      "Episode 3800/10000\n",
      "Average Reward: 156.64\n",
      "Average Steps: 156.89\n",
      "\n",
      "Episode 3900/10000\n",
      "Average Reward: 1050.16\n",
      "Average Steps: 1050.28\n",
      "\n",
      "Metrics Debug at Episode 4000:\n",
      "Recent Average Reward: 1780.93\n",
      "Average Balance Time: 1780.98\n",
      "Balance Time Variance: 265740.52\n",
      "\n",
      "Episode 4000/10000\n",
      "Average Reward: 1780.93\n",
      "Average Steps: 1780.98\n",
      "\n",
      "Episode 4100/10000\n",
      "Average Reward: 715.97\n",
      "Average Steps: 716.15\n",
      "\n",
      "Episode 4200/10000\n",
      "Average Reward: 70.46\n",
      "Average Steps: 70.68\n",
      "\n",
      "Episode 4300/10000\n",
      "Average Reward: 77.29\n",
      "Average Steps: 77.42\n",
      "\n",
      "Episode 4400/10000\n",
      "Average Reward: 10.98\n",
      "Average Steps: 11.06\n",
      "\n",
      "Episode 4500/10000\n",
      "Average Reward: 36.98\n",
      "Average Steps: 37.16\n",
      "\n",
      "Episode 4600/10000\n",
      "Average Reward: 65.98\n",
      "Average Steps: 66.17\n",
      "\n",
      "Episode 4700/10000\n",
      "Average Reward: 222.33\n",
      "Average Steps: 222.80\n",
      "\n",
      "Episode 4800/10000\n",
      "Average Reward: 183.46\n",
      "Average Steps: 183.85\n",
      "\n",
      "Episode 4900/10000\n",
      "Average Reward: 84.67\n",
      "Average Steps: 84.95\n",
      "\n",
      "Metrics Debug at Episode 5000:\n",
      "Recent Average Reward: 57.55\n",
      "Average Balance Time: 57.80\n",
      "Balance Time Variance: 316.54\n",
      "Environment parameters updated: masscart=1.0, length=0.3, gravity=9.8\n",
      "\n",
      "Changed pole length to: 0.3m at episode 5000\n",
      "\n",
      "Episode 5000/10000\n",
      "Average Reward: 57.55\n",
      "Average Steps: 57.80\n",
      "\n",
      "Episode 5100/10000\n",
      "Average Reward: 44.59\n",
      "Average Steps: 44.83\n",
      "\n",
      "Episode 5200/10000\n",
      "Average Reward: 38.18\n",
      "Average Steps: 38.40\n",
      "\n",
      "Episode 5300/10000\n",
      "Average Reward: 44.15\n",
      "Average Steps: 44.41\n",
      "\n",
      "Episode 5400/10000\n",
      "Average Reward: 11.62\n",
      "Average Steps: 11.71\n",
      "\n",
      "Episode 5500/10000\n",
      "Average Reward: 10.05\n",
      "Average Steps: 10.12\n",
      "\n",
      "Episode 5600/10000\n",
      "Average Reward: 13.53\n",
      "Average Steps: 13.61\n",
      "\n",
      "Episode 5700/10000\n",
      "Average Reward: 32.51\n",
      "Average Steps: 32.70\n",
      "\n",
      "Episode 5800/10000\n",
      "Average Reward: 38.22\n",
      "Average Steps: 38.44\n",
      "\n",
      "Episode 5900/10000\n",
      "Average Reward: 39.73\n",
      "Average Steps: 39.93\n",
      "\n",
      "Metrics Debug at Episode 6000:\n",
      "Recent Average Reward: 41.52\n",
      "Average Balance Time: 41.76\n",
      "Balance Time Variance: 817.14\n",
      "\n",
      "Episode 6000/10000\n",
      "Average Reward: 41.52\n",
      "Average Steps: 41.76\n",
      "\n",
      "Episode 6100/10000\n",
      "Average Reward: 49.19\n",
      "Average Steps: 49.48\n",
      "\n",
      "Episode 6200/10000\n",
      "Average Reward: 68.90\n",
      "Average Steps: 69.31\n",
      "\n",
      "Episode 6300/10000\n",
      "Average Reward: 106.54\n",
      "Average Steps: 106.83\n",
      "\n",
      "Episode 6400/10000\n",
      "Average Reward: 185.89\n",
      "Average Steps: 186.04\n",
      "\n",
      "Episode 6500/10000\n",
      "Average Reward: 763.53\n",
      "Average Steps: 763.69\n",
      "\n",
      "Episode 6600/10000\n",
      "Average Reward: 982.31\n",
      "Average Steps: 982.47\n",
      "\n",
      "Episode 6700/10000\n",
      "Average Reward: 1433.26\n",
      "Average Steps: 1433.35\n",
      "\n",
      "Episode 6800/10000\n",
      "Average Reward: 9.33\n",
      "Average Steps: 9.40\n",
      "\n",
      "Episode 6900/10000\n",
      "Average Reward: 9.35\n",
      "Average Steps: 9.41\n",
      "\n",
      "Metrics Debug at Episode 7000:\n",
      "Recent Average Reward: 9.24\n",
      "Average Balance Time: 9.30\n",
      "Balance Time Variance: 0.65\n",
      "\n",
      "Episode 7000/10000\n",
      "Average Reward: 9.24\n",
      "Average Steps: 9.30\n",
      "\n",
      "Episode 7100/10000\n",
      "Average Reward: 9.36\n",
      "Average Steps: 9.43\n",
      "\n",
      "Episode 7200/10000\n",
      "Average Reward: 26.14\n",
      "Average Steps: 26.30\n",
      "\n",
      "Episode 7300/10000\n",
      "Average Reward: 51.49\n",
      "Average Steps: 51.73\n",
      "\n",
      "Episode 7400/10000\n",
      "Average Reward: 35.13\n",
      "Average Steps: 35.34\n",
      "\n",
      "Episode 7500/10000\n",
      "Average Reward: 27.24\n",
      "Average Steps: 27.43\n",
      "\n",
      "Episode 7600/10000\n",
      "Average Reward: 21.51\n",
      "Average Steps: 21.66\n",
      "\n",
      "Episode 7700/10000\n",
      "Average Reward: 18.79\n",
      "Average Steps: 18.90\n",
      "\n",
      "Episode 7800/10000\n",
      "Average Reward: 14.27\n",
      "Average Steps: 14.35\n",
      "\n",
      "Episode 7900/10000\n",
      "Average Reward: 14.71\n",
      "Average Steps: 14.79\n",
      "\n",
      "Metrics Debug at Episode 8000:\n",
      "Recent Average Reward: 11.23\n",
      "Average Balance Time: 11.30\n",
      "Balance Time Variance: 18.85\n",
      "\n",
      "Episode 8000/10000\n",
      "Average Reward: 11.23\n",
      "Average Steps: 11.30\n",
      "\n",
      "Episode 8100/10000\n",
      "Average Reward: 13.06\n",
      "Average Steps: 13.13\n",
      "\n",
      "Episode 8200/10000\n",
      "Average Reward: 11.54\n",
      "Average Steps: 11.61\n",
      "\n",
      "Episode 8300/10000\n",
      "Average Reward: 13.66\n",
      "Average Steps: 13.74\n",
      "\n",
      "Episode 8400/10000\n",
      "Average Reward: 18.64\n",
      "Average Steps: 18.74\n",
      "\n",
      "Episode 8500/10000\n",
      "Average Reward: 18.45\n",
      "Average Steps: 18.57\n",
      "\n",
      "Episode 8600/10000\n",
      "Average Reward: 25.33\n",
      "Average Steps: 25.48\n",
      "\n",
      "Episode 8700/10000\n",
      "Average Reward: 27.10\n",
      "Average Steps: 27.26\n",
      "\n",
      "Episode 8800/10000\n",
      "Average Reward: 26.08\n",
      "Average Steps: 26.23\n",
      "\n",
      "Episode 8900/10000\n",
      "Average Reward: 27.98\n",
      "Average Steps: 28.15\n",
      "\n",
      "Metrics Debug at Episode 9000:\n",
      "Recent Average Reward: 24.14\n",
      "Average Balance Time: 24.29\n",
      "Balance Time Variance: 179.51\n",
      "\n",
      "Episode 9000/10000\n",
      "Average Reward: 24.14\n",
      "Average Steps: 24.29\n",
      "\n",
      "Episode 9100/10000\n",
      "Average Reward: 23.65\n",
      "Average Steps: 23.80\n",
      "\n",
      "Episode 9200/10000\n",
      "Average Reward: 22.82\n",
      "Average Steps: 22.94\n",
      "\n",
      "Episode 9300/10000\n",
      "Average Reward: 16.02\n",
      "Average Steps: 16.11\n",
      "\n",
      "Episode 9400/10000\n",
      "Average Reward: 14.41\n",
      "Average Steps: 14.49\n",
      "\n",
      "Episode 9500/10000\n",
      "Average Reward: 14.94\n",
      "Average Steps: 15.03\n",
      "\n",
      "Episode 9600/10000\n",
      "Average Reward: 16.77\n",
      "Average Steps: 16.87\n",
      "\n",
      "Episode 9700/10000\n",
      "Average Reward: 15.81\n",
      "Average Steps: 15.90\n",
      "\n",
      "Episode 9800/10000\n",
      "Average Reward: 15.39\n",
      "Average Steps: 15.49\n",
      "\n",
      "Episode 9900/10000\n",
      "Average Reward: 14.11\n",
      "Average Steps: 14.20\n",
      "\n",
      "Completed testing pbrs\n",
      "Final average reward: 17.22\n",
      "Final average balance time: 17.33\n",
      "Saved plot: performance_comparison_20250221_154233.png in PerformanceExperiment/experiment_20250221_144741/run_2\n",
      "Saved metrics table: metrics_run_2_20250221_154235.png in PerformanceExperiment/experiment_20250221_144741/run_2\n",
      "Saved metrics table: final_results_20250221_154235.png in PerformanceExperiment/experiment_20250221_144741\n",
      "\n",
      "Experiment results saved in: PerformanceExperiment/experiment_20250221_144741\n"
     ]
    }
   ],
   "source": [
    "# confidenceIntervals, allResults, resultsTable = runMultipleExperiments(\n",
    "#     numRuns=4,\n",
    "#     episodes=10000,\n",
    "#     changeInterval=5000\n",
    "# )\n",
    "\n",
    "confidenceIntervals, allResults, resultsTable = runMultipleExperiments(\n",
    "    numRuns=2,\n",
    "    episodes=10000,\n",
    "    changeInterval=5000\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
