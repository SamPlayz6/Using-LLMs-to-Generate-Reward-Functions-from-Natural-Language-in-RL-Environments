{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering Adding:\n",
    "- Confidence intervals for performance metrics (Most papers only really seem to go this far)\n",
    "- Statistical significance tests between different approaches\n",
    "- Variance analysis across multiple runs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **This experiment is investigating the performance of an adaptive reward function to state of the art reward functions in environments with environmentally variable changes.**\n",
    "\n",
    "-> Is there a statisitcally significant improvement in performance over time in this varying environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "current_dir = os.getcwd()  \n",
    "project_root = str(Path(current_dir).parent.parent)\n",
    "sys.path.append(project_root)\n",
    "\n",
    "\n",
    "# Initialize environment and device\n",
    "from AdaptiveRewardFunctionLearning.Prompts.prompts import device, apiKey,modelName\n",
    "\n",
    "#Cu stomCartPoleEnv\n",
    "from RLEnvironment.env import CustomCartPoleEnv\n",
    "#RewardUpdateSystem\n",
    "from AdaptiveRewardFunctionLearning.RewardGeneration.rewardCritic import RewardUpdateSystem\n",
    "#DQLearningAgent\n",
    "from RLEnvironment.training.agent import DQLearningAgent\n",
    "from RLEnvironment.training.training import trainDQLearning\n",
    "\n",
    "#DynamicRewardFunction\n",
    "from AdaptiveRewardFunctionLearning.RewardGeneration.rewardCodeGeneration import dynamicRewardFunction\n",
    "\n",
    "#import\n",
    "from AdaptiveRewardFunctionLearning.Visualisation.trainingTestFunctions import (\n",
    "    runEpisode,\n",
    "    detectJumps,\n",
    "    analyzeRewardSensibility,\n",
    "    performUpdate,\n",
    "    updateCompositeRewardFunction,\n",
    "    plotExperimentResults,\n",
    "    savePlot\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**State of the Art Reward Functions with Reference to Papers**\n",
    "\n",
    "**Potential-based Reward Shaping (PBRS):**\n",
    "```python\n",
    "def potentialBasedRewardShaping(observation, action):\n",
    "    x, xDot, angle, angleDot = observation\n",
    "    gamma = 0.99  # Example discount factor\n",
    "\n",
    "    def phi(x, xDot, angle, angleDot):\n",
    "        # Example potential function\n",
    "        return -abs(x) - abs(angle)\n",
    "\n",
    "    current_potential = phi(x, xDot, angle, angleDot)\n",
    "    next_potential = phi(x + xDot, angle + angleDot, xDot, angleDot)  # Simplified next state\n",
    "    return float(gamma * next_potential - current_potential)\n",
    "```\n",
    "\n",
    "Paper: \"Potential-based Shaping in Model-based Reinforcement Learning\"\n",
    "\n",
    "Link: https://cdn.aaai.org/AAAI/2008/AAAI08-096.pdf\n",
    "\n",
    "\n",
    "**Parameterized Reward Shaping:**\n",
    "```python\n",
    "def parameterizedRewardShaping(observation, action):\n",
    "    x, xDot, angle, angleDot = observation\n",
    "    original_reward = 1.0  # Assuming default CartPole reward\n",
    "\n",
    "    def f(x, xDot, angle, angleDot):\n",
    "        # Example shaping reward function\n",
    "        return -abs(angle)\n",
    "\n",
    "    def z_phi(x, xDot, angle, angleDot):\n",
    "        # Example shaping weight function\n",
    "        return 0.5\n",
    "\n",
    "    shaping_reward = f(x, xDot, angle, angleDot)\n",
    "    shaping_weight = z_phi(x, xDot, angle, angleDot)\n",
    "    return float(original_reward + shaping_weight * shaping_reward)\n",
    "```\n",
    "\n",
    "Paper: \"Learning to Utilize Shaping Rewards: A New Approach of Reward Shaping\"\n",
    "\n",
    "Link: http://arxiv.org/pdf/2011.02669.pdf\n",
    "\n",
    "\n",
    "**Energy Based Reward Function - Physics Based**\n",
    "\n",
    "```python\n",
    "def energyBasedReward(observation, action):\n",
    "    x, xDot, angle, angleDot = observation\n",
    "    \n",
    "    # Calculate kinetic and potential energy components\n",
    "    kineticEnergy = 0.5 * (xDot**2 + angleDot**2)\n",
    "    potentialEnergy = 9.8 * (1 + cos(angle))  # g * h, where h depends on angle\n",
    "    \n",
    "    # Reward is inverse of total energy (less energy = more stable = better reward)\n",
    "    energyPenalty = -(kineticEnergy + potentialEnergy)\n",
    "    return float(1.0 + 0.1 * energyPenalty)  # Base reward plus energy term\n",
    "```\n",
    "\n",
    "Paper: \"Energy-Based Control for Safe Robot Learning\" (2019)\n",
    "\n",
    "Link: https://ieeexplore.ieee.org/document/8794207\n",
    "\n",
    "\n",
    "**Baseline Reward Function:**\n",
    "```python\n",
    "def baselineCartPoleReward(observation, action):\n",
    "    return 1.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Performance Experiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State of the art Reward Functions\n",
    "\n",
    "def potentialBasedRewardShaping(observation, action):\n",
    "    x, xDot, angle, angleDot = observation\n",
    "    gamma = 0.99  # Example discount factor\n",
    "\n",
    "    def phi(x, xDot, angle, angleDot):\n",
    "        # Example potential function\n",
    "        return -abs(x) - abs(angle)\n",
    "\n",
    "    current_potential = phi(x, xDot, angle, angleDot)\n",
    "    next_potential = phi(x + xDot, angle + angleDot, xDot, angleDot)  # Simplified next state\n",
    "    return float(gamma * next_potential - current_potential)\n",
    "\n",
    "\n",
    "def parameterizedRewardShaping(observation, action):\n",
    "    x, xDot, angle, angleDot = observation\n",
    "    original_reward = 1.0  # Assuming default CartPole reward\n",
    "\n",
    "    def f(x, xDot, angle, angleDot):\n",
    "        # Example shaping reward function\n",
    "        return -abs(angle)\n",
    "\n",
    "    def z_phi(x, xDot, angle, angleDot):\n",
    "        # Example shaping weight function\n",
    "        return 0.5\n",
    "\n",
    "    shaping_reward = f(x, xDot, angle, angleDot)\n",
    "    shaping_weight = z_phi(x, xDot, angle, angleDot)\n",
    "    return float(original_reward + shaping_weight * shaping_reward)\n",
    "\n",
    "\n",
    "def energyBasedReward(observation, action):\n",
    "    x, xDot, angle, angleDot = observation\n",
    "    \n",
    "    # Calculate kinetic and potential energy components\n",
    "    kineticEnergy = 0.5 * (xDot**2 + angleDot**2)\n",
    "    potentialEnergy = 9.8 * (1 + cos(angle))  # g * h, where h depends on angle\n",
    "    \n",
    "    # Reward is inverse of total energy (less energy = more stable = better reward)\n",
    "    energyPenalty = -(kineticEnergy + potentialEnergy)\n",
    "    return float(1.0 + 0.1 * energyPenalty)  # Base reward plus energy term\n",
    "\n",
    "\n",
    "def baselineCartPoleReward(observation, action):\n",
    "    return 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runPerformanceComparisonTest(episodes=1000,changeInterval=50):\n",
    "    print(\"Starting Performance Comparison Test...\")\n",
    "    \n",
    "    # Initialize environments and agents\n",
    "    env = gym.make('CartPole-v1')\n",
    "    env = CustomCartPoleEnv(env)\n",
    "    \n",
    "    # Define all reward functions\n",
    "    rewardfunctions = {\n",
    "        'adaptivereward': {\n",
    "            'agent': DQLearningAgent(env, 4, 2, device),\n",
    "            'updatesystem': RewardUpdateSystem(apiKey, modelName),\n",
    "            'rewardfunction': None  # Uses your existing adaptive reward\n",
    "        },\n",
    "        'baselinereward': {\n",
    "            'agent': DQLearningAgent(env, 4, 2, device),\n",
    "            'updatesystem': None,\n",
    "            'rewardfunction': baselineCartPoleReward\n",
    "        },\n",
    "        'pbrs': {\n",
    "            'agent': DQLearningAgent(env, 4, 2, device),\n",
    "            'updatesystem': None,\n",
    "            'rewardfunction': potentialBasedRewardShaping\n",
    "        },\n",
    "        'parameterized': {\n",
    "            'agent': DQLearningAgent(env, 4, 2, device),\n",
    "            'updatesystem': None,\n",
    "            'rewardfunction': parameterizedRewardShaping\n",
    "        },\n",
    "        'energybased': {\n",
    "            'agent': DQLearningAgent(env, 4, 2, device),\n",
    "            'updatesystem': None,\n",
    "            'rewardfunction': energyBasedReward\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for rewardname, rewardinfo in rewardfunctions.items():\n",
    "        print(f\"\\nTesting reward function: {rewardname}\")\n",
    "        \n",
    "        # Reset environment for each test\n",
    "        env.reset()\n",
    "        \n",
    "        # Set the reward function for this test\n",
    "        if rewardname != 'adaptivereward':\n",
    "            env.setRewardFunction(rewardinfo['rewardfunction'])\n",
    "        \n",
    "        # Training metrics\n",
    "        episoderewards = []\n",
    "        episodebalancetimes = []\n",
    "        rewardchangeepisodes = [] if rewardname == 'adaptivereward' else None\n",
    "        \n",
    "        def onEpisodeEnd(env, updatesystem, episode, reward, steps):\n",
    "            nonlocal episoderewards, episodebalancetimes, rewardchangeepisodes\n",
    "            \n",
    "            episoderewards.append(reward)\n",
    "            episodebalancetimes.append(steps)\n",
    "            \n",
    "            if rewardname == 'adaptivereward':\n",
    "                # Your existing adaptive reward update logic\n",
    "                if hasattr(env.rewardFunction, 'compositeHistory'):\n",
    "                    latest_updates = [\n",
    "                        update['episode'] for update in env.rewardFunction.compositeHistory \n",
    "                        if update['episode'] == episode\n",
    "                    ]\n",
    "                    if latest_updates:\n",
    "                        rewardchangeepisodes.append(episode)\n",
    "            \n",
    "            if episode % changeInterval == 0:\n",
    "                print(f\"Episode {episode}: Average Balance Time = {np.mean(episodebalancetimes[-50:]):.2f}\")\n",
    "        \n",
    "        # Train using your existing function\n",
    "        agent, env, rewards = trainDQLearning(\n",
    "            agent=rewardinfo['agent'],\n",
    "            env=env,\n",
    "            numEpisodes=episodes,\n",
    "            updateSystem=rewardinfo['updatesystem'],\n",
    "            onEpisodeEnd=onEpisodeEnd\n",
    "        )\n",
    "        \n",
    "        results[rewardname] = {\n",
    "            'rewards': episoderewards,\n",
    "            'balancetimes': episodebalancetimes,\n",
    "            'rewardchanges': rewardchangeepisodes\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nCompleted testing {rewardname}\")\n",
    "        print(f\"Final average reward: {np.mean(episoderewards[-100:]):.2f}\")\n",
    "        print(f\"Final average balance time: {np.mean(episodebalancetimes[-100:]):.2f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Performance Comparison Test...\n",
      "\n",
      "Testing reward function: adaptivereward\n",
      "Episode 0: Average Balance Time = 11.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\samdd\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50: Average Balance Time = 19.86\n",
      "Episode 100: Average Balance Time = 17.44\n",
      "Episode 150: Average Balance Time = 13.16\n",
      "Episode 200: Average Balance Time = 14.08\n",
      "Episode 250: Average Balance Time = 17.66\n",
      "Episode 300: Average Balance Time = 11.40\n",
      "Episode 350: Average Balance Time = 16.64\n",
      "Episode 400: Average Balance Time = 19.16\n",
      "Episode 450: Average Balance Time = 32.12\n",
      "Episode 500: Average Balance Time = 48.74\n",
      "Episode 550: Average Balance Time = 66.12\n",
      "Episode 600: Average Balance Time = 66.88\n",
      "Episode 650: Average Balance Time = 81.70\n",
      "Episode 700: Average Balance Time = 108.12\n",
      "Episode 750: Average Balance Time = 51.98\n",
      "Episode 800: Average Balance Time = 70.70\n",
      "Episode 850: Average Balance Time = 62.54\n",
      "Episode 900: Average Balance Time = 89.70\n",
      "Episode 950: Average Balance Time = 81.88\n",
      "\n",
      "Completed testing adaptivereward\n",
      "Final average reward: 130.09\n",
      "Final average balance time: 130.54\n",
      "\n",
      "Testing reward function: baselinereward\n",
      "Episode 0: Average Balance Time = 36.00\n",
      "Episode 50: Average Balance Time = 23.84\n",
      "Episode 100: Average Balance Time = 17.94\n",
      "Episode 150: Average Balance Time = 22.16\n",
      "Episode 200: Average Balance Time = 31.42\n",
      "Episode 250: Average Balance Time = 44.48\n",
      "Episode 300: Average Balance Time = 57.66\n",
      "Episode 350: Average Balance Time = 37.02\n",
      "Episode 400: Average Balance Time = 86.96\n",
      "Episode 450: Average Balance Time = 138.88\n",
      "Episode 500: Average Balance Time = 196.72\n",
      "Episode 550: Average Balance Time = 239.40\n",
      "Episode 600: Average Balance Time = 198.14\n",
      "Episode 650: Average Balance Time = 292.42\n",
      "Episode 700: Average Balance Time = 236.42\n",
      "Episode 750: Average Balance Time = 112.50\n",
      "Episode 800: Average Balance Time = 182.40\n",
      "Episode 850: Average Balance Time = 177.12\n",
      "Episode 900: Average Balance Time = 245.58\n",
      "Episode 950: Average Balance Time = 191.88\n",
      "\n",
      "Completed testing baselinereward\n",
      "Final average reward: 191.38\n",
      "Final average balance time: 191.38\n",
      "\n",
      "Testing reward function: pbrs\n",
      "Episode 0: Average Balance Time = 26.00\n",
      "Episode 50: Average Balance Time = 22.22\n",
      "Episode 100: Average Balance Time = 36.02\n",
      "Episode 150: Average Balance Time = 30.76\n",
      "Episode 200: Average Balance Time = 44.66\n",
      "Episode 250: Average Balance Time = 33.06\n",
      "Episode 300: Average Balance Time = 32.66\n",
      "Episode 350: Average Balance Time = 32.26\n",
      "Episode 400: Average Balance Time = 31.64\n",
      "Episode 450: Average Balance Time = 27.40\n",
      "Episode 500: Average Balance Time = 34.70\n",
      "Episode 550: Average Balance Time = 32.76\n",
      "Episode 600: Average Balance Time = 34.08\n",
      "Episode 650: Average Balance Time = 34.04\n",
      "Episode 700: Average Balance Time = 29.90\n",
      "Episode 750: Average Balance Time = 34.22\n",
      "Episode 800: Average Balance Time = 34.26\n",
      "Episode 850: Average Balance Time = 45.58\n",
      "Episode 900: Average Balance Time = 43.42\n",
      "Episode 950: Average Balance Time = 37.58\n",
      "\n",
      "Completed testing pbrs\n",
      "Final average reward: -5.01\n",
      "Final average balance time: 37.58\n",
      "\n",
      "Testing reward function: parameterized\n",
      "Episode 0: Average Balance Time = 32.00\n",
      "Episode 50: Average Balance Time = 21.48\n",
      "Episode 100: Average Balance Time = 19.20\n",
      "Episode 150: Average Balance Time = 27.06\n",
      "Episode 200: Average Balance Time = 47.64\n",
      "Episode 250: Average Balance Time = 56.06\n"
     ]
    }
   ],
   "source": [
    "# Run Experiment\n",
    "results = runPerformanceComparisonTest(1000,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizePerformanceComparison(results):\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "    \n",
    "    # Color map for different reward functions\n",
    "    colors = ['b', 'g', 'r', 'c', 'm']  # Different color for each function\n",
    "    \n",
    "    # Plot rewards for each reward function\n",
    "    for idx, (rewardname, rewardresults) in enumerate(results.items()):\n",
    "        rewards = rewardresults['rewards']\n",
    "        ax1.plot(pd.Series(rewards).rolling(50).mean(), \n",
    "                label=f'{rewardname}', linewidth=2, color=colors[idx])\n",
    "        \n",
    "        # Add reward function change markers for adaptive model\n",
    "        if rewardname == 'adaptivereward' and rewardresults['rewardchanges']:\n",
    "            for episode in rewardresults['rewardchanges']:\n",
    "                ax1.axvline(x=episode, color='g', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    ax1.set_title('Average Reward Over Time')\n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.set_ylabel('Reward')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot balance times\n",
    "    for idx, (rewardname, rewardresults) in enumerate(results.items()):\n",
    "        balancetimes = rewardresults['balancetimes']\n",
    "        ax2.plot(pd.Series(balancetimes).rolling(50).mean(),\n",
    "                label=f'{rewardname}', linewidth=2, color=colors[idx])\n",
    "    \n",
    "    ax2.set_title('Average Balance Time Over Episodes')\n",
    "    ax2.set_xlabel('Episode')\n",
    "    ax2.set_ylabel('Steps')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    savePlot(fig, \"performancecomparison\", \"PerformanceExperiment\")\n",
    "    plt.close()\n",
    "\n",
    "def calculateStability(rewards):\n",
    "    \"\"\"\n",
    "    Calculate stability score based on reward variance in the last 100 episodes\n",
    "    Lower variance = higher stability\n",
    "    \"\"\"\n",
    "    if len(rewards) < 100:\n",
    "        return 0.0\n",
    "    \n",
    "    last_hundred = rewards[-100:]\n",
    "    mean_reward = np.mean(last_hundred)\n",
    "    if mean_reward == 0:\n",
    "        return 0.0\n",
    "        \n",
    "    # Calculate coefficient of variation (normalized standard deviation)\n",
    "    stability = 1 - (np.std(last_hundred) / mean_reward)\n",
    "    return max(0, min(1, stability))  # Normalize between 0 and 1\n",
    "\n",
    "def calculateConvergenceTime(rewards, threshold=195, window=50):\n",
    "    \"\"\"\n",
    "    Calculate the number of episodes needed to reach and maintain a certain performance\n",
    "    threshold for a given window of episodes\n",
    "    \"\"\"\n",
    "    if len(rewards) < window:\n",
    "        return len(rewards)\n",
    "    \n",
    "    rolling_mean = pd.Series(rewards).rolling(window).mean()\n",
    "    \n",
    "    for episode in range(window, len(rewards)):\n",
    "        if rolling_mean[episode] >= threshold:\n",
    "            # Check if performance is maintained\n",
    "            maintained = all(avg >= threshold * 0.9 for avg in rolling_mean[episode:episode+window])\n",
    "            if maintained:\n",
    "                return episode\n",
    "    \n",
    "    return len(rewards)  # If never converged, return total episodes\n",
    "\n",
    "def calculatePerformanceMetrics(results):\n",
    "    metrics = {}\n",
    "    for rewardname, rewardresults in results.items():\n",
    "        metrics[rewardname] = {\n",
    "            'finalavgreward': np.mean(rewardresults['rewards'][-100:]),\n",
    "            'finalavgbalance': np.mean(rewardresults['balancetimes'][-100:]),\n",
    "            'convergencetime': calculateConvergenceTime(rewardresults['rewards']),\n",
    "            'stability': calculateStability(rewardresults['rewards'])\n",
    "        }\n",
    "    return pd.DataFrame(metrics).T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gymenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
