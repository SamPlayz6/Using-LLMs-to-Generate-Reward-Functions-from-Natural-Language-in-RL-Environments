{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering Adding:\n",
    "- Confidence intervals for performance metrics (Most papers only really seem to go this far)\n",
    "- Statistical significance tests between different approaches\n",
    "- Variance analysis across multiple runs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **This experiment is investigating the performance of an adaptive reward function to state of the art reward functions in enironments with enironmental variable changes.**\n",
    "\n",
    "-> Is there a statisitcally significant improvement in performance over time in this varying environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Performance Experiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State of the art agents \n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, env, device):\n",
    "        # PPO implementation...\n",
    "        pass\n",
    "\n",
    "class A2CAgent:\n",
    "    def __init__(self, env, device):\n",
    "        # A2C implementation...\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "def runPerformanceComparisonTest(episodes=1000):\n",
    "    print(\"Starting Performance Comparison Test...\")\n",
    "    \n",
    "    # Initialize environments and agents\n",
    "    env = gym.make('CartPole-v1')\n",
    "    env = CustomCartPoleEnv(env)\n",
    "    \n",
    "    models = {\n",
    "        'adaptive_reward': {\n",
    "            'agent': DQLearningAgent(env, 4, 2, device),\n",
    "            'update_system': RewardUpdateSystem(apiKey, modelName)\n",
    "        },\n",
    "        'ppo': {\n",
    "            'agent': PPOAgent(env, device),\n",
    "            'update_system': None\n",
    "        },\n",
    "        'a2c': {\n",
    "            'agent': A2CAgent(env, device),\n",
    "            'update_system': None\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for model_name, model_info in models.items():\n",
    "        print(f\"\\nTesting model: {model_name}\")\n",
    "        \n",
    "        # Reset environment for each model\n",
    "        env.reset()\n",
    "        \n",
    "        # Training metrics\n",
    "        episode_rewards = []\n",
    "        episode_balance_times = []\n",
    "        reward_change_episodes = [] if model_name == 'adaptive_reward' else None\n",
    "        \n",
    "        def onEpisodeEnd(env, updateSystem, episode, reward, steps):\n",
    "            # Similar to your existing callback, but model-specific...\n",
    "            pass\n",
    "        \n",
    "        # Train using your existing function\n",
    "        agent, env, rewards = trainDQLearning(\n",
    "            agent=model_info['agent'],\n",
    "            env=env,\n",
    "            numEpisodes=episodes,\n",
    "            updateSystem=model_info['update_system'],\n",
    "            onEpisodeEnd=onEpisodeEnd\n",
    "        )\n",
    "        \n",
    "        results[model_name] = {\n",
    "            'rewards': episode_rewards,\n",
    "            'balance_times': episode_balance_times,\n",
    "            'reward_changes': reward_change_episodes\n",
    "        }\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Experiment\n",
    "results = runPerformanceComparisonTest(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation and Metrics\n",
    "\n",
    "def visualizePerformanceComparison(results):\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "    \n",
    "    # Plot rewards for each model\n",
    "    for model_name, model_results in results.items():\n",
    "        rewards = model_results['rewards']\n",
    "        ax1.plot(pd.Series(rewards).rolling(50).mean(), \n",
    "                label=f'{model_name}', linewidth=2)\n",
    "        \n",
    "        # Add reward function change markers for adaptive model\n",
    "        if model_name == 'adaptive_reward' and model_results['reward_changes']:\n",
    "            for ep in model_results['reward_changes']:\n",
    "                ax1.axvline(x=ep, color='g', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    ax1.set_title('Average Reward Over Time')\n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.set_ylabel('Reward')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Similar plotting for balance times...\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    savePlot(fig, \"performanceComparison\", \"PerformanceExperiment\")\n",
    "    plt.close()\n",
    "\n",
    "# Generate and Display Metrics\n",
    "def calculatePerformanceMetrics(results):\n",
    "    metrics = {}\n",
    "    for model_name, model_results in results.items():\n",
    "        metrics[model_name] = {\n",
    "            'final_avg_reward': np.mean(model_results['rewards'][-100:]),\n",
    "            'final_avg_balance': np.mean(model_results['balance_times'][-100:]),\n",
    "            'learning_speed': calculateLearningSpeed(model_results['rewards']),\n",
    "            'stability': calculateStability(model_results['rewards'])\n",
    "        }\n",
    "    return pd.DataFrame(metrics).T\n",
    "\n",
    "metrics = calculatePerformanceMetrics(results)\n",
    "display(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gymenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
