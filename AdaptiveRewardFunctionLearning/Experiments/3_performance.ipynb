{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering Adding:\n",
    "- Confidence intervals for performance metrics (Most papers only really seem to go this far)\n",
    "- Statistical significance tests between different approaches\n",
    "- Variance analysis across multiple runs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in /home/sd37/.local/lib/python3.10/site-packages (4.1.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /home/sd37/.local/lib/python3.10/site-packages (from optuna) (1.14.0)\n",
      "Requirement already satisfied: colorlog in /home/sd37/.local/lib/python3.10/site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: numpy in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from optuna) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from optuna) (24.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from optuna) (2.0.37)\n",
      "Requirement already satisfied: tqdm in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from optuna) (4.66.5)\n",
      "Requirement already satisfied: PyYAML in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from optuna) (6.0)\n",
      "Requirement already satisfied: Mako in /home/sd37/.local/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (1.3.8)\n",
      "Requirement already satisfied: typing-extensions>=4 in /home/sd37/.local/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sd37/.conda/envs/thesis/bin/python\n",
      "4.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sd37/.conda/envs/thesis/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "\n",
    "import optuna\n",
    "print(optuna.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **This experiment is investigating the performance of an adaptive reward function to state of the art reward functions in environments with environmentally variable changes.**\n",
    "\n",
    "-> Is there a statisitcally significant improvement in performance over time in this varying environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "current_dir = os.getcwd()  \n",
    "project_root = str(Path(current_dir).parent.parent)\n",
    "sys.path.append(project_root)\n",
    "\n",
    "\n",
    "# Initialize environment and device\n",
    "from AdaptiveRewardFunctionLearning.Prompts.prompts import device, apiKey,modelName\n",
    "\n",
    "#Cu stomCartPoleEnv\n",
    "from RLEnvironment.env import CustomCartPoleEnv\n",
    "#RewardUpdateSystem\n",
    "from AdaptiveRewardFunctionLearning.RewardGeneration.rewardCritic import RewardUpdateSystem\n",
    "#DQLearningAgent\n",
    "from RLEnvironment.training.agent import DQLearningAgent\n",
    "from RLEnvironment.training.training import trainDQLearning\n",
    "\n",
    "#DynamicRewardFunction\n",
    "from AdaptiveRewardFunctionLearning.RewardGeneration.rewardCodeGeneration import dynamicRewardFunction\n",
    "\n",
    "#import\n",
    "from AdaptiveRewardFunctionLearning.Visualisation.trainingTestFunctions import (\n",
    "    runEpisode,\n",
    "    detectJumps,\n",
    "    analyzeRewardSensibility,\n",
    "    performUpdate,\n",
    "    updateCompositeRewardFunction,\n",
    "    plotExperimentResults,\n",
    "    savePlot\n",
    ")\n",
    "\n",
    "# Import new reward functions\n",
    "from AdaptiveRewardFunctionLearning.RewardGeneration.cartpole_energy_reward import EnergyBasedRewardFunction\n",
    "from AdaptiveRewardFunctionLearning.RewardGeneration.cartpole_meta_learning import meta_learning_cartpole\n",
    "from AdaptiveRewardFunctionLearning.RewardGeneration.reward_meta_learning import RewardFunctionMetaLearner"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**State of the Art Reward Functions with Reference to Papers**\n",
    "\n",
    "**Potential-based Reward Shaping (PBRS):**\n",
    "```python\n",
    "def potentialBasedRewardShaping(observation, action):\n",
    "    x, xDot, angle, angleDot = observation\n",
    "    gamma = 0.99  # Example discount factor\n",
    "\n",
    "    def phi(x, xDot, angle, angleDot):\n",
    "        # Example potential function\n",
    "        return -abs(x) - abs(angle)\n",
    "\n",
    "    current_potential = phi(x, xDot, angle, angleDot)\n",
    "    next_potential = phi(x + xDot, angle + angleDot, xDot, angleDot)  # Simplified next state\n",
    "    return float(gamma * next_potential - current_potential)\n",
    "```\n",
    "\n",
    "Paper: \"Potential-based Shaping in Model-based Reinforcement Learning\"\n",
    "\n",
    "Link: https://cdn.aaai.org/AAAI/2008/AAAI08-096.pdf\n",
    "\n",
    "\n",
    "**Parameterized Reward Shaping:**\n",
    "```python\n",
    "def parameterizedRewardShaping(observation, action):\n",
    "    x, xDot, angle, angleDot = observation\n",
    "    original_reward = 1.0  # Assuming default CartPole reward\n",
    "\n",
    "    def f(x, xDot, angle, angleDot):\n",
    "        # Example shaping reward function\n",
    "        return -abs(angle)\n",
    "\n",
    "    def z_phi(x, xDot, angle, angleDot):\n",
    "        # Example shaping weight function\n",
    "        return 0.5\n",
    "\n",
    "    shaping_reward = f(x, xDot, angle, angleDot)\n",
    "    shaping_weight = z_phi(x, xDot, angle, angleDot)\n",
    "    return float(original_reward + shaping_weight * shaping_reward)\n",
    "```\n",
    "\n",
    "Paper: \"Learning to Utilize Shaping Rewards: A New Approach of Reward Shaping\"\n",
    "\n",
    "Link: http://arxiv.org/pdf/2011.02669.pdf\n",
    "\n",
    "\n",
    "**Energy Based Reward Function - Physics Based**\n",
    "\n",
    "```python\n",
    "def energyBasedReward(observation, action):\n",
    "    x, xDot, angle, angleDot = observation\n",
    "    \n",
    "    # Calculate kinetic and potential energy components\n",
    "    kineticEnergy = 0.5 * (xDot**2 + angleDot**2)\n",
    "    potentialEnergy = 9.8 * (1 + cos(angle))  # g * h, where h depends on angle\n",
    "    \n",
    "    # Reward is inverse of total energy (less energy = more stable = better reward)\n",
    "    energyPenalty = -(kineticEnergy + potentialEnergy)\n",
    "    return float(1.0 + 0.1 * energyPenalty)  # Base reward plus energy term\n",
    "```\n",
    "\n",
    "Paper: \"Energy-Based Control for Safe Robot Learning\" (2019)\n",
    "\n",
    "Link: https://ieeexplore.ieee.org/document/8794207\n",
    "\n",
    "\n",
    "**Baseline Reward Function:**\n",
    "```python\n",
    "def baselineCartPoleReward(observation, action):\n",
    "    return 1.0\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Performance Experiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize reward functions and meta-learners\n",
    "energy_reward = EnergyBasedRewardFunction(mass_cart=1.0, mass_pole=0.1, length=0.5, gravity=9.8)\n",
    "meta_reward = RewardFunctionMetaLearner(state_dim=4, action_dim=1)  # CartPole has 4 state dims, 1 action dim\n",
    "\n",
    "def potentialBasedRewardShaping(observation, action):\n",
    "    \"\"\"Advanced potential-based reward shaping using meta-learning\"\"\"\n",
    "    reward_func = meta_reward.generate_reward_function()\n",
    "    return float(reward_func(observation, action))\n",
    "\n",
    "def parameterizedRewardShaping(observation, action):\n",
    "    \"\"\"Meta-learning based parameterized reward shaping\"\"\"\n",
    "    # Use meta-learning framework for parameter optimization\n",
    "    learner = meta_learning_cartpole()\n",
    "    return float(learner.parameterized_reward(observation, action))\n",
    "\n",
    "def energyBasedReward(observation, action):\n",
    "    \"\"\"Enhanced physics-based energy reward\"\"\"\n",
    "    return float(energy_reward.compute_reward(observation, action))\n",
    "\n",
    "def baselineReward(observation, action):\n",
    "    \"\"\"Standard baseline reward\"\"\"\n",
    "    return 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runPerformanceComparisonTest(\n",
    "    episodes=1000, \n",
    "    changeInterval=500, \n",
    "    lengthchanges=[0.5, 1.5],\n",
    "    mass_cart=1.0,\n",
    "    mass_pole=0.1,\n",
    "    initial_length=0.5,\n",
    "    gravity=9.8\n",
    "):\n",
    "    print(\"Starting Performance Comparison Test...\")\n",
    "    \n",
    "    currentlengthidx = 0\n",
    "    \n",
    "    env = gym.make('CartPole-v1', max_episode_steps=2000)\n",
    "    env = CustomCartPoleEnv(env)\n",
    "    env.setEnvironmentParameters(masscart=mass_cart, length=lengthchanges[0], gravity=gravity)\n",
    "    \n",
    "    energy_reward = EnergyBasedRewardFunction(\n",
    "        mass_cart=mass_cart, \n",
    "        mass_pole=mass_pole, \n",
    "        length=initial_length, \n",
    "        gravity=gravity\n",
    "    )\n",
    "    meta_reward = RewardFunctionMetaLearner(state_dim=4, action_dim=1)\n",
    "    \n",
    "    rewardfunctions = {\n",
    "        'adaptivereward': {\n",
    "            'agent': DQLearningAgent(env, 4, 2, device),\n",
    "            'updatesystem': RewardUpdateSystem(apiKey, modelName), \n",
    "            'rewardfunction': None,\n",
    "            'update_method': 'llm'\n",
    "        },\n",
    "        'meta_learning': {\n",
    "            'agent': DQLearningAgent(env, 4, 2, device),\n",
    "            'updatesystem': meta_reward,\n",
    "            'rewardfunction': meta_reward.generate_reward_function(),\n",
    "            'update_method': 'meta'\n",
    "        },\n",
    "        'energy_based': {\n",
    "            'agent': DQLearningAgent(env, 4, 2, device),\n",
    "            'updatesystem': energy_reward,\n",
    "            'rewardfunction': energy_reward.compute_reward,\n",
    "            'update_method': 'physics'\n",
    "        },\n",
    "        'baseline': {                           # Add this new entry\n",
    "            'agent': DQLearningAgent(env, 4, 2, device),\n",
    "            'updatesystem': None,               # No update system needed\n",
    "            'rewardfunction': baselineReward,   # Use our simple function\n",
    "            'update_method': None               # No updates needed\n",
    "        }\n",
    "    }\n",
    "\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for rewardname, rewardinfo in rewardfunctions.items():\n",
    "        print(f\"\\nTesting reward function: {rewardname}\")\n",
    "        \n",
    "        env.reset()\n",
    "        if rewardname != 'adaptivereward':\n",
    "            env.setRewardFunction(rewardinfo['rewardfunction'])\n",
    "        else:\n",
    "            # Reset lastUpdateEpisode for adaptive reward at start of its run\n",
    "            rewardinfo['updatesystem'].lastUpdateEpisode = 0\n",
    "        \n",
    "        episoderewards = []\n",
    "        episodebalancetimes = []\n",
    "        rewardchangeepisodes = []\n",
    "        \n",
    "        def onEpisodeEnd(env, updatesystem, episode, reward, steps):\n",
    "            nonlocal episoderewards, episodebalancetimes, rewardchangeepisodes, currentlengthidx\n",
    "            \n",
    "            # Record episode results\n",
    "            episoderewards.append(reward)\n",
    "            episodebalancetimes.append(steps)\n",
    "            \n",
    "            # Create metrics dictionary\n",
    "            metrics = {\n",
    "                'currentEpisode': episode,\n",
    "                'recentRewards': episoderewards[-100:] if len(episoderewards) > 100 else episoderewards,\n",
    "                'averageBalanceTime': np.mean(episodebalancetimes[-100:]) if episodebalancetimes else 0,\n",
    "                'balanceTimeVariance': np.var(episodebalancetimes[-100:]) if len(episodebalancetimes) > 1 else 0\n",
    "            }\n",
    "            \n",
    "            # Debug print for metrics\n",
    "            if episode % 1000 == 0:\n",
    "                print(f\"\\nMetrics Debug at Episode {episode}:\")\n",
    "                print(f\"Recent Average Reward: {np.mean(metrics['recentRewards']):.2f}\")\n",
    "                print(f\"Average Balance Time: {metrics['averageBalanceTime']:.2f}\")\n",
    "                print(f\"Balance Time Variance: {metrics['balanceTimeVariance']:.2f}\")\n",
    "            \n",
    "            # Handle different update methods\n",
    "            if rewardinfo['update_method'] == 'llm':\n",
    "                # Debug print for LLM update check\n",
    "                if episode % 1000 == 0:\n",
    "                    print(f\"Last Update Episode: {updatesystem.lastUpdateEpisode}\")\n",
    "                    print(f\"Time Since Last Update: {episode - updatesystem.lastUpdateEpisode}\")\n",
    "                \n",
    "                if updatesystem.waitingTime('stability', metrics, updatesystem.lastUpdateEpisode):\n",
    "                    print(f\"\\nAttempting LLM update at episode {episode}\")\n",
    "                    updateCompositeRewardFunction(env, updatesystem, metrics, dynamicRewardFunction)\n",
    "                    \n",
    "                    if hasattr(env.rewardFunction, 'compositeHistory'):\n",
    "                        latest_updates = [update['episode'] for update in env.rewardFunction.compositeHistory \n",
    "                                        if update['episode'] == episode]\n",
    "                        if latest_updates:\n",
    "                            rewardchangeepisodes.append(episode)\n",
    "                            updatesystem.lastUpdateEpisode = episode\n",
    "                            print(f\"✓ LLM update successful at episode {episode}\")\n",
    "                            print(f\"Current reward changes: {rewardchangeepisodes}\")\n",
    "            \n",
    "            # In onEpisodeEnd function:\n",
    "            if rewardinfo['update_method'] == 'meta':\n",
    "                # Update every 1000 episodes instead of at environment change\n",
    "                if episode % 1000 == 0 and episode > 0:\n",
    "                    print(f\"\\nAttempting meta-learning update at episode {episode}\")\n",
    "                    recent_memories = list(agent.memory)[-100:] if len(agent.memory) >= 100 else list(agent.memory)\n",
    "                    trajectory = {\n",
    "                        'states': recent_memories,\n",
    "                        'episode_length': steps,\n",
    "                        'total_reward': reward,\n",
    "                        'env_params': {\n",
    "                            'length': env.env.length,\n",
    "                            'mass_cart': env.env.masscart\n",
    "                        }\n",
    "                    }\n",
    "                    meta_loss = updatesystem.meta_update([trajectory])\n",
    "                    env.setRewardFunction(updatesystem.generate_reward_function())\n",
    "                    rewardchangeepisodes.append(episode)\n",
    "                    print(f\"✓ Meta-learning update completed, loss: {meta_loss}\")\n",
    "            \n",
    "            elif rewardinfo['update_method'] == 'physics':\n",
    "                if episode % changeInterval == 0 and episode > 0:\n",
    "                    print(f\"\\nUpdating physics-based reward at episode {episode}\")\n",
    "                    updatesystem.length = lengthchanges[currentlengthidx]\n",
    "                    env.setRewardFunction(updatesystem.compute_reward)\n",
    "                    rewardchangeepisodes.append(episode)\n",
    "                    print(\"✓ Physics-based update completed\")\n",
    "            \n",
    "            # Environment changes\n",
    "            if episode % changeInterval == 0 and episode > 0:\n",
    "                currentlengthidx = (currentlengthidx + 1) % len(lengthchanges)\n",
    "                newlength = lengthchanges[currentlengthidx]\n",
    "                env.setEnvironmentParameters(length=newlength)\n",
    "                print(f\"\\nChanged pole length to: {newlength}m at episode {episode}\")\n",
    "            \n",
    "            # Regular progress updates\n",
    "            if episode % 10000 == 0:\n",
    "                print(f\"\\nEpisode {episode}\")\n",
    "                print(f\"  Steps: {steps}\")\n",
    "                print(f\"  Total Reward: {reward}\")\n",
    "                print(f\"  Average Balance Time = {np.mean(episodebalancetimes[-50:]):.2f}\")\n",
    "                print(f\"  Average Reward = {np.mean(episoderewards[-50:]):.2f}\")\n",
    "        \n",
    "        agent, env, rewards = trainDQLearning(\n",
    "            agent=rewardinfo['agent'],\n",
    "            env=env,\n",
    "            numEpisodes=episodes,\n",
    "            updateSystem=rewardinfo['updatesystem'],\n",
    "            onEpisodeEnd=onEpisodeEnd\n",
    "        )\n",
    "        \n",
    "        results[rewardname] = {\n",
    "            'rewards': episoderewards,\n",
    "            'balancetimes': episodebalancetimes,\n",
    "            'rewardChanges': rewardchangeepisodes\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nCompleted testing {rewardname}\")\n",
    "        print(f\"Final average reward: {np.mean(episoderewards[-100:]):.2f}\")\n",
    "        print(f\"Final average balance time: {np.mean(episodebalancetimes[-100:]):.2f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Experiment\n",
    "\n",
    "changeInterval = 20000\n",
    "\n",
    "# results = runPerformanceComparisonTest(\n",
    "#     episodes=40000,  \n",
    "#     changeInterval=changeInterval,\n",
    "#     mass_cart=1.0,\n",
    "#     lengthchanges=[0.3, 0.9]  \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizePerformanceComparison(results, changeInterval, folder_path):\n",
    "    \"\"\"\n",
    "    Create and save performance comparison visualizations\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 15))\n",
    "    \n",
    "    # Color map for different reward functions\n",
    "    colors = ['b', 'g', 'r', 'c', 'm']\n",
    "    \n",
    "    # Plot rewards for each reward function with variance\n",
    "    for idx, (rewardname, rewardresults) in enumerate(results.items()):\n",
    "        rewards = pd.Series(rewardresults['rewards'])\n",
    "        \n",
    "        # Calculate rolling mean and standard deviation for rewards\n",
    "        window = 50\n",
    "        rolling_mean_rewards = rewards.rolling(window=window).mean()\n",
    "        rolling_std_rewards = rewards.rolling(window=window).std()\n",
    "        \n",
    "        # Plot mean line for rewards\n",
    "        ax1.plot(rolling_mean_rewards, \n",
    "                label=f'{rewardname}', \n",
    "                linewidth=2, \n",
    "                color=colors[idx])\n",
    "        \n",
    "        # Plot variance area for rewards\n",
    "        ax1.fill_between(\n",
    "            range(len(rewards)),\n",
    "            rolling_mean_rewards - rolling_std_rewards,\n",
    "            rolling_mean_rewards + rolling_std_rewards,\n",
    "            color=colors[idx],\n",
    "            alpha=0.2\n",
    "        )\n",
    "        \n",
    "        # Add vertical lines for environment changes (red)\n",
    "        change_episodes = range(changeInterval, len(rewards), changeInterval)\n",
    "        for ep in change_episodes:\n",
    "            ax1.axvline(x=ep, color='r', linestyle='--', alpha=0.3,\n",
    "                       label='Environment Change' if ep == change_episodes[0] else None)\n",
    "        \n",
    "        # Add vertical lines for reward function changes (green)\n",
    "        if 'rewardChanges' in rewardresults:\n",
    "            for ep in rewardresults['rewardChanges']:\n",
    "                ax1.axvline(x=ep, color='g', linestyle='--', alpha=0.3,\n",
    "                          label=f'{rewardname} Update' if ep == rewardresults['rewardChanges'][0] else None)\n",
    "    \n",
    "    ax1.set_title('Average Reward Over Time with Variance')\n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.set_ylabel('Reward')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot balance times\n",
    "    for idx, (rewardname, rewardresults) in enumerate(results.items()):\n",
    "        balancetimes = pd.Series(rewardresults['balancetimes'])\n",
    "        \n",
    "        rolling_mean_balance = balancetimes.rolling(window=window).mean()\n",
    "        rolling_std_balance = balancetimes.rolling(window=window).std()\n",
    "        \n",
    "        ax2.plot(rolling_mean_balance,\n",
    "                label=f'{rewardname}', \n",
    "                linewidth=2, \n",
    "                color=colors[idx])\n",
    "        \n",
    "        ax2.fill_between(\n",
    "            range(len(balancetimes)),\n",
    "            rolling_mean_balance - rolling_std_balance,\n",
    "            rolling_mean_balance + rolling_std_balance,\n",
    "            color=colors[idx],\n",
    "            alpha=0.2\n",
    "        )\n",
    "        \n",
    "        # Add vertical lines for environment changes\n",
    "        for ep in change_episodes:\n",
    "            ax2.axvline(x=ep, color='r', linestyle='--', alpha=0.3,\n",
    "                       label='Environment Change' if ep == change_episodes[0] else None)\n",
    "        \n",
    "        # Add vertical lines for reward function changes\n",
    "        if 'rewardChanges' in rewardresults:\n",
    "            for ep in rewardresults['rewardChanges']:\n",
    "                ax2.axvline(x=ep, color='g', linestyle='--', alpha=0.3,\n",
    "                          label=f'{rewardname} Update' if ep == rewardresults['rewardChanges'][0] else None)\n",
    "    \n",
    "    ax2.set_title('Average Balance Time Over Episodes with Variance')\n",
    "    ax2.set_xlabel('Episode')\n",
    "    ax2.set_ylabel('Steps')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    # Plot environment parameters\n",
    "    env_param_history = []\n",
    "    for episode in range(len(next(iter(results.values()))['rewards'])):\n",
    "        idx = (episode // changeInterval) % 2\n",
    "        length = 0.3 if idx == 0 else 0.9  # Alternating between 0.3 and 0.9\n",
    "        env_param_history.append(length)\n",
    "    \n",
    "    ax3.plot(env_param_history, label='Pole Length', color='purple')\n",
    "    ax3.set_title('Environment Parameters Over Episodes')\n",
    "    ax3.set_xlabel('Episode')\n",
    "    ax3.set_ylabel('Pole Length (m)')\n",
    "    ax3.grid(True)\n",
    "    ax3.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plots\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filepath = os.path.join(folder_path, f\"performance_comparison_{timestamp}.png\")\n",
    "    plt.savefig(filepath, bbox_inches='tight', dpi=300)\n",
    "    print(f\"Saved plot: performance_comparison_{timestamp}.png in {folder_path}\")\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "    \n",
    "def calculateStability(rewards):\n",
    "    \"\"\"\n",
    "    Calculate stability score based on reward variance in the last 100 episodes\n",
    "    Lower variance = higher stability\n",
    "    \"\"\"\n",
    "    if len(rewards) < 100:\n",
    "        return 0.0\n",
    "    \n",
    "    last_hundred = rewards[-100:]\n",
    "    mean_reward = np.mean(last_hundred)\n",
    "    if mean_reward == 0:\n",
    "        return 0.0\n",
    "        \n",
    "    # Calculate coefficient of variation (normalized standard deviation)\n",
    "    stability = 1 - (np.std(last_hundred) / mean_reward)\n",
    "    return max(0, min(1, stability))  # Normalize between 0 and 1\n",
    "\n",
    "def calculateConvergenceTime(rewards, threshold=195, window=50):\n",
    "    \"\"\"\n",
    "    Calculate the number of episodes needed to reach and maintain a certain performance\n",
    "    threshold for a given window of episodes\n",
    "    \"\"\"\n",
    "    if len(rewards) < window:\n",
    "        return len(rewards)\n",
    "    \n",
    "    rolling_mean = pd.Series(rewards).rolling(window).mean()\n",
    "    \n",
    "    for episode in range(window, len(rewards)):\n",
    "        if rolling_mean[episode] >= threshold:\n",
    "            # Check if performance is maintained\n",
    "            maintained = all(avg >= threshold * 0.9 for avg in rolling_mean[episode:episode+window])\n",
    "            if maintained:\n",
    "                return episode\n",
    "    \n",
    "    return len(rewards)  # If never converged, return total episodes\n",
    "\n",
    "def calculatePerformanceMetrics(results):\n",
    "    metrics = {}\n",
    "    for rewardname, rewardresults in results.items():\n",
    "        metrics[rewardname] = {\n",
    "            'finalavgreward': np.mean(rewardresults['rewards'][-100:]),\n",
    "            'finalavgbalance': np.mean(rewardresults['balancetimes'][-100:]),\n",
    "            'convergencetime': calculateConvergenceTime(rewardresults['rewards']),\n",
    "            'stability': calculateStability(rewardresults['rewards'])\n",
    "        }\n",
    "    return pd.DataFrame(metrics).T\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def saveMetricsTable(metrics, filename, folder_path):\n",
    "    \"\"\"Save metrics table to specified folder\"\"\"\n",
    "    # Create figure for metrics table\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Create table with formatted metrics\n",
    "    table = ax.table(\n",
    "        cellText=metrics.values.round(3),\n",
    "        colLabels=metrics.columns,\n",
    "        rowLabels=metrics.index,\n",
    "        cellLoc='center',\n",
    "        loc='center'\n",
    "    )\n",
    "    \n",
    "    # Adjust font size and scaling\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(9)\n",
    "    table.scale(1.2, 1.5)\n",
    "    \n",
    "    plt.title(\"Performance Metrics Comparison\")\n",
    "    \n",
    "    # Save with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filepath = os.path.join(folder_path, f\"{filename}_{timestamp}.png\")\n",
    "    plt.savefig(filepath, bbox_inches='tight', dpi=300)\n",
    "    print(f\"Saved metrics table: {filename}_{timestamp}.png in {folder_path}\")\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize the results\n",
    "# visualizePerformanceComparison(results,changeInterval)\n",
    "\n",
    "\n",
    "# # Calculate and display the metrics\n",
    "# metrics = calculatePerformanceMetrics(results)\n",
    "# print(\"\\nPerformance Metrics:\")\n",
    "# print(metrics)\n",
    "    \n",
    "# Call the function\n",
    "# saveMetricsTable(metrics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple runs to generate confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createExperimentFolder():\n",
    "    \"\"\"Create a timestamped folder for experiment results\"\"\"\n",
    "    from datetime import datetime\n",
    "    import os\n",
    "    \n",
    "    # Create base experiment folder if it doesn't exist\n",
    "    if not os.path.exists(\"PerformanceExperiment\"):\n",
    "        os.makedirs(\"PerformanceExperiment\")\n",
    "    \n",
    "    # Create timestamped subfolder\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    experiment_folder = os.path.join(\"PerformanceExperiment\", f\"experiment_{timestamp}\")\n",
    "    os.makedirs(experiment_folder)\n",
    "    \n",
    "    return experiment_folder\n",
    "\n",
    "def savePlot(fig, filename, folder_path):\n",
    "    \"\"\"Save plot to specified folder with timestamp\"\"\"\n",
    "    from datetime import datetime\n",
    "    import os\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filepath = os.path.join(folder_path, f\"{filename}_{timestamp}.png\")\n",
    "    fig.savefig(filepath, bbox_inches='tight', dpi=300)\n",
    "    print(f\"Saved plot: {filename}_{timestamp}.png in {folder_path}\")\n",
    "    \n",
    "def saveMetricsTable(metrics, filename, folder_path):\n",
    "    \"\"\"Save metrics table to specified folder\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    from datetime import datetime\n",
    "    import os\n",
    "    \n",
    "    # Style the DataFrame for visualization\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    table = ax.table(cellText=metrics.values,\n",
    "                    colLabels=metrics.columns,\n",
    "                    rowLabels=metrics.index,\n",
    "                    cellLoc='center',\n",
    "                    loc='center')\n",
    "    \n",
    "    # Adjust font size and scaling\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(9)\n",
    "    table.scale(1.2, 1.5)\n",
    "    \n",
    "    plt.title(\"Performance Metrics Comparison\")\n",
    "    \n",
    "    # Save with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filepath = os.path.join(folder_path, f\"{filename}_{timestamp}.png\")\n",
    "    plt.savefig(filepath, bbox_inches='tight', dpi=300)\n",
    "    print(f\"Saved metrics table: {filename}_{timestamp}.png in {folder_path}\")\n",
    "    plt.close()\n",
    "\n",
    "def runMultipleExperiments(numRuns=4, episodes=40000, changeInterval=20000):\n",
    "    \"\"\"\n",
    "    Run multiple experiments and save results in organized folders\n",
    "    \"\"\"\n",
    "    # Create main experiment folder\n",
    "    experiment_folder = createExperimentFolder()\n",
    "    \n",
    "    allResults = []\n",
    "    allMetrics = []\n",
    "    aggregatedMetrics = {\n",
    "        'adaptivereward': [],\n",
    "        'meta_learning': [],\n",
    "        'energy_based': []\n",
    "    }\n",
    "    \n",
    "    # Create run folders\n",
    "    for run in range(numRuns):\n",
    "        print(f\"\\nStarting Run {run + 1}/{numRuns}\")\n",
    "        \n",
    "        # Create folder for this run\n",
    "        run_folder = os.path.join(experiment_folder, f\"run_{run + 1}\")\n",
    "        os.makedirs(run_folder)\n",
    "        \n",
    "        # Run experiment\n",
    "        results = runPerformanceComparisonTest(\n",
    "            episodes=episodes,\n",
    "            changeInterval=changeInterval,\n",
    "            mass_cart=1.0,\n",
    "            lengthchanges=[0.3, 0.9]\n",
    "        )\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = calculatePerformanceMetrics(results)\n",
    "        \n",
    "        # Store results\n",
    "        allResults.append(results)\n",
    "        allMetrics.append(metrics)\n",
    "        \n",
    "        # Store metrics by reward type\n",
    "        for rewardType in results.keys():\n",
    "            if rewardType not in aggregatedMetrics:\n",
    "                aggregatedMetrics[rewardType] = []\n",
    "            aggregatedMetrics[rewardType].append(metrics.loc[rewardType])\n",
    "        \n",
    "        # Visualize and save individual run results\n",
    "        visualizePerformanceComparison(results, changeInterval, run_folder)\n",
    "        saveMetricsTable(metrics, f\"metrics_run_{run + 1}\", run_folder)\n",
    "    \n",
    "    # Calculate confidence intervals (95%)\n",
    "    confidenceIntervals = {}\n",
    "    \n",
    "    for rewardType, metrics_list in aggregatedMetrics.items():\n",
    "        metrics_df = pd.DataFrame(metrics_list)\n",
    "        confidenceIntervals[rewardType] = {}\n",
    "        \n",
    "        for column in metrics_df.columns:\n",
    "            mean = metrics_df[column].mean()\n",
    "            ci = 1.96 * metrics_df[column].std() / np.sqrt(numRuns)\n",
    "            \n",
    "            confidenceIntervals[rewardType][column] = {\n",
    "                'mean': mean,\n",
    "                'ci': ci\n",
    "            }\n",
    "    \n",
    "    # Create final results table\n",
    "    resultsTable = pd.DataFrame({\n",
    "        rewardType: {\n",
    "            metric: f\"{data[metric]['mean']:.2f} ± {data[metric]['ci']:.2f}\"\n",
    "            for metric in ['finalavgreward', 'finalavgbalance', 'convergencetime', 'stability']\n",
    "        }\n",
    "        for rewardType, data in confidenceIntervals.items()\n",
    "    }).T\n",
    "    \n",
    "    # Save final results in main experiment folder\n",
    "    saveMetricsTable(resultsTable, \"final_results\", experiment_folder)\n",
    "    \n",
    "    # Save aggregate statistics\n",
    "    with open(os.path.join(experiment_folder, \"aggregate_statistics.txt\"), \"w\") as f:\n",
    "        f.write(\"Aggregate Statistics:\\n\")\n",
    "        for rewardType, metrics in confidenceIntervals.items():\n",
    "            f.write(f\"\\n{rewardType}:\\n\")\n",
    "            for metric, values in metrics.items():\n",
    "                f.write(f\"{metric}: {values['mean']:.2f} ± {values['ci']:.2f}\\n\")\n",
    "    \n",
    "    print(f\"\\nExperiment results saved in: {experiment_folder}\")\n",
    "    return confidenceIntervals, allResults, resultsTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Run 1/4\n",
      "Starting Performance Comparison Test...\n",
      "Environment parameters updated: masscart=1.0, length=0.3, gravity=9.8\n",
      "\n",
      "Testing reward function: adaptivereward\n",
      "\n",
      "Metrics Debug at Episode 0:\n",
      "Recent Average Reward: 13.92\n",
      "Average Balance Time: 14.00\n",
      "Balance Time Variance: 0.00\n",
      "Last Update Episode: 0\n",
      "Time Since Last Update: 0\n",
      "\n",
      "Checking for update at episode 0, time since last update: 0\n",
      "\n",
      "Episode 0\n",
      "  Steps: 14\n",
      "  Total Reward: 13.916301727294922\n",
      "  Average Balance Time = 14.00\n",
      "  Average Reward = 13.92\n",
      "\n",
      "Debug Metrics at episode 1000:\n",
      "Recent average reward: 155.15\n",
      "Recent average steps: 155.90\n",
      "Last update episode: 0\n",
      "\n",
      "Metrics Debug at Episode 1000:\n",
      "Recent Average Reward: 155.74\n",
      "Average Balance Time: 156.49\n",
      "Balance Time Variance: 11116.01\n",
      "Last Update Episode: 0\n",
      "Time Since Last Update: 1000\n",
      "\n",
      "Debug Metrics at episode 2000:\n",
      "Recent average reward: 103.74\n",
      "Recent average steps: 104.26\n",
      "Last update episode: 0\n",
      "\n",
      "Metrics Debug at Episode 2000:\n",
      "Recent Average Reward: 103.70\n",
      "Average Balance Time: 104.22\n",
      "Balance Time Variance: 5060.19\n",
      "Last Update Episode: 0\n",
      "Time Since Last Update: 2000\n",
      "\n",
      "Checking for update at episode 2000, time since last update: 2000\n",
      "\n",
      "Debug Metrics at episode 3000:\n",
      "Recent average reward: 207.65\n",
      "Recent average steps: 208.14\n",
      "Last update episode: 0\n",
      "\n",
      "Metrics Debug at Episode 3000:\n",
      "Recent Average Reward: 208.06\n",
      "Average Balance Time: 208.55\n",
      "Balance Time Variance: 15459.89\n",
      "Last Update Episode: 0\n",
      "Time Since Last Update: 3000\n",
      "\n",
      "Debug Metrics at episode 4000:\n",
      "Recent average reward: 176.84\n",
      "Recent average steps: 177.20\n",
      "Last update episode: 0\n",
      "\n",
      "Metrics Debug at Episode 4000:\n",
      "Recent Average Reward: 175.97\n",
      "Average Balance Time: 176.33\n",
      "Balance Time Variance: 9532.18\n",
      "Last Update Episode: 0\n",
      "Time Since Last Update: 4000\n",
      "\n",
      "Checking for update at episode 4000, time since last update: 4000\n",
      "\n",
      "Debug Metrics at episode 5000:\n",
      "Recent average reward: 163.13\n",
      "Recent average steps: 163.30\n",
      "Last update episode: 0\n",
      "\n",
      "Metrics Debug at Episode 5000:\n",
      "Recent Average Reward: 166.03\n",
      "Average Balance Time: 166.20\n",
      "Balance Time Variance: 14546.16\n",
      "Last Update Episode: 0\n",
      "Time Since Last Update: 5000\n",
      "\n",
      "Checking for update at episode 5000, time since last update: 5000\n",
      "✓ Update triggered at episode 5000\n",
      "\n",
      "Attempting LLM update at episode 5000\n",
      "\n",
      "Checking for update at episode 5000, time since last update: 5000\n",
      "✓ Update triggered at episode 5000\n",
      "\n",
      "Attempting to update composite component: stability\n",
      "Updated stability weight: 0.330 -> 0.330\n",
      "\n",
      "Checking for update at episode 5000, time since last update: 5000\n",
      "✓ Update triggered at episode 5000\n",
      "\n",
      "Attempting to update composite component: efficiency\n",
      "Updated efficiency weight: 0.330 -> 0.330\n",
      "\n",
      "Checking for update at episode 5000, time since last update: 5000\n",
      "✓ Update triggered at episode 5000\n",
      "\n",
      "Attempting to update composite component: time\n",
      "Updated time weight: 0.340 -> 0.400\n",
      "Environment parameters updated: masscart=1.0, length=0.9, gravity=9.8\n",
      "\n",
      "Changed pole length to: 0.9m at episode 5000\n",
      "\n",
      "Debug Metrics at episode 6000:\n",
      "Recent average reward: 850.02\n",
      "Recent average steps: 850.08\n",
      "Last update episode: 0\n",
      "\n",
      "Metrics Debug at Episode 6000:\n",
      "Recent Average Reward: 842.80\n",
      "Average Balance Time: 842.86\n",
      "Balance Time Variance: 952604.42\n",
      "Last Update Episode: 0\n",
      "Time Since Last Update: 6000\n",
      "\n",
      "Checking for update at episode 6000, time since last update: 6000\n",
      "\n",
      "Debug Metrics at episode 7000:\n",
      "Recent average reward: 13.26\n",
      "Recent average steps: 13.36\n",
      "Last update episode: 0\n",
      "\n",
      "Metrics Debug at Episode 7000:\n",
      "Recent Average Reward: 13.33\n",
      "Average Balance Time: 13.43\n",
      "Balance Time Variance: 4.63\n",
      "Last Update Episode: 0\n",
      "Time Since Last Update: 7000\n",
      "\n",
      "Debug Metrics at episode 8000:\n",
      "Recent average reward: 37.56\n",
      "Recent average steps: 37.90\n",
      "Last update episode: 0\n",
      "\n",
      "Metrics Debug at Episode 8000:\n",
      "Recent Average Reward: 37.40\n",
      "Average Balance Time: 37.74\n",
      "Balance Time Variance: 90.19\n",
      "Last Update Episode: 0\n",
      "Time Since Last Update: 8000\n",
      "\n",
      "Checking for update at episode 8000, time since last update: 8000\n",
      "\n",
      "Debug Metrics at episode 9000:\n",
      "Recent average reward: 329.87\n",
      "Recent average steps: 330.17\n",
      "Last update episode: 0\n",
      "\n",
      "Metrics Debug at Episode 9000:\n",
      "Recent Average Reward: 331.59\n",
      "Average Balance Time: 331.89\n",
      "Balance Time Variance: 58632.56\n",
      "Last Update Episode: 0\n",
      "Time Since Last Update: 9000\n",
      "\n",
      "Debug Metrics at episode 10000:\n",
      "Recent average reward: 107.50\n",
      "Recent average steps: 108.08\n",
      "Last update episode: 0\n",
      "\n",
      "Completed testing adaptivereward\n",
      "Final average reward: 107.50\n",
      "Final average balance time: 108.08\n",
      "\n",
      "Testing reward function: meta_learning\n",
      "\n",
      "Metrics Debug at Episode 0:\n",
      "Recent Average Reward: -50.26\n",
      "Average Balance Time: 14.00\n",
      "Balance Time Variance: 0.00\n",
      "\n",
      "Episode 0\n",
      "  Steps: 14\n",
      "  Total Reward: -50.25977938249707\n",
      "  Average Balance Time = 14.00\n",
      "  Average Reward = -50.26\n",
      "\n",
      "Metrics Debug at Episode 1000:\n",
      "Recent Average Reward: -18.21\n",
      "Average Balance Time: 70.54\n",
      "Balance Time Variance: 2436.45\n",
      "\n",
      "Metrics Debug at Episode 2000:\n",
      "Recent Average Reward: -21.08\n",
      "Average Balance Time: 94.52\n",
      "Balance Time Variance: 9384.97\n",
      "\n",
      "Metrics Debug at Episode 3000:\n",
      "Recent Average Reward: -23.09\n",
      "Average Balance Time: 108.85\n",
      "Balance Time Variance: 15294.43\n",
      "\n",
      "Metrics Debug at Episode 4000:\n",
      "Recent Average Reward: -20.26\n",
      "Average Balance Time: 79.81\n",
      "Balance Time Variance: 8642.21\n",
      "\n",
      "Metrics Debug at Episode 5000:\n",
      "Recent Average Reward: -22.83\n",
      "Average Balance Time: 92.14\n",
      "Balance Time Variance: 7306.68\n",
      "Environment parameters updated: masscart=1.0, length=0.3, gravity=9.8\n",
      "\n",
      "Changed pole length to: 0.3m at episode 5000\n",
      "\n",
      "Metrics Debug at Episode 6000:\n",
      "Recent Average Reward: -30.24\n",
      "Average Balance Time: 133.70\n",
      "Balance Time Variance: 32245.63\n",
      "\n",
      "Metrics Debug at Episode 7000:\n",
      "Recent Average Reward: -17.51\n",
      "Average Balance Time: 69.26\n",
      "Balance Time Variance: 3084.07\n",
      "\n",
      "Metrics Debug at Episode 8000:\n",
      "Recent Average Reward: -24.23\n",
      "Average Balance Time: 88.33\n",
      "Balance Time Variance: 9491.96\n",
      "\n",
      "Metrics Debug at Episode 9000:\n",
      "Recent Average Reward: -18.41\n",
      "Average Balance Time: 70.21\n",
      "Balance Time Variance: 2671.47\n",
      "\n",
      "Completed testing meta_learning\n",
      "Final average reward: -17.37\n",
      "Final average balance time: 64.67\n",
      "\n",
      "Testing reward function: energy_based\n",
      "\n",
      "Metrics Debug at Episode 0:\n",
      "Recent Average Reward: -13.33\n",
      "Average Balance Time: 24.00\n",
      "Balance Time Variance: 0.00\n",
      "\n",
      "Episode 0\n",
      "  Steps: 24\n",
      "  Total Reward: -13.32966423034668\n",
      "  Average Balance Time = 24.00\n",
      "  Average Reward = -13.33\n",
      "\n",
      "Metrics Debug at Episode 1000:\n",
      "Recent Average Reward: 1.80\n",
      "Average Balance Time: 70.33\n",
      "Balance Time Variance: 3808.36\n",
      "\n",
      "Metrics Debug at Episode 2000:\n",
      "Recent Average Reward: 378.85\n",
      "Average Balance Time: 2000.00\n",
      "Balance Time Variance: 0.00\n",
      "\n",
      "Metrics Debug at Episode 3000:\n",
      "Recent Average Reward: 167.69\n",
      "Average Balance Time: 403.79\n",
      "Balance Time Variance: 25294.61\n",
      "\n",
      "Metrics Debug at Episode 4000:\n",
      "Recent Average Reward: 2952.97\n",
      "Average Balance Time: 1974.22\n",
      "Balance Time Variance: 34678.31\n",
      "\n",
      "Metrics Debug at Episode 5000:\n",
      "Recent Average Reward: 101.79\n",
      "Average Balance Time: 175.14\n",
      "Balance Time Variance: 93.66\n",
      "\n",
      "Updating physics-based reward at episode 5000\n",
      "✓ Physics-based update completed\n",
      "Environment parameters updated: masscart=1.0, length=0.9, gravity=9.8\n",
      "\n",
      "Changed pole length to: 0.9m at episode 5000\n",
      "\n",
      "Metrics Debug at Episode 6000:\n",
      "Recent Average Reward: 2866.53\n",
      "Average Balance Time: 1974.94\n",
      "Balance Time Variance: 21829.46\n",
      "\n",
      "Metrics Debug at Episode 7000:\n",
      "Recent Average Reward: 3349.59\n",
      "Average Balance Time: 1906.02\n",
      "Balance Time Variance: 138445.78\n",
      "\n",
      "Metrics Debug at Episode 8000:\n",
      "Recent Average Reward: 2552.89\n",
      "Average Balance Time: 1602.63\n",
      "Balance Time Variance: 353022.61\n",
      "\n",
      "Metrics Debug at Episode 9000:\n",
      "Recent Average Reward: 3426.88\n",
      "Average Balance Time: 1969.76\n",
      "Balance Time Variance: 44850.74\n",
      "\n",
      "Completed testing energy_based\n",
      "Final average reward: -425.42\n",
      "Final average balance time: 1164.94\n",
      "\n",
      "Testing reward function: baseline\n",
      "\n",
      "Metrics Debug at Episode 0:\n",
      "Recent Average Reward: 13.00\n",
      "Average Balance Time: 13.00\n",
      "Balance Time Variance: 0.00\n",
      "\n",
      "Episode 0\n",
      "  Steps: 13\n",
      "  Total Reward: 13.0\n",
      "  Average Balance Time = 13.00\n",
      "  Average Reward = 13.00\n",
      "\n",
      "Metrics Debug at Episode 1000:\n",
      "Recent Average Reward: 44.65\n",
      "Average Balance Time: 44.65\n",
      "Balance Time Variance: 661.87\n",
      "\n",
      "Metrics Debug at Episode 2000:\n",
      "Recent Average Reward: 141.49\n",
      "Average Balance Time: 141.49\n",
      "Balance Time Variance: 6992.73\n",
      "\n",
      "Metrics Debug at Episode 3000:\n",
      "Recent Average Reward: 322.50\n",
      "Average Balance Time: 322.50\n",
      "Balance Time Variance: 35870.73\n",
      "\n",
      "Metrics Debug at Episode 4000:\n",
      "Recent Average Reward: 296.21\n",
      "Average Balance Time: 296.21\n",
      "Balance Time Variance: 15844.65\n",
      "\n",
      "Metrics Debug at Episode 5000:\n",
      "Recent Average Reward: 109.85\n",
      "Average Balance Time: 109.85\n",
      "Balance Time Variance: 13548.13\n",
      "Environment parameters updated: masscart=1.0, length=0.3, gravity=9.8\n",
      "\n",
      "Changed pole length to: 0.3m at episode 5000\n",
      "\n",
      "Metrics Debug at Episode 6000:\n",
      "Recent Average Reward: 132.75\n",
      "Average Balance Time: 132.75\n",
      "Balance Time Variance: 15366.97\n",
      "\n",
      "Metrics Debug at Episode 7000:\n",
      "Recent Average Reward: 72.85\n",
      "Average Balance Time: 72.85\n",
      "Balance Time Variance: 768.87\n",
      "\n",
      "Metrics Debug at Episode 8000:\n",
      "Recent Average Reward: 167.77\n",
      "Average Balance Time: 167.77\n",
      "Balance Time Variance: 5303.40\n",
      "\n",
      "Metrics Debug at Episode 9000:\n",
      "Recent Average Reward: 86.37\n",
      "Average Balance Time: 86.37\n",
      "Balance Time Variance: 1008.83\n",
      "\n",
      "Completed testing baseline\n",
      "Final average reward: 137.21\n",
      "Final average balance time: 137.21\n",
      "Saved plot: performance_comparison_20250208_104538.png in PerformanceExperiment/experiment_20250208_101230/run_1\n",
      "Saved metrics table: metrics_run_1_20250208_104539.png in PerformanceExperiment/experiment_20250208_101230/run_1\n",
      "\n",
      "Starting Run 2/4\n",
      "Starting Performance Comparison Test...\n",
      "Environment parameters updated: masscart=1.0, length=0.3, gravity=9.8\n",
      "\n",
      "Testing reward function: adaptivereward\n",
      "\n",
      "Metrics Debug at Episode 0:\n",
      "Recent Average Reward: 12.89\n",
      "Average Balance Time: 13.00\n",
      "Balance Time Variance: 0.00\n",
      "Last Update Episode: 0\n",
      "Time Since Last Update: 0\n",
      "\n",
      "Checking for update at episode 0, time since last update: 0\n",
      "\n",
      "Episode 0\n",
      "  Steps: 13\n",
      "  Total Reward: 12.891340255737305\n",
      "  Average Balance Time = 13.00\n",
      "  Average Reward = 12.89\n",
      "\n",
      "Debug Metrics at episode 1000:\n",
      "Recent average reward: 270.33\n",
      "Recent average steps: 270.42\n",
      "Last update episode: 0\n",
      "\n",
      "Metrics Debug at Episode 1000:\n",
      "Recent Average Reward: 270.00\n",
      "Average Balance Time: 270.09\n",
      "Balance Time Variance: 35971.62\n",
      "Last Update Episode: 0\n",
      "Time Since Last Update: 1000\n",
      "\n",
      "Debug Metrics at episode 2000:\n",
      "Recent average reward: 503.45\n",
      "Recent average steps: 503.89\n",
      "Last update episode: 0\n",
      "\n",
      "Metrics Debug at Episode 2000:\n",
      "Recent Average Reward: 507.14\n",
      "Average Balance Time: 507.58\n",
      "Balance Time Variance: 399278.90\n",
      "Last Update Episode: 0\n",
      "Time Since Last Update: 2000\n",
      "\n",
      "Checking for update at episode 2000, time since last update: 2000\n",
      "\n",
      "Debug Metrics at episode 3000:\n",
      "Recent average reward: 1217.61\n",
      "Recent average steps: 1219.14\n",
      "Last update episode: 0\n",
      "\n",
      "Metrics Debug at Episode 3000:\n",
      "Recent Average Reward: 1235.48\n",
      "Average Balance Time: 1237.02\n",
      "Balance Time Variance: 734223.64\n",
      "Last Update Episode: 0\n",
      "Time Since Last Update: 3000\n",
      "\n",
      "Debug Metrics at episode 4000:\n",
      "Recent average reward: 44.76\n",
      "Recent average steps: 44.95\n",
      "Last update episode: 0\n",
      "\n",
      "Metrics Debug at Episode 4000:\n",
      "Recent Average Reward: 44.82\n",
      "Average Balance Time: 45.01\n",
      "Balance Time Variance: 17447.87\n",
      "Last Update Episode: 0\n",
      "Time Since Last Update: 4000\n",
      "\n",
      "Checking for update at episode 4000, time since last update: 4000\n",
      "\n",
      "Debug Metrics at episode 5000:\n",
      "Recent average reward: 254.56\n",
      "Recent average steps: 254.76\n",
      "Last update episode: 0\n",
      "\n",
      "Metrics Debug at Episode 5000:\n",
      "Recent Average Reward: 254.53\n",
      "Average Balance Time: 254.73\n",
      "Balance Time Variance: 24652.56\n",
      "Last Update Episode: 0\n",
      "Time Since Last Update: 5000\n",
      "\n",
      "Checking for update at episode 5000, time since last update: 5000\n",
      "✓ Update triggered at episode 5000\n",
      "\n",
      "Attempting LLM update at episode 5000\n",
      "\n",
      "Checking for update at episode 5000, time since last update: 0\n",
      "✓ Update triggered at episode 5000\n",
      "\n",
      "Attempting to update composite component: stability\n",
      "Updated stability weight: 0.300 -> 0.300\n",
      "\n",
      "Checking for update at episode 5000, time since last update: 0\n",
      "✓ Update triggered at episode 5000\n",
      "\n",
      "Attempting to update composite component: efficiency\n",
      "Updated efficiency weight: 0.300 -> 0.300\n",
      "\n",
      "Checking for update at episode 5000, time since last update: 0\n",
      "✓ Update triggered at episode 5000\n",
      "\n",
      "Attempting to update composite component: time\n",
      "Updated time weight: 0.400 -> 0.455\n",
      "Environment parameters updated: masscart=1.0, length=0.9, gravity=9.8\n",
      "\n",
      "Changed pole length to: 0.9m at episode 5000\n",
      "\n",
      "Debug Metrics at episode 6000:\n",
      "Recent average reward: 550.51\n",
      "Recent average steps: 551.07\n",
      "Last update episode: 0\n",
      "\n",
      "Metrics Debug at Episode 6000:\n",
      "Recent Average Reward: 550.26\n",
      "Average Balance Time: 550.83\n",
      "Balance Time Variance: 594423.90\n",
      "Last Update Episode: 0\n",
      "Time Since Last Update: 6000\n",
      "\n",
      "Checking for update at episode 6000, time since last update: 6000\n",
      "\n",
      "Debug Metrics at episode 7000:\n",
      "Recent average reward: 736.20\n",
      "Recent average steps: 736.27\n",
      "Last update episode: 0\n",
      "\n",
      "Metrics Debug at Episode 7000:\n",
      "Recent Average Reward: 754.42\n",
      "Average Balance Time: 754.49\n",
      "Balance Time Variance: 456396.25\n",
      "Last Update Episode: 0\n",
      "Time Since Last Update: 7000\n",
      "\n",
      "Debug Metrics at episode 8000:\n",
      "Recent average reward: 312.50\n",
      "Recent average steps: 312.77\n",
      "Last update episode: 0\n",
      "\n",
      "Metrics Debug at Episode 8000:\n",
      "Recent Average Reward: 330.79\n",
      "Average Balance Time: 331.06\n",
      "Balance Time Variance: 165473.04\n",
      "Last Update Episode: 0\n",
      "Time Since Last Update: 8000\n",
      "\n",
      "Checking for update at episode 8000, time since last update: 8000\n",
      "\n",
      "Debug Metrics at episode 9000:\n",
      "Recent average reward: 1370.93\n",
      "Recent average steps: 1373.07\n",
      "Last update episode: 0\n",
      "\n",
      "Metrics Debug at Episode 9000:\n",
      "Recent Average Reward: 1370.96\n",
      "Average Balance Time: 1373.10\n",
      "Balance Time Variance: 818900.81\n",
      "Last Update Episode: 0\n",
      "Time Since Last Update: 9000\n",
      "\n",
      "Debug Metrics at episode 10000:\n",
      "Recent average reward: 1922.93\n",
      "Recent average steps: 1923.24\n",
      "Last update episode: 0\n",
      "\n",
      "Completed testing adaptivereward\n",
      "Final average reward: 1922.93\n",
      "Final average balance time: 1923.24\n",
      "\n",
      "Testing reward function: meta_learning\n",
      "\n",
      "Metrics Debug at Episode 0:\n",
      "Recent Average Reward: -31.46\n",
      "Average Balance Time: 19.00\n",
      "Balance Time Variance: 0.00\n",
      "\n",
      "Episode 0\n",
      "  Steps: 19\n",
      "  Total Reward: -31.458078511059284\n",
      "  Average Balance Time = 19.00\n",
      "  Average Reward = -31.46\n",
      "\n",
      "Metrics Debug at Episode 1000:\n",
      "Recent Average Reward: -13.38\n",
      "Average Balance Time: 58.09\n",
      "Balance Time Variance: 454.88\n",
      "\n",
      "Metrics Debug at Episode 2000:\n",
      "Recent Average Reward: -13.89\n",
      "Average Balance Time: 66.82\n",
      "Balance Time Variance: 1092.57\n",
      "\n",
      "Metrics Debug at Episode 3000:\n",
      "Recent Average Reward: -13.51\n",
      "Average Balance Time: 66.54\n",
      "Balance Time Variance: 1440.63\n",
      "\n",
      "Metrics Debug at Episode 4000:\n",
      "Recent Average Reward: -13.36\n",
      "Average Balance Time: 58.33\n",
      "Balance Time Variance: 438.84\n",
      "\n",
      "Metrics Debug at Episode 5000:\n",
      "Recent Average Reward: -14.88\n",
      "Average Balance Time: 71.79\n",
      "Balance Time Variance: 1602.39\n",
      "Environment parameters updated: masscart=1.0, length=0.3, gravity=9.8\n",
      "\n",
      "Changed pole length to: 0.3m at episode 5000\n",
      "\n",
      "Metrics Debug at Episode 6000:\n",
      "Recent Average Reward: -16.68\n",
      "Average Balance Time: 78.63\n",
      "Balance Time Variance: 2256.01\n",
      "\n",
      "Metrics Debug at Episode 7000:\n",
      "Recent Average Reward: -14.42\n",
      "Average Balance Time: 64.65\n",
      "Balance Time Variance: 851.83\n",
      "\n",
      "Metrics Debug at Episode 8000:\n",
      "Recent Average Reward: -11.57\n",
      "Average Balance Time: 47.36\n",
      "Balance Time Variance: 216.15\n",
      "\n",
      "Metrics Debug at Episode 9000:\n",
      "Recent Average Reward: -19.01\n",
      "Average Balance Time: 156.74\n",
      "Balance Time Variance: 37933.89\n",
      "\n",
      "Completed testing meta_learning\n",
      "Final average reward: -57.12\n",
      "Final average balance time: 784.06\n",
      "\n",
      "Testing reward function: energy_based\n",
      "\n",
      "Metrics Debug at Episode 0:\n",
      "Recent Average Reward: -3.83\n",
      "Average Balance Time: 28.00\n",
      "Balance Time Variance: 0.00\n",
      "\n",
      "Episode 0\n",
      "  Steps: 28\n",
      "  Total Reward: -3.8346405029296875\n",
      "  Average Balance Time = 28.00\n",
      "  Average Reward = -3.83\n",
      "\n",
      "Metrics Debug at Episode 1000:\n",
      "Recent Average Reward: -7.55\n",
      "Average Balance Time: 151.01\n",
      "Balance Time Variance: 140377.07\n",
      "\n",
      "Metrics Debug at Episode 2000:\n",
      "Recent Average Reward: -4.65\n",
      "Average Balance Time: 21.95\n",
      "Balance Time Variance: 251.21\n",
      "\n",
      "Metrics Debug at Episode 3000:\n",
      "Recent Average Reward: -32.23\n",
      "Average Balance Time: 57.18\n",
      "Balance Time Variance: 44138.63\n",
      "\n",
      "Metrics Debug at Episode 4000:\n",
      "Recent Average Reward: -103.45\n",
      "Average Balance Time: 132.51\n",
      "Balance Time Variance: 48059.33\n",
      "\n",
      "Metrics Debug at Episode 5000:\n",
      "Recent Average Reward: -1436.91\n",
      "Average Balance Time: 790.16\n",
      "Balance Time Variance: 825002.49\n",
      "\n",
      "Updating physics-based reward at episode 5000\n",
      "✓ Physics-based update completed\n",
      "Environment parameters updated: masscart=1.0, length=0.9, gravity=9.8\n",
      "\n",
      "Changed pole length to: 0.9m at episode 5000\n",
      "\n",
      "Metrics Debug at Episode 6000:\n",
      "Recent Average Reward: 349.97\n",
      "Average Balance Time: 1741.33\n",
      "Balance Time Variance: 447783.68\n",
      "\n",
      "Metrics Debug at Episode 7000:\n",
      "Recent Average Reward: -423.79\n",
      "Average Balance Time: 424.43\n",
      "Balance Time Variance: 512054.87\n",
      "\n",
      "Metrics Debug at Episode 8000:\n",
      "Recent Average Reward: -85.39\n",
      "Average Balance Time: 166.46\n",
      "Balance Time Variance: 75094.11\n",
      "\n",
      "Metrics Debug at Episode 9000:\n",
      "Recent Average Reward: -1522.80\n",
      "Average Balance Time: 1348.61\n",
      "Balance Time Variance: 741770.90\n",
      "\n",
      "Completed testing energy_based\n",
      "Final average reward: -136.70\n",
      "Final average balance time: 418.58\n",
      "\n",
      "Testing reward function: baseline\n",
      "\n",
      "Metrics Debug at Episode 0:\n",
      "Recent Average Reward: 13.00\n",
      "Average Balance Time: 13.00\n",
      "Balance Time Variance: 0.00\n",
      "\n",
      "Episode 0\n",
      "  Steps: 13\n",
      "  Total Reward: 13.0\n",
      "  Average Balance Time = 13.00\n",
      "  Average Reward = 13.00\n",
      "\n",
      "Metrics Debug at Episode 1000:\n",
      "Recent Average Reward: 157.69\n",
      "Average Balance Time: 157.69\n",
      "Balance Time Variance: 1923.89\n",
      "\n",
      "Metrics Debug at Episode 2000:\n",
      "Recent Average Reward: 41.05\n",
      "Average Balance Time: 41.05\n",
      "Balance Time Variance: 206.19\n",
      "\n",
      "Metrics Debug at Episode 3000:\n",
      "Recent Average Reward: 814.72\n",
      "Average Balance Time: 814.72\n",
      "Balance Time Variance: 521711.14\n",
      "\n",
      "Metrics Debug at Episode 4000:\n",
      "Recent Average Reward: 1437.23\n",
      "Average Balance Time: 1437.23\n",
      "Balance Time Variance: 352350.78\n",
      "\n",
      "Metrics Debug at Episode 5000:\n",
      "Recent Average Reward: 1132.93\n",
      "Average Balance Time: 1132.93\n",
      "Balance Time Variance: 574669.85\n",
      "Environment parameters updated: masscart=1.0, length=0.3, gravity=9.8\n",
      "\n",
      "Changed pole length to: 0.3m at episode 5000\n",
      "\n",
      "Metrics Debug at Episode 6000:\n",
      "Recent Average Reward: 1711.54\n",
      "Average Balance Time: 1711.54\n",
      "Balance Time Variance: 280547.85\n",
      "\n",
      "Metrics Debug at Episode 7000:\n",
      "Recent Average Reward: 1457.52\n",
      "Average Balance Time: 1457.52\n",
      "Balance Time Variance: 221760.03\n",
      "\n",
      "Metrics Debug at Episode 8000:\n",
      "Recent Average Reward: 2000.00\n",
      "Average Balance Time: 2000.00\n",
      "Balance Time Variance: 0.00\n",
      "\n",
      "Metrics Debug at Episode 9000:\n",
      "Recent Average Reward: 2000.00\n",
      "Average Balance Time: 2000.00\n",
      "Balance Time Variance: 0.00\n",
      "\n",
      "Completed testing baseline\n",
      "Final average reward: 182.82\n",
      "Final average balance time: 182.82\n",
      "Saved plot: performance_comparison_20250208_112401.png in PerformanceExperiment/experiment_20250208_101230/run_2\n",
      "Saved metrics table: metrics_run_2_20250208_112402.png in PerformanceExperiment/experiment_20250208_101230/run_2\n",
      "\n",
      "Starting Run 3/4\n",
      "Starting Performance Comparison Test...\n",
      "Environment parameters updated: masscart=1.0, length=0.3, gravity=9.8\n",
      "\n",
      "Testing reward function: adaptivereward\n",
      "\n",
      "Metrics Debug at Episode 0:\n",
      "Recent Average Reward: 15.89\n",
      "Average Balance Time: 16.00\n",
      "Balance Time Variance: 0.00\n",
      "Last Update Episode: 0\n",
      "Time Since Last Update: 0\n",
      "\n",
      "Checking for update at episode 0, time since last update: 0\n",
      "\n",
      "Episode 0\n",
      "  Steps: 16\n",
      "  Total Reward: 15.893426895141602\n",
      "  Average Balance Time = 16.00\n",
      "  Average Reward = 15.89\n",
      "\n",
      "Debug Metrics at episode 1000:\n",
      "Recent average reward: 172.17\n",
      "Recent average steps: 172.29\n",
      "Last update episode: 0\n",
      "\n",
      "Metrics Debug at Episode 1000:\n",
      "Recent Average Reward: 170.66\n",
      "Average Balance Time: 170.78\n",
      "Balance Time Variance: 20935.51\n",
      "Last Update Episode: 0\n",
      "Time Since Last Update: 1000\n",
      "\n",
      "Debug Metrics at episode 2000:\n",
      "Recent average reward: 374.40\n",
      "Recent average steps: 374.83\n",
      "Last update episode: 0\n",
      "\n",
      "Metrics Debug at Episode 2000:\n",
      "Recent Average Reward: 375.87\n",
      "Average Balance Time: 376.31\n",
      "Balance Time Variance: 240871.93\n",
      "Last Update Episode: 0\n",
      "Time Since Last Update: 2000\n",
      "\n",
      "Checking for update at episode 2000, time since last update: 2000\n",
      "\n",
      "Debug Metrics at episode 3000:\n",
      "Recent average reward: 269.37\n",
      "Recent average steps: 269.85\n",
      "Last update episode: 0\n",
      "\n",
      "Metrics Debug at Episode 3000:\n",
      "Recent Average Reward: 269.25\n",
      "Average Balance Time: 269.73\n",
      "Balance Time Variance: 208918.12\n",
      "Last Update Episode: 0\n",
      "Time Since Last Update: 3000\n",
      "\n",
      "Debug Metrics at episode 4000:\n",
      "Recent average reward: 212.30\n",
      "Recent average steps: 212.82\n",
      "Last update episode: 0\n",
      "\n",
      "Metrics Debug at Episode 4000:\n",
      "Recent Average Reward: 216.15\n",
      "Average Balance Time: 216.67\n",
      "Balance Time Variance: 33945.30\n",
      "Last Update Episode: 0\n",
      "Time Since Last Update: 4000\n",
      "\n",
      "Checking for update at episode 4000, time since last update: 4000\n",
      "\n",
      "Debug Metrics at episode 5000:\n",
      "Recent average reward: 344.65\n",
      "Recent average steps: 344.95\n",
      "Last update episode: 0\n",
      "\n",
      "Metrics Debug at Episode 5000:\n",
      "Recent Average Reward: 348.71\n",
      "Average Balance Time: 349.00\n",
      "Balance Time Variance: 48154.46\n",
      "Last Update Episode: 0\n",
      "Time Since Last Update: 5000\n",
      "\n",
      "Checking for update at episode 5000, time since last update: 5000\n",
      "✓ Update triggered at episode 5000\n",
      "\n",
      "Attempting LLM update at episode 5000\n",
      "\n",
      "Checking for update at episode 5000, time since last update: 0\n",
      "✓ Update triggered at episode 5000\n",
      "\n",
      "Attempting to update composite component: stability\n",
      "Updated stability weight: 0.273 -> 0.273\n",
      "\n",
      "Checking for update at episode 5000, time since last update: 0\n",
      "✓ Update triggered at episode 5000\n",
      "\n",
      "Attempting to update composite component: efficiency\n",
      "Updated efficiency weight: 0.273 -> 0.273\n",
      "\n",
      "Checking for update at episode 5000, time since last update: 0\n",
      "✓ Update triggered at episode 5000\n",
      "\n",
      "Attempting to update composite component: time\n",
      "Updated time weight: 0.455 -> 0.504\n",
      "Environment parameters updated: masscart=1.0, length=0.9, gravity=9.8\n",
      "\n",
      "Changed pole length to: 0.9m at episode 5000\n",
      "\n",
      "Debug Metrics at episode 6000:\n",
      "Recent average reward: 80.89\n",
      "Recent average steps: 81.84\n",
      "Last update episode: 0\n",
      "\n",
      "Metrics Debug at Episode 6000:\n",
      "Recent Average Reward: 80.66\n",
      "Average Balance Time: 81.61\n",
      "Balance Time Variance: 272.58\n",
      "Last Update Episode: 0\n",
      "Time Since Last Update: 6000\n",
      "\n",
      "Checking for update at episode 6000, time since last update: 6000\n",
      "\n",
      "Debug Metrics at episode 7000:\n",
      "Recent average reward: 557.73\n",
      "Recent average steps: 558.21\n",
      "Last update episode: 0\n",
      "\n",
      "Metrics Debug at Episode 7000:\n",
      "Recent Average Reward: 556.96\n",
      "Average Balance Time: 557.45\n",
      "Balance Time Variance: 569414.77\n",
      "Last Update Episode: 0\n",
      "Time Since Last Update: 7000\n",
      "\n",
      "Debug Metrics at episode 8000:\n",
      "Recent average reward: 13.00\n",
      "Recent average steps: 13.11\n",
      "Last update episode: 0\n",
      "\n",
      "Metrics Debug at Episode 8000:\n",
      "Recent Average Reward: 12.91\n",
      "Average Balance Time: 13.02\n",
      "Balance Time Variance: 25.96\n",
      "Last Update Episode: 0\n",
      "Time Since Last Update: 8000\n",
      "\n",
      "Checking for update at episode 8000, time since last update: 8000\n",
      "\n",
      "Debug Metrics at episode 9000:\n",
      "Recent average reward: 1280.95\n",
      "Recent average steps: 1281.14\n",
      "Last update episode: 0\n",
      "\n",
      "Metrics Debug at Episode 9000:\n",
      "Recent Average Reward: 1281.50\n",
      "Average Balance Time: 1281.69\n",
      "Balance Time Variance: 580021.61\n",
      "Last Update Episode: 0\n",
      "Time Since Last Update: 9000\n",
      "\n",
      "Debug Metrics at episode 10000:\n",
      "Recent average reward: 1999.95\n",
      "Recent average steps: 2000.00\n",
      "Last update episode: 0\n",
      "\n",
      "Completed testing adaptivereward\n",
      "Final average reward: 1999.95\n",
      "Final average balance time: 2000.00\n",
      "\n",
      "Testing reward function: meta_learning\n",
      "\n",
      "Metrics Debug at Episode 0:\n",
      "Recent Average Reward: -57.74\n",
      "Average Balance Time: 124.00\n",
      "Balance Time Variance: 0.00\n",
      "\n",
      "Episode 0\n",
      "  Steps: 124\n",
      "  Total Reward: -57.74201522604562\n",
      "  Average Balance Time = 124.00\n",
      "  Average Reward = -57.74\n",
      "\n",
      "Metrics Debug at Episode 1000:\n",
      "Recent Average Reward: -20.34\n",
      "Average Balance Time: 56.79\n",
      "Balance Time Variance: 595.15\n",
      "\n",
      "Metrics Debug at Episode 2000:\n",
      "Recent Average Reward: -24.83\n",
      "Average Balance Time: 71.87\n",
      "Balance Time Variance: 776.55\n",
      "\n",
      "Metrics Debug at Episode 3000:\n",
      "Recent Average Reward: -20.53\n",
      "Average Balance Time: 60.92\n",
      "Balance Time Variance: 683.45\n",
      "\n",
      "Metrics Debug at Episode 4000:\n",
      "Recent Average Reward: -20.55\n",
      "Average Balance Time: 67.46\n",
      "Balance Time Variance: 1615.83\n",
      "\n",
      "Metrics Debug at Episode 5000:\n",
      "Recent Average Reward: -24.67\n",
      "Average Balance Time: 100.45\n",
      "Balance Time Variance: 5588.81\n",
      "Environment parameters updated: masscart=1.0, length=0.3, gravity=9.8\n",
      "\n",
      "Changed pole length to: 0.3m at episode 5000\n",
      "\n",
      "Metrics Debug at Episode 6000:\n",
      "Recent Average Reward: -31.68\n",
      "Average Balance Time: 85.39\n",
      "Balance Time Variance: 1812.44\n",
      "\n",
      "Metrics Debug at Episode 7000:\n",
      "Recent Average Reward: -21.78\n",
      "Average Balance Time: 85.12\n",
      "Balance Time Variance: 16575.81\n",
      "\n",
      "Metrics Debug at Episode 8000:\n",
      "Recent Average Reward: -19.36\n",
      "Average Balance Time: 68.28\n",
      "Balance Time Variance: 6025.26\n",
      "\n",
      "Metrics Debug at Episode 9000:\n",
      "Recent Average Reward: -33.86\n",
      "Average Balance Time: 142.50\n",
      "Balance Time Variance: 27549.81\n",
      "\n",
      "Completed testing meta_learning\n",
      "Final average reward: -33.04\n",
      "Final average balance time: 158.20\n",
      "\n",
      "Testing reward function: energy_based\n",
      "\n",
      "Metrics Debug at Episode 0:\n",
      "Recent Average Reward: -3.51\n",
      "Average Balance Time: 16.00\n",
      "Balance Time Variance: 0.00\n",
      "\n",
      "Episode 0\n",
      "  Steps: 16\n",
      "  Total Reward: -3.5096819400787354\n",
      "  Average Balance Time = 16.00\n",
      "  Average Reward = -3.51\n",
      "\n",
      "Metrics Debug at Episode 1000:\n",
      "Recent Average Reward: 2.87\n",
      "Average Balance Time: 259.52\n",
      "Balance Time Variance: 287133.15\n",
      "\n",
      "Metrics Debug at Episode 2000:\n",
      "Recent Average Reward: -3.08\n",
      "Average Balance Time: 69.28\n",
      "Balance Time Variance: 1941.32\n",
      "\n",
      "Metrics Debug at Episode 3000:\n",
      "Recent Average Reward: 136.55\n",
      "Average Balance Time: 285.28\n",
      "Balance Time Variance: 328100.52\n",
      "\n",
      "Metrics Debug at Episode 4000:\n",
      "Recent Average Reward: 916.69\n",
      "Average Balance Time: 1764.10\n",
      "Balance Time Variance: 408113.75\n",
      "\n",
      "Metrics Debug at Episode 5000:\n",
      "Recent Average Reward: 1606.36\n",
      "Average Balance Time: 1607.38\n",
      "Balance Time Variance: 616687.82\n",
      "\n",
      "Updating physics-based reward at episode 5000\n",
      "✓ Physics-based update completed\n",
      "Environment parameters updated: masscart=1.0, length=0.9, gravity=9.8\n",
      "\n",
      "Changed pole length to: 0.9m at episode 5000\n",
      "\n",
      "Metrics Debug at Episode 6000:\n",
      "Recent Average Reward: 1852.92\n",
      "Average Balance Time: 2000.00\n",
      "Balance Time Variance: 0.00\n",
      "\n",
      "Metrics Debug at Episode 7000:\n",
      "Recent Average Reward: 2421.42\n",
      "Average Balance Time: 1980.98\n",
      "Balance Time Variance: 25315.38\n",
      "\n",
      "Metrics Debug at Episode 8000:\n",
      "Recent Average Reward: -34.34\n",
      "Average Balance Time: 64.49\n",
      "Balance Time Variance: 1311.81\n",
      "\n",
      "Metrics Debug at Episode 9000:\n",
      "Recent Average Reward: 0.73\n",
      "Average Balance Time: 204.04\n",
      "Balance Time Variance: 79056.92\n",
      "\n",
      "Completed testing energy_based\n",
      "Final average reward: 947.53\n",
      "Final average balance time: 1981.51\n",
      "\n",
      "Testing reward function: baseline\n",
      "\n",
      "Metrics Debug at Episode 0:\n",
      "Recent Average Reward: 12.00\n",
      "Average Balance Time: 12.00\n",
      "Balance Time Variance: 0.00\n",
      "\n",
      "Episode 0\n",
      "  Steps: 12\n",
      "  Total Reward: 12.0\n",
      "  Average Balance Time = 12.00\n",
      "  Average Reward = 12.00\n",
      "\n",
      "Metrics Debug at Episode 1000:\n",
      "Recent Average Reward: 41.36\n",
      "Average Balance Time: 41.36\n",
      "Balance Time Variance: 253.77\n",
      "\n",
      "Metrics Debug at Episode 2000:\n",
      "Recent Average Reward: 307.39\n",
      "Average Balance Time: 307.39\n",
      "Balance Time Variance: 130065.14\n",
      "\n",
      "Metrics Debug at Episode 3000:\n",
      "Recent Average Reward: 448.05\n",
      "Average Balance Time: 448.05\n",
      "Balance Time Variance: 383636.47\n",
      "\n",
      "Metrics Debug at Episode 4000:\n",
      "Recent Average Reward: 56.94\n",
      "Average Balance Time: 56.94\n",
      "Balance Time Variance: 41272.22\n",
      "\n",
      "Metrics Debug at Episode 5000:\n",
      "Recent Average Reward: 584.02\n",
      "Average Balance Time: 584.02\n",
      "Balance Time Variance: 604192.22\n",
      "Environment parameters updated: masscart=1.0, length=0.3, gravity=9.8\n",
      "\n",
      "Changed pole length to: 0.3m at episode 5000\n",
      "\n",
      "Metrics Debug at Episode 6000:\n",
      "Recent Average Reward: 671.81\n",
      "Average Balance Time: 671.81\n",
      "Balance Time Variance: 695192.51\n",
      "\n",
      "Metrics Debug at Episode 7000:\n",
      "Recent Average Reward: 568.80\n",
      "Average Balance Time: 568.80\n",
      "Balance Time Variance: 565802.76\n",
      "\n",
      "Metrics Debug at Episode 8000:\n",
      "Recent Average Reward: 232.61\n",
      "Average Balance Time: 232.61\n",
      "Balance Time Variance: 132511.50\n",
      "\n",
      "Metrics Debug at Episode 9000:\n",
      "Recent Average Reward: 9.46\n",
      "Average Balance Time: 9.46\n",
      "Balance Time Variance: 0.73\n",
      "\n",
      "Completed testing baseline\n",
      "Final average reward: 10.20\n",
      "Final average balance time: 10.20\n",
      "Saved plot: performance_comparison_20250208_120143.png in PerformanceExperiment/experiment_20250208_101230/run_3\n",
      "Saved metrics table: metrics_run_3_20250208_120145.png in PerformanceExperiment/experiment_20250208_101230/run_3\n",
      "\n",
      "Starting Run 4/4\n",
      "Starting Performance Comparison Test...\n",
      "Environment parameters updated: masscart=1.0, length=0.3, gravity=9.8\n",
      "\n",
      "Testing reward function: adaptivereward\n",
      "\n",
      "Metrics Debug at Episode 0:\n",
      "Recent Average Reward: 21.93\n",
      "Average Balance Time: 22.00\n",
      "Balance Time Variance: 0.00\n",
      "Last Update Episode: 0\n",
      "Time Since Last Update: 0\n",
      "\n",
      "Checking for update at episode 0, time since last update: 0\n",
      "\n",
      "Episode 0\n",
      "  Steps: 22\n",
      "  Total Reward: 21.93315887451172\n",
      "  Average Balance Time = 22.00\n",
      "  Average Reward = 21.93\n",
      "\n",
      "Debug Metrics at episode 1000:\n",
      "Recent average reward: 612.42\n",
      "Recent average steps: 612.58\n",
      "Last update episode: 0\n",
      "\n",
      "Metrics Debug at Episode 1000:\n",
      "Recent Average Reward: 610.09\n",
      "Average Balance Time: 610.25\n",
      "Balance Time Variance: 323728.55\n",
      "Last Update Episode: 0\n",
      "Time Since Last Update: 1000\n",
      "\n",
      "Debug Metrics at episode 2000:\n",
      "Recent average reward: 568.77\n",
      "Recent average steps: 569.30\n",
      "Last update episode: 0\n",
      "\n",
      "Metrics Debug at Episode 2000:\n",
      "Recent Average Reward: 550.22\n",
      "Average Balance Time: 550.74\n",
      "Balance Time Variance: 549959.97\n",
      "Last Update Episode: 0\n",
      "Time Since Last Update: 2000\n",
      "\n",
      "Checking for update at episode 2000, time since last update: 2000\n",
      "\n",
      "Debug Metrics at episode 3000:\n",
      "Recent average reward: 1963.63\n",
      "Recent average steps: 1963.73\n",
      "Last update episode: 0\n",
      "\n",
      "Metrics Debug at Episode 3000:\n",
      "Recent Average Reward: 1963.63\n",
      "Average Balance Time: 1963.73\n",
      "Balance Time Variance: 65076.14\n",
      "Last Update Episode: 0\n",
      "Time Since Last Update: 3000\n",
      "\n",
      "Debug Metrics at episode 4000:\n",
      "Recent average reward: 1386.67\n",
      "Recent average steps: 1386.80\n",
      "Last update episode: 0\n",
      "\n",
      "Metrics Debug at Episode 4000:\n",
      "Recent Average Reward: 1388.89\n",
      "Average Balance Time: 1389.02\n",
      "Balance Time Variance: 769615.42\n",
      "Last Update Episode: 0\n",
      "Time Since Last Update: 4000\n",
      "\n",
      "Checking for update at episode 4000, time since last update: 4000\n",
      "\n",
      "Debug Metrics at episode 5000:\n",
      "Recent average reward: 252.63\n",
      "Recent average steps: 252.86\n",
      "Last update episode: 0\n",
      "\n",
      "Metrics Debug at Episode 5000:\n",
      "Recent Average Reward: 250.80\n",
      "Average Balance Time: 251.03\n",
      "Balance Time Variance: 112417.83\n",
      "Last Update Episode: 0\n",
      "Time Since Last Update: 5000\n",
      "\n",
      "Checking for update at episode 5000, time since last update: 5000\n",
      "✓ Update triggered at episode 5000\n",
      "\n",
      "Attempting LLM update at episode 5000\n",
      "\n",
      "Checking for update at episode 5000, time since last update: 0\n",
      "✓ Update triggered at episode 5000\n",
      "\n",
      "Attempting to update composite component: stability\n",
      "Updated stability weight: 0.248 -> 0.248\n",
      "\n",
      "Checking for update at episode 5000, time since last update: 0\n",
      "✓ Update triggered at episode 5000\n",
      "\n",
      "Attempting to update composite component: efficiency\n",
      "Updated efficiency weight: 0.248 -> 0.248\n",
      "\n",
      "Checking for update at episode 5000, time since last update: 0\n",
      "✓ Update triggered at episode 5000\n",
      "\n",
      "Attempting to update composite component: time\n",
      "Updated time weight: 0.504 -> 0.549\n",
      "Environment parameters updated: masscart=1.0, length=0.9, gravity=9.8\n",
      "\n",
      "Changed pole length to: 0.9m at episode 5000\n",
      "\n",
      "Debug Metrics at episode 6000:\n",
      "Recent average reward: 258.58\n",
      "Recent average steps: 258.66\n",
      "Last update episode: 0\n",
      "\n",
      "Metrics Debug at Episode 6000:\n",
      "Recent Average Reward: 258.27\n",
      "Average Balance Time: 258.35\n",
      "Balance Time Variance: 18196.63\n",
      "Last Update Episode: 0\n",
      "Time Since Last Update: 6000\n",
      "\n",
      "Checking for update at episode 6000, time since last update: 6000\n",
      "\n",
      "Debug Metrics at episode 7000:\n",
      "Recent average reward: 163.00\n",
      "Recent average steps: 163.21\n",
      "Last update episode: 0\n",
      "\n",
      "Metrics Debug at Episode 7000:\n",
      "Recent Average Reward: 165.16\n",
      "Average Balance Time: 165.37\n",
      "Balance Time Variance: 1755.45\n",
      "Last Update Episode: 0\n",
      "Time Since Last Update: 7000\n",
      "\n",
      "Debug Metrics at episode 8000:\n",
      "Recent average reward: 1113.86\n",
      "Recent average steps: 1113.87\n",
      "Last update episode: 0\n",
      "\n",
      "Metrics Debug at Episode 8000:\n",
      "Recent Average Reward: 1112.56\n",
      "Average Balance Time: 1112.57\n",
      "Balance Time Variance: 209287.51\n",
      "Last Update Episode: 0\n",
      "Time Since Last Update: 8000\n",
      "\n",
      "Checking for update at episode 8000, time since last update: 8000\n",
      "\n",
      "Debug Metrics at episode 9000:\n",
      "Recent average reward: 1356.10\n",
      "Recent average steps: 1356.12\n",
      "Last update episode: 0\n",
      "\n",
      "Metrics Debug at Episode 9000:\n",
      "Recent Average Reward: 1339.75\n",
      "Average Balance Time: 1339.77\n",
      "Balance Time Variance: 510547.40\n",
      "Last Update Episode: 0\n",
      "Time Since Last Update: 9000\n",
      "\n",
      "Debug Metrics at episode 10000:\n",
      "Recent average reward: 147.93\n",
      "Recent average steps: 148.32\n",
      "Last update episode: 0\n",
      "\n",
      "Completed testing adaptivereward\n",
      "Final average reward: 147.93\n",
      "Final average balance time: 148.32\n",
      "\n",
      "Testing reward function: meta_learning\n",
      "\n",
      "Metrics Debug at Episode 0:\n",
      "Recent Average Reward: -13.40\n",
      "Average Balance Time: 15.00\n",
      "Balance Time Variance: 0.00\n",
      "\n",
      "Episode 0\n",
      "  Steps: 15\n",
      "  Total Reward: -13.39857729524374\n",
      "  Average Balance Time = 15.00\n",
      "  Average Reward = -13.40\n",
      "\n",
      "Metrics Debug at Episode 1000:\n",
      "Recent Average Reward: 1.36\n",
      "Average Balance Time: 65.30\n",
      "Balance Time Variance: 564.07\n",
      "\n",
      "Metrics Debug at Episode 2000:\n",
      "Recent Average Reward: -0.53\n",
      "Average Balance Time: 79.13\n",
      "Balance Time Variance: 5438.81\n",
      "\n",
      "Metrics Debug at Episode 3000:\n",
      "Recent Average Reward: 0.21\n",
      "Average Balance Time: 62.23\n",
      "Balance Time Variance: 1986.50\n",
      "\n",
      "Metrics Debug at Episode 4000:\n",
      "Recent Average Reward: 0.35\n",
      "Average Balance Time: 50.56\n",
      "Balance Time Variance: 672.17\n",
      "\n",
      "Metrics Debug at Episode 5000:\n",
      "Recent Average Reward: 1.56\n",
      "Average Balance Time: 60.97\n",
      "Balance Time Variance: 848.67\n",
      "Environment parameters updated: masscart=1.0, length=0.3, gravity=9.8\n",
      "\n",
      "Changed pole length to: 0.3m at episode 5000\n",
      "\n",
      "Metrics Debug at Episode 6000:\n",
      "Recent Average Reward: 1.43\n",
      "Average Balance Time: 53.33\n",
      "Balance Time Variance: 1380.14\n",
      "\n",
      "Metrics Debug at Episode 7000:\n",
      "Recent Average Reward: 1.52\n",
      "Average Balance Time: 56.09\n",
      "Balance Time Variance: 1624.62\n",
      "\n",
      "Metrics Debug at Episode 8000:\n",
      "Recent Average Reward: 1.67\n",
      "Average Balance Time: 53.04\n",
      "Balance Time Variance: 166.06\n",
      "\n",
      "Metrics Debug at Episode 9000:\n",
      "Recent Average Reward: 1.58\n",
      "Average Balance Time: 50.43\n",
      "Balance Time Variance: 1099.45\n",
      "\n",
      "Completed testing meta_learning\n",
      "Final average reward: 0.81\n",
      "Final average balance time: 79.79\n",
      "\n",
      "Testing reward function: energy_based\n",
      "\n",
      "Metrics Debug at Episode 0:\n",
      "Recent Average Reward: -2.66\n",
      "Average Balance Time: 20.00\n",
      "Balance Time Variance: 0.00\n",
      "\n",
      "Episode 0\n",
      "  Steps: 20\n",
      "  Total Reward: -2.6623706817626953\n",
      "  Average Balance Time = 20.00\n",
      "  Average Reward = -2.66\n",
      "\n",
      "Metrics Debug at Episode 1000:\n",
      "Recent Average Reward: -1.27\n",
      "Average Balance Time: 34.04\n",
      "Balance Time Variance: 219.56\n",
      "\n",
      "Metrics Debug at Episode 2000:\n",
      "Recent Average Reward: 1403.59\n",
      "Average Balance Time: 1420.37\n",
      "Balance Time Variance: 784120.59\n",
      "\n",
      "Metrics Debug at Episode 3000:\n",
      "Recent Average Reward: 1826.37\n",
      "Average Balance Time: 2000.00\n",
      "Balance Time Variance: 0.00\n",
      "\n",
      "Metrics Debug at Episode 4000:\n",
      "Recent Average Reward: -26.24\n",
      "Average Balance Time: 65.21\n",
      "Balance Time Variance: 50.89\n",
      "\n",
      "Metrics Debug at Episode 5000:\n",
      "Recent Average Reward: -207.41\n",
      "Average Balance Time: 243.28\n",
      "Balance Time Variance: 9210.84\n",
      "\n",
      "Updating physics-based reward at episode 5000\n",
      "✓ Physics-based update completed\n",
      "Environment parameters updated: masscart=1.0, length=0.9, gravity=9.8\n",
      "\n",
      "Changed pole length to: 0.9m at episode 5000\n",
      "\n",
      "Metrics Debug at Episode 6000:\n",
      "Recent Average Reward: 35.77\n",
      "Average Balance Time: 156.96\n",
      "Balance Time Variance: 46621.98\n",
      "\n",
      "Metrics Debug at Episode 7000:\n",
      "Recent Average Reward: -25.50\n",
      "Average Balance Time: 128.89\n",
      "Balance Time Variance: 295.62\n",
      "\n",
      "Metrics Debug at Episode 8000:\n",
      "Recent Average Reward: 56.23\n",
      "Average Balance Time: 175.72\n",
      "Balance Time Variance: 671.40\n",
      "\n",
      "Metrics Debug at Episode 9000:\n",
      "Recent Average Reward: 131.80\n",
      "Average Balance Time: 288.88\n",
      "Balance Time Variance: 24211.07\n",
      "\n",
      "Completed testing energy_based\n",
      "Final average reward: 308.72\n",
      "Final average balance time: 330.43\n",
      "\n",
      "Testing reward function: baseline\n",
      "\n",
      "Metrics Debug at Episode 0:\n",
      "Recent Average Reward: 9.00\n",
      "Average Balance Time: 9.00\n",
      "Balance Time Variance: 0.00\n",
      "\n",
      "Episode 0\n",
      "  Steps: 9\n",
      "  Total Reward: 9.0\n",
      "  Average Balance Time = 9.00\n",
      "  Average Reward = 9.00\n",
      "\n",
      "Metrics Debug at Episode 1000:\n",
      "Recent Average Reward: 237.01\n",
      "Average Balance Time: 237.01\n",
      "Balance Time Variance: 34504.33\n",
      "\n",
      "Metrics Debug at Episode 2000:\n",
      "Recent Average Reward: 261.80\n",
      "Average Balance Time: 261.80\n",
      "Balance Time Variance: 57526.70\n",
      "\n",
      "Metrics Debug at Episode 3000:\n",
      "Recent Average Reward: 908.20\n",
      "Average Balance Time: 908.20\n",
      "Balance Time Variance: 415436.12\n",
      "\n",
      "Metrics Debug at Episode 4000:\n",
      "Recent Average Reward: 58.29\n",
      "Average Balance Time: 58.29\n",
      "Balance Time Variance: 78408.31\n",
      "\n",
      "Metrics Debug at Episode 5000:\n",
      "Recent Average Reward: 79.27\n",
      "Average Balance Time: 79.27\n",
      "Balance Time Variance: 62.18\n",
      "Environment parameters updated: masscart=1.0, length=0.3, gravity=9.8\n",
      "\n",
      "Changed pole length to: 0.3m at episode 5000\n",
      "\n",
      "Metrics Debug at Episode 6000:\n",
      "Recent Average Reward: 598.50\n",
      "Average Balance Time: 598.50\n",
      "Balance Time Variance: 357892.63\n",
      "\n",
      "Metrics Debug at Episode 7000:\n",
      "Recent Average Reward: 71.29\n",
      "Average Balance Time: 71.29\n",
      "Balance Time Variance: 377.37\n",
      "\n",
      "Metrics Debug at Episode 8000:\n",
      "Recent Average Reward: 133.79\n",
      "Average Balance Time: 133.79\n",
      "Balance Time Variance: 430.23\n",
      "\n",
      "Metrics Debug at Episode 9000:\n",
      "Recent Average Reward: 815.84\n",
      "Average Balance Time: 815.84\n",
      "Balance Time Variance: 934865.49\n",
      "\n",
      "Completed testing baseline\n",
      "Final average reward: 1158.24\n",
      "Final average balance time: 1158.24\n",
      "Saved plot: performance_comparison_20250208_123658.png in PerformanceExperiment/experiment_20250208_101230/run_4\n",
      "Saved metrics table: metrics_run_4_20250208_123659.png in PerformanceExperiment/experiment_20250208_101230/run_4\n",
      "Saved metrics table: final_results_20250208_123659.png in PerformanceExperiment/experiment_20250208_101230\n",
      "\n",
      "Experiment results saved in: PerformanceExperiment/experiment_20250208_101230\n"
     ]
    }
   ],
   "source": [
    "confidenceIntervals, allResults, resultsTable = runMultipleExperiments(\n",
    "    numRuns=4,\n",
    "    episodes=10000,\n",
    "    changeInterval=5000\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
