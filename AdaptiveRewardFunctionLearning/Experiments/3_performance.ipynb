{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering Adding:\n",
    "- Confidence intervals for performance metrics (Most papers only really seem to go this far)\n",
    "- Statistical significance tests between different approaches\n",
    "- Variance analysis across multiple runs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "\n",
    "import optuna\n",
    "print(optuna.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **This experiment is investigating the performance of an adaptive reward function to state of the art reward functions in environments with environmentally variable changes.**\n",
    "\n",
    "-> Is there a statisitcally significant improvement in performance over time in this varying environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "current_dir = os.getcwd()  \n",
    "project_root = str(Path(current_dir).parent.parent)\n",
    "sys.path.append(project_root)\n",
    "\n",
    "\n",
    "# Initialize environment and device\n",
    "from AdaptiveRewardFunctionLearning.Prompts.prompts import device, apiKey,modelName\n",
    "\n",
    "#Cu stomCartPoleEnv\n",
    "from RLEnvironment.env import CustomCartPoleEnv\n",
    "#RewardUpdateSystem\n",
    "from AdaptiveRewardFunctionLearning.RewardGeneration.rewardCritic import RewardUpdateSystem\n",
    "#DQLearningAgent\n",
    "from RLEnvironment.training.agent import DQLearningAgent\n",
    "from RLEnvironment.training.training import trainDQLearning\n",
    "\n",
    "#DynamicRewardFunction\n",
    "from AdaptiveRewardFunctionLearning.RewardGeneration.rewardCodeGeneration import stabilityReward, efficiencyReward, dynamicRewardFunction\n",
    "\n",
    "#import\n",
    "from AdaptiveRewardFunctionLearning.Visualisation.trainingTestFunctions import (\n",
    "    runEpisode,\n",
    "    detectJumps,\n",
    "    analyzeRewardSensibility,\n",
    "    performUpdate,\n",
    "    updateCompositeRewardFunction,\n",
    "    plotExperimentResults,\n",
    "    savePlot\n",
    ")\n",
    "\n",
    "# Import new reward functions\n",
    "from AdaptiveRewardFunctionLearning.RewardGeneration.cartpole_energy_reward import EnergyBasedRewardFunction\n",
    "\n",
    "\n",
    "# from AdaptiveRewardFunctionLearning.RewardGeneration.cartpole_meta_learning import meta_learning_cartpole\n",
    "# from AdaptiveRewardFunctionLearning.RewardGeneration.reward_meta_learning import RewardFunctionMetaLearner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**State of the Art Reward Functions with Reference to Papers**\n",
    "\n",
    "**Potential-based Reward Shaping (PBRS):**\n",
    "```python\n",
    "def potentialBasedRewardShaping(observation, action):\n",
    "    x, xDot, angle, angleDot = observation\n",
    "    gamma = 0.99  # Example discount factor\n",
    "\n",
    "    def phi(x, xDot, angle, angleDot):\n",
    "        # Example potential function\n",
    "        return -abs(x) - abs(angle)\n",
    "\n",
    "    current_potential = phi(x, xDot, angle, angleDot)\n",
    "    next_potential = phi(x + xDot, angle + angleDot, xDot, angleDot)  # Simplified next state\n",
    "    return float(gamma * next_potential - current_potential)\n",
    "```\n",
    "\n",
    "Paper: \"Potential-based Shaping in Model-based Reinforcement Learning\"\n",
    "\n",
    "Link: https://cdn.aaai.org/AAAI/2008/AAAI08-096.pdf\n",
    "\n",
    "\n",
    "**Parameterized Reward Shaping:**\n",
    "```python\n",
    "def parameterizedRewardShaping(observation, action):\n",
    "    x, xDot, angle, angleDot = observation\n",
    "    original_reward = 1.0  # Assuming default CartPole reward\n",
    "\n",
    "    def f(x, xDot, angle, angleDot):\n",
    "        # Example shaping reward function\n",
    "        return -abs(angle)\n",
    "\n",
    "    def z_phi(x, xDot, angle, angleDot):\n",
    "        # Example shaping weight function\n",
    "        return 0.5\n",
    "\n",
    "    shaping_reward = f(x, xDot, angle, angleDot)\n",
    "    shaping_weight = z_phi(x, xDot, angle, angleDot)\n",
    "    return float(original_reward + shaping_weight * shaping_reward)\n",
    "```\n",
    "\n",
    "Paper: \"Learning to Utilize Shaping Rewards: A New Approach of Reward Shaping\"\n",
    "\n",
    "Link: http://arxiv.org/pdf/2011.02669.pdf\n",
    "\n",
    "\n",
    "**Energy Based Reward Function - Physics Based**\n",
    "\n",
    "```python\n",
    "def energyBasedReward(observation, action):\n",
    "    x, xDot, angle, angleDot = observation\n",
    "    \n",
    "    # Calculate kinetic and potential energy components\n",
    "    kineticEnergy = 0.5 * (xDot**2 + angleDot**2)\n",
    "    potentialEnergy = 9.8 * (1 + cos(angle))  # g * h, where h depends on angle\n",
    "    \n",
    "    # Reward is inverse of total energy (less energy = more stable = better reward)\n",
    "    energyPenalty = -(kineticEnergy + potentialEnergy)\n",
    "    return float(1.0 + 0.1 * energyPenalty)  # Base reward plus energy term\n",
    "```\n",
    "\n",
    "Paper: \"Energy-Based Control for Safe Robot Learning\" (2019)\n",
    "\n",
    "Link: https://ieeexplore.ieee.org/document/8794207\n",
    "\n",
    "\n",
    "**Baseline Reward Function:**\n",
    "```python\n",
    "def baselineCartPoleReward(observation, action):\n",
    "    return 1.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Performance Experiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize reward functions and meta-learners\n",
    "energy_reward = EnergyBasedRewardFunction(mass_cart=1.0, mass_pole=0.1, length=0.5, gravity=9.8)\n",
    "# meta_reward = RewardFunctionMetaLearner(state_dim=4, action_dim=1)  # CartPole has 4 state dims, 1 action dim\n",
    "\n",
    "# def potentialBasedRewardShaping(observation, action):\n",
    "#     \"\"\"Advanced potential-based reward shaping using meta-learning\"\"\"\n",
    "#     reward_func = meta_reward.generate_reward_function()\n",
    "#     return float(reward_func(observation, action))\n",
    "\n",
    "# def parameterizedRewardShaping(observation, action):\n",
    "#     \"\"\"Meta-learning based parameterized reward shaping\"\"\"\n",
    "#     # Use meta-learning framework for parameter optimization\n",
    "#     learner = meta_learning_cartpole()\n",
    "#     return float(learner.parameterized_reward(observation, action))\n",
    "\n",
    "def energyBasedReward(observation, action):\n",
    "    \"\"\"Enhanced physics-based energy reward\"\"\"\n",
    "    return float(energy_reward.compute_reward(observation, action))\n",
    "\n",
    "\n",
    "def potentialBasedReward(observation, action):\n",
    "    \"\"\"Potential-based reward shaping for CartPole  - This one is not dynamic\"\"\"\n",
    "    x, x_dot, theta, theta_dot = observation\n",
    "    gamma = 0.99\n",
    "    \n",
    "    def potential(state):\n",
    "        # Potential function based on cart position and pole angle\n",
    "        # Higher potential for centered cart and upright pole\n",
    "        cart_potential = -(state[0] ** 2)  # Penalize distance from center\n",
    "        angle_potential = -((state[2] ** 2))  # Penalize angle from vertical\n",
    "        velocity_potential = -(state[1] ** 2)  # Penalize high velocities\n",
    "        ang_velocity_potential = -(state[3] ** 2)  # Penalize high angular velocities\n",
    "        \n",
    "        return cart_potential + 2*angle_potential + velocity_potential + ang_velocity_potential\n",
    "\n",
    "    current_potential = potential(observation)\n",
    "    next_potential = potential([x + x_dot, x_dot, theta + theta_dot, theta_dot])\n",
    "    \n",
    "    # PBRS formula: γΦ(s') - Φ(s)\n",
    "    shaped_reward = gamma * next_potential - current_potential\n",
    "    \n",
    "    return 1.0 + shaped_reward\n",
    "\n",
    "\n",
    "def baselineReward(observation, action):\n",
    "    \"\"\"Standard baseline reward\"\"\"\n",
    "    return 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runPerformanceComparisonTest(\n",
    "    episodes=1000, \n",
    "    changeInterval=500, \n",
    "    lengthchanges=[0.5, 1.5],\n",
    "    mass_cart=1.0,\n",
    "    mass_pole=0.1,\n",
    "    initial_length=0.5,\n",
    "    gravity=9.8,\n",
    "    seed=42\n",
    "):\n",
    "    print(f\"Starting Performance Comparison Test with seed {seed}...\")\n",
    "    \n",
    "    # Set all random seeds\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    def create_fresh_env():\n",
    "        # Helper function to create fresh environment with consistent settings\n",
    "        env = gym.make('CartPole-v1', max_episode_steps=20000, render_mode=None)\n",
    "        env.action_space.seed(seed)\n",
    "        env.observation_space.seed(seed)\n",
    "        env.reset(seed=seed)\n",
    "        env = CustomCartPoleEnv(env, numComponents=2)\n",
    "        env.setEnvironmentParameters(masscart=mass_cart, length=lengthchanges[0], gravity=gravity)\n",
    "        return env\n",
    "    \n",
    "    def create_dqn_agent(env, device):\n",
    "        return DQLearningAgent(\n",
    "            env=env, \n",
    "            stateSize=4, \n",
    "            actionSize=2, \n",
    "            device=device,\n",
    "            learningRate=0.0005,  # Reduced from 0.001\n",
    "            discountFactor=0.99,\n",
    "            epsilon=1.0,\n",
    "            epsilonDecay=0.999,  # Slower decay\n",
    "            epsilonMin=0.01,\n",
    "            replayBufferSize=100000,\n",
    "            batchSize=32,  # Back to original\n",
    "            targetUpdateFreq=200  # Much less frequent updates\n",
    "        )\n",
    "    \n",
    "    # Initialize energy-based reward function\n",
    "    energy_reward = EnergyBasedRewardFunction(\n",
    "        mass_cart=mass_cart, \n",
    "        mass_pole=mass_pole, \n",
    "        length=initial_length, \n",
    "        gravity=gravity\n",
    "    )\n",
    "    \n",
    "    # Create initial environment\n",
    "    env = create_fresh_env()\n",
    "    \n",
    "    # Define reward function configurations\n",
    "    rewardfunctions = {\n",
    "        'adaptivereward': {\n",
    "            'agent': None,  # Will be created fresh for each test\n",
    "            'updatesystem': RewardUpdateSystem(apiKey, modelName),\n",
    "            'rewardfunction': None,\n",
    "            'update_method': 'llm'\n",
    "        },\n",
    "        'pbrs': {\n",
    "            'agent': None,  # Will be created fresh for each test\n",
    "            'updatesystem': None,\n",
    "            'rewardfunction': potentialBasedReward,\n",
    "            'update_method': None\n",
    "        },\n",
    "        'energy_based': {\n",
    "            'agent': None,  # Will be created fresh for each test\n",
    "            'updatesystem': energy_reward,\n",
    "            'rewardfunction': energy_reward.compute_reward,\n",
    "            'update_method': 'physics'\n",
    "        },\n",
    "        'baseline': {\n",
    "            'agent': None,  # Will be created fresh for each test\n",
    "            'updatesystem': None,\n",
    "            'rewardfunction': baselineReward,\n",
    "            'update_method': None\n",
    "        }\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "    test_order = ['adaptivereward', 'energy_based', 'baseline', 'pbrs']\n",
    "    \n",
    "    # Test each reward function\n",
    "    for rewardname in test_order:\n",
    "        print(f\"\\nTesting reward function: {rewardname}\")\n",
    "        \n",
    "        # Create fresh environment and agent for each test\n",
    "        env = create_fresh_env()\n",
    "        rewardfunctions[rewardname]['agent'] = create_dqn_agent(env, device)\n",
    "        rewardinfo = rewardfunctions[rewardname]\n",
    "        \n",
    "        # Reset length index for each test\n",
    "        currentlengthidx = 0\n",
    "        \n",
    "        if rewardname == 'adaptivereward':\n",
    "            # Initialize both components for adaptive reward\n",
    "            env.setComponentReward(1, stabilityReward)\n",
    "            env.setComponentReward(2, efficiencyReward)\n",
    "            rewardinfo['updatesystem'].lastUpdateEpisode = 0\n",
    "        else:\n",
    "            env.setRewardFunction(rewardinfo['rewardfunction'])\n",
    "        \n",
    "        episoderewards = []\n",
    "        episodebalancetimes = []\n",
    "        rewardchangeepisodes = []\n",
    "        \n",
    "        def onEpisodeEnd(env, updatesystem, episode, reward, steps):\n",
    "            nonlocal episoderewards, episodebalancetimes, rewardchangeepisodes, currentlengthidx\n",
    "            \n",
    "            # Record episode results\n",
    "            episoderewards.append(reward)\n",
    "            episodebalancetimes.append(steps)\n",
    "            \n",
    "            # Create metrics dictionary\n",
    "            metrics = {\n",
    "                'currentEpisode': episode,\n",
    "                'recentRewards': episoderewards[-100:] if len(episoderewards) > 100 else episoderewards,\n",
    "                'averageBalanceTime': np.mean(episodebalancetimes[-100:]) if episodebalancetimes else 0,\n",
    "                'balanceTimeVariance': np.var(episodebalancetimes[-100:]) if len(episodebalancetimes) > 1 else 0\n",
    "            }\n",
    "            \n",
    "            # Debug print for metrics\n",
    "            if episode % 1000 == 0:\n",
    "                print(f\"\\nMetrics Debug at Episode {episode}:\")\n",
    "                print(f\"Recent Average Reward: {np.mean(metrics['recentRewards']):.2f}\")\n",
    "                print(f\"Average Balance Time: {metrics['averageBalanceTime']:.2f}\")\n",
    "                print(f\"Balance Time Variance: {metrics['balanceTimeVariance']:.2f}\")\n",
    "                \n",
    "                if rewardname == 'adaptivereward' and hasattr(env, 'getCurrentWeights'):\n",
    "                    weights = env.getCurrentWeights()\n",
    "                    print(f\"Component Weights - Stability: {weights['stability']:.2f}, \"\n",
    "                          f\"Efficiency: {weights['efficiency']:.2f}\")\n",
    "            \n",
    "            # Handle LLM updates only for adaptive reward\n",
    "            if rewardname == 'adaptivereward' and updatesystem is not None:\n",
    "                if episode % 1000 == 0:\n",
    "                    print(f\"\\nEpisode {episode} - Time Since Last Update: {episode - updatesystem.lastUpdateEpisode}\")\n",
    "                \n",
    "                for component in range(1, 3):\n",
    "                    updatesystem.targetComponent = component\n",
    "                    if updatesystem.waitingTime(f'component_{component}', metrics, updatesystem.lastUpdateEpisode):\n",
    "                        current_func = env.rewardComponents[f'rewardFunction{component}']\n",
    "                        new_function, updated = updatesystem.validateAndUpdate(current_func)\n",
    "                        \n",
    "                        if updated:\n",
    "                            env.setComponentReward(component, new_function)\n",
    "                            rewardchangeepisodes.append(episode)\n",
    "                            updatesystem.lastUpdateEpisode = episode\n",
    "                            print(f\"✓ LLM update for component {component} at episode {episode}\")\n",
    "            \n",
    "            # Handle physics-based updates\n",
    "            elif rewardinfo['update_method'] == 'physics':\n",
    "                if episode % changeInterval == 0 and episode > 0:\n",
    "                    print(f\"\\nUpdating physics-based reward at episode {episode}\")\n",
    "                    updatesystem.length = lengthchanges[currentlengthidx]\n",
    "                    env.setRewardFunction(updatesystem.compute_reward)\n",
    "                    rewardchangeepisodes.append(episode)\n",
    "                    print(\"✓ Physics-based update completed\")\n",
    "            \n",
    "            # Environment changes\n",
    "            if episode % changeInterval == 0 and episode > 0:\n",
    "                currentlengthidx = (currentlengthidx + 1) % len(lengthchanges)\n",
    "                newlength = lengthchanges[currentlengthidx]\n",
    "                env.setEnvironmentParameters(length=newlength)\n",
    "                print(f\"\\nChanged pole length to: {newlength}m at episode {episode}\")\n",
    "        \n",
    "        # Train the agent\n",
    "        agent, env, rewards = trainDQLearning(\n",
    "            agent=rewardinfo['agent'],\n",
    "            env=env,\n",
    "            numEpisodes=episodes,\n",
    "            updateSystem=rewardinfo['updatesystem'],\n",
    "            onEpisodeEnd=onEpisodeEnd\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        results[rewardname] = {\n",
    "            'rewards': episoderewards,\n",
    "            'balancetimes': episodebalancetimes,\n",
    "            'rewardChanges': rewardchangeepisodes\n",
    "        }\n",
    "        \n",
    "        # Print final performance metrics\n",
    "        print(f\"\\nCompleted testing {rewardname}\")\n",
    "        print(f\"Final average reward: {np.mean(episoderewards[-100:]):.2f}\")\n",
    "        print(f\"Final average balance time: {np.mean(episodebalancetimes[-100:]):.2f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Experiment\n",
    "\n",
    "changeInterval = 20000\n",
    "\n",
    "# results = runPerformanceComparisonTest(\n",
    "#     episodes=40000,  \n",
    "#     changeInterval=changeInterval,\n",
    "#     mass_cart=1.0,\n",
    "#     lengthchanges=[0.3, 0.9]  \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizePerformanceComparison(results, changeInterval, folder_path):\n",
    "    \"\"\"\n",
    "    Create and save performance comparison visualizations\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 15))\n",
    "    \n",
    "    # Color map for different reward functions\n",
    "    colors = ['b', 'g', 'r', 'c', 'm']\n",
    "    \n",
    "    # Plot rewards for each reward function with variance\n",
    "    for idx, (rewardname, rewardresults) in enumerate(results.items()):\n",
    "        rewards = pd.Series(rewardresults['rewards'])\n",
    "        \n",
    "        # Calculate rolling mean and standard deviation for rewards\n",
    "        window = 50\n",
    "        rolling_mean_rewards = rewards.rolling(window=window).mean()\n",
    "        rolling_std_rewards = rewards.rolling(window=window).std()\n",
    "        \n",
    "        # Plot mean line for rewards\n",
    "        ax1.plot(rolling_mean_rewards, \n",
    "                label=f'{rewardname}', \n",
    "                linewidth=2, \n",
    "                color=colors[idx])\n",
    "        \n",
    "        # Plot variance area for rewards\n",
    "        ax1.fill_between(\n",
    "            range(len(rewards)),\n",
    "            rolling_mean_rewards - rolling_std_rewards,\n",
    "            rolling_mean_rewards + rolling_std_rewards,\n",
    "            color=colors[idx],\n",
    "            alpha=0.2\n",
    "        )\n",
    "        \n",
    "        # Add vertical lines for environment changes (red)\n",
    "        change_episodes = range(changeInterval, len(rewards), changeInterval)\n",
    "        for ep in change_episodes:\n",
    "            ax1.axvline(x=ep, color='r', linestyle='--', alpha=0.3,\n",
    "                       label='Environment Change' if ep == change_episodes[0] else None)\n",
    "        \n",
    "        # Add vertical lines for reward function changes (green)\n",
    "        if 'rewardChanges' in rewardresults:\n",
    "            for ep in rewardresults['rewardChanges']:\n",
    "                ax1.axvline(x=ep, color='g', linestyle='--', alpha=0.3,\n",
    "                          label=f'{rewardname} Update' if ep == rewardresults['rewardChanges'][0] else None)\n",
    "    \n",
    "    ax1.set_title('Average Reward Over Time with Variance')\n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.set_ylabel('Reward')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot balance times\n",
    "    for idx, (rewardname, rewardresults) in enumerate(results.items()):\n",
    "        balancetimes = pd.Series(rewardresults['balancetimes'])\n",
    "        \n",
    "        rolling_mean_balance = balancetimes.rolling(window=window).mean()\n",
    "        rolling_std_balance = balancetimes.rolling(window=window).std()\n",
    "        \n",
    "        ax2.plot(rolling_mean_balance,\n",
    "                label=f'{rewardname}', \n",
    "                linewidth=2, \n",
    "                color=colors[idx])\n",
    "        \n",
    "        ax2.fill_between(\n",
    "            range(len(balancetimes)),\n",
    "            rolling_mean_balance - rolling_std_balance,\n",
    "            rolling_mean_balance + rolling_std_balance,\n",
    "            color=colors[idx],\n",
    "            alpha=0.2\n",
    "        )\n",
    "        \n",
    "        # Add vertical lines for environment changes\n",
    "        for ep in change_episodes:\n",
    "            ax2.axvline(x=ep, color='r', linestyle='--', alpha=0.3,\n",
    "                       label='Environment Change' if ep == change_episodes[0] else None)\n",
    "        \n",
    "        # Add vertical lines for reward function changes\n",
    "        if 'rewardChanges' in rewardresults:\n",
    "            for ep in rewardresults['rewardChanges']:\n",
    "                ax2.axvline(x=ep, color='g', linestyle='--', alpha=0.3,\n",
    "                          label=f'{rewardname} Update' if ep == rewardresults['rewardChanges'][0] else None)\n",
    "    \n",
    "    ax2.set_title('Average Balance Time Over Episodes with Variance')\n",
    "    ax2.set_xlabel('Episode')\n",
    "    ax2.set_ylabel('Steps')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    # Plot environment parameters\n",
    "    env_param_history = []\n",
    "    for episode in range(len(next(iter(results.values()))['rewards'])):\n",
    "        idx = (episode // changeInterval) % 2\n",
    "        length = 0.3 if idx == 0 else 0.9  # Alternating between 0.3 and 0.9\n",
    "        env_param_history.append(length)\n",
    "    \n",
    "    ax3.plot(env_param_history, label='Pole Length', color='purple')\n",
    "    ax3.set_title('Environment Parameters Over Episodes')\n",
    "    ax3.set_xlabel('Episode')\n",
    "    ax3.set_ylabel('Pole Length (m)')\n",
    "    ax3.grid(True)\n",
    "    ax3.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plots\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filepath = os.path.join(folder_path, f\"performance_comparison_{timestamp}.png\")\n",
    "    plt.savefig(filepath, bbox_inches='tight', dpi=300)\n",
    "    print(f\"Saved plot: performance_comparison_{timestamp}.png in {folder_path}\")\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "    \n",
    "def calculateStability(rewards):\n",
    "    \"\"\"\n",
    "    Calculate stability score based on reward variance in the last 100 episodes\n",
    "    Lower variance = higher stability\n",
    "    \"\"\"\n",
    "    if len(rewards) < 100:\n",
    "        return 0.0\n",
    "    \n",
    "    last_hundred = rewards[-100:]\n",
    "    mean_reward = np.mean(last_hundred)\n",
    "    if mean_reward == 0:\n",
    "        return 0.0\n",
    "        \n",
    "    # Calculate coefficient of variation (normalized standard deviation)\n",
    "    stability = 1 - (np.std(last_hundred) / mean_reward)\n",
    "    return max(0, min(1, stability))  # Normalize between 0 and 1\n",
    "\n",
    "def calculateConvergenceTime(rewards, threshold=195, window=50):\n",
    "    \"\"\"\n",
    "    Calculate the number of episodes needed to reach and maintain a certain performance\n",
    "    threshold for a given window of episodes\n",
    "    \"\"\"\n",
    "    if len(rewards) < window:\n",
    "        return len(rewards)\n",
    "    \n",
    "    rolling_mean = pd.Series(rewards).rolling(window).mean()\n",
    "    \n",
    "    for episode in range(window, len(rewards)):\n",
    "        if rolling_mean[episode] >= threshold:\n",
    "            # Check if performance is maintained\n",
    "            maintained = all(avg >= threshold * 0.9 for avg in rolling_mean[episode:episode+window])\n",
    "            if maintained:\n",
    "                return episode\n",
    "    \n",
    "    return len(rewards)  # If never converged, return total episodes\n",
    "\n",
    "def calculatePerformanceMetrics(results):\n",
    "    \"\"\"Calculate performance metrics for each reward type\"\"\"\n",
    "    metrics = {}\n",
    "    for rewardname, rewardresults in results.items():\n",
    "        metrics[rewardname] = {\n",
    "            'finalavgreward': np.mean(rewardresults['rewards'][-100:]),\n",
    "            'finalavgbalance': np.mean(rewardresults['balancetimes'][-100:]),\n",
    "            'convergencetime': calculateConvergenceTime(rewardresults['rewards']),\n",
    "            'stability': calculateStability(rewardresults['rewards'])\n",
    "        }\n",
    "    return pd.DataFrame(metrics).T\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def saveMetricsTable(metrics, filename, folder_path):\n",
    "    \"\"\"Save metrics table to specified folder\"\"\"\n",
    "    # Create figure for metrics table\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Create table with formatted metrics\n",
    "    table = ax.table(\n",
    "        cellText=metrics.values.round(3),\n",
    "        colLabels=metrics.columns,\n",
    "        rowLabels=metrics.index,\n",
    "        cellLoc='center',\n",
    "        loc='center'\n",
    "    )\n",
    "    \n",
    "    # Adjust font size and scaling\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(9)\n",
    "    table.scale(1.2, 1.5)\n",
    "    \n",
    "    plt.title(\"Performance Metrics Comparison\")\n",
    "    \n",
    "    # Save with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filepath = os.path.join(folder_path, f\"{filename}_{timestamp}.png\")\n",
    "    plt.savefig(filepath, bbox_inches='tight', dpi=300)\n",
    "    print(f\"Saved metrics table: {filename}_{timestamp}.png in {folder_path}\")\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize the results\n",
    "# visualizePerformanceComparison(results,changeInterval)\n",
    "\n",
    "\n",
    "# # Calculate and display the metrics\n",
    "# metrics = calculatePerformanceMetrics(results)\n",
    "# print(\"\\nPerformance Metrics:\")\n",
    "# print(metrics)\n",
    "    \n",
    "# Call the function\n",
    "# saveMetricsTable(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple runs to generate confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createExperimentFolder():\n",
    "    \"\"\"Create a timestamped folder for experiment results\"\"\"\n",
    "    from datetime import datetime\n",
    "    import os\n",
    "    \n",
    "    # Create base experiment folder if it doesn't exist\n",
    "    if not os.path.exists(\"PerformanceExperiment\"):\n",
    "        os.makedirs(\"PerformanceExperiment\")\n",
    "    \n",
    "    # Create timestamped subfolder\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    experiment_folder = os.path.join(\"PerformanceExperiment\", f\"experiment_{timestamp}\")\n",
    "    os.makedirs(experiment_folder)\n",
    "    \n",
    "    return experiment_folder\n",
    "\n",
    "def savePlot(fig, filename, folder_path):\n",
    "    \"\"\"Save plot to specified folder with timestamp\"\"\"\n",
    "    from datetime import datetime\n",
    "    import os\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filepath = os.path.join(folder_path, f\"{filename}_{timestamp}.png\")\n",
    "    fig.savefig(filepath, bbox_inches='tight', dpi=300)\n",
    "    print(f\"Saved plot: {filename}_{timestamp}.png in {folder_path}\")\n",
    "    \n",
    "def saveMetricsTable(metrics, filename, folder_path):\n",
    "    \"\"\"Save metrics table to specified folder\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    from datetime import datetime\n",
    "    import os\n",
    "    \n",
    "    # Style the DataFrame for visualization\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    table = ax.table(cellText=metrics.values,\n",
    "                    colLabels=metrics.columns,\n",
    "                    rowLabels=metrics.index,\n",
    "                    cellLoc='center',\n",
    "                    loc='center')\n",
    "    \n",
    "    # Adjust font size and scaling\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(9)\n",
    "    table.scale(1.2, 1.5)\n",
    "    \n",
    "    plt.title(\"Performance Metrics Comparison\")\n",
    "    \n",
    "    # Save with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filepath = os.path.join(folder_path, f\"{filename}_{timestamp}.png\")\n",
    "    plt.savefig(filepath, bbox_inches='tight', dpi=300)\n",
    "    print(f\"Saved metrics table: {filename}_{timestamp}.png in {folder_path}\")\n",
    "    plt.close()\n",
    "\n",
    "def runMultipleExperiments(numRuns=4, episodes=40000, changeInterval=20000):\n",
    "    \"\"\"\n",
    "    Run multiple experiments and save results in organized folders\n",
    "    \"\"\"\n",
    "    # Create main experiment folder\n",
    "    experiment_folder = createExperimentFolder()\n",
    "    \n",
    "    allResults = []\n",
    "    allMetrics = []\n",
    "    aggregatedMetrics = {}\n",
    "    \n",
    "    # Create run folders\n",
    "    for run in range(numRuns):\n",
    "        print(f\"\\nStarting Run {run + 1}/{numRuns}\")\n",
    "        \n",
    "        # Create folder for this run\n",
    "        run_folder = os.path.join(experiment_folder, f\"run_{run + 1}\")\n",
    "        os.makedirs(run_folder)\n",
    "        \n",
    "        # Run experiment\n",
    "        results = runPerformanceComparisonTest(\n",
    "            episodes=episodes,\n",
    "            changeInterval=changeInterval,\n",
    "            mass_cart=1.0,\n",
    "            lengthchanges=[0.3, 0.9]\n",
    "        )\n",
    "        \n",
    "        # Calculate metrics for this run\n",
    "        metrics = calculatePerformanceMetrics(results)\n",
    "        \n",
    "        # Store results\n",
    "        allResults.append(results)\n",
    "        allMetrics.append(metrics)\n",
    "        \n",
    "        # Store metrics by reward type\n",
    "        for idx, row in metrics.iterrows():\n",
    "            if idx not in aggregatedMetrics:\n",
    "                aggregatedMetrics[idx] = []\n",
    "            aggregatedMetrics[idx].append(row.to_dict())\n",
    "        \n",
    "        # Visualize and save individual run results\n",
    "        visualizePerformanceComparison(results, changeInterval, run_folder)\n",
    "        saveMetricsTable(metrics, f\"metrics_run_{run + 1}\", run_folder)\n",
    "    \n",
    "    # Calculate confidence intervals (95%)\n",
    "    confidenceIntervals = {}\n",
    "    resultsTable = pd.DataFrame()\n",
    "    \n",
    "    for rewardType, metrics_list in aggregatedMetrics.items():\n",
    "        metrics_df = pd.DataFrame(metrics_list)\n",
    "        means = metrics_df.mean()\n",
    "        cis = 1.96 * metrics_df.std() / np.sqrt(numRuns)\n",
    "        \n",
    "        # Create row for this reward type\n",
    "        resultsTable.loc[rewardType, 'Average Reward'] = f\"{means['finalavgreward']:.2f} ± {cis['finalavgreward']:.2f}\"\n",
    "        resultsTable.loc[rewardType, 'Average Balance'] = f\"{means['finalavgbalance']:.2f} ± {cis['finalavgbalance']:.2f}\"\n",
    "        resultsTable.loc[rewardType, 'Convergence Time'] = f\"{means['convergencetime']:.2f} ± {cis['convergencetime']:.2f}\"\n",
    "        resultsTable.loc[rewardType, 'Stability'] = f\"{means['stability']:.2f} ± {cis['stability']:.2f}\"\n",
    "        \n",
    "        # Store the raw values for possible further analysis\n",
    "        confidenceIntervals[rewardType] = {\n",
    "            'finalavgreward': {'mean': means['finalavgreward'], 'ci': cis['finalavgreward']},\n",
    "            'finalavgbalance': {'mean': means['finalavgbalance'], 'ci': cis['finalavgbalance']},\n",
    "            'convergencetime': {'mean': means['convergencetime'], 'ci': cis['convergencetime']},\n",
    "            'stability': {'mean': means['stability'], 'ci': cis['stability']}\n",
    "        }\n",
    "    \n",
    "    # Save aggregate statistics\n",
    "    with open(os.path.join(experiment_folder, \"aggregate_statistics.txt\"), \"w\") as f:\n",
    "        f.write(\"Aggregate Statistics:\\n\")\n",
    "        for rewardType, metrics in confidenceIntervals.items():\n",
    "            f.write(f\"\\n{rewardType}:\\n\")\n",
    "            for metric, values in metrics.items():\n",
    "                f.write(f\"{metric}: {values['mean']:.2f} ± {values['ci']:.2f}\\n\")\n",
    "    \n",
    "    # Save final results table\n",
    "    saveMetricsTable(resultsTable, \"final_results\", experiment_folder)\n",
    "    \n",
    "    print(f\"\\nExperiment results saved in: {experiment_folder}\")\n",
    "    return confidenceIntervals, allResults, resultsTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confidenceIntervals, allResults, resultsTable = runMultipleExperiments(\n",
    "#     numRuns=4,\n",
    "#     episodes=10000,\n",
    "#     changeInterval=5000\n",
    "# )\n",
    "\n",
    "confidenceIntervals, allResults, resultsTable = runMultipleExperiments(\n",
    "    numRuns=1,\n",
    "    episodes=20000,\n",
    "    changeInterval=10000\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
