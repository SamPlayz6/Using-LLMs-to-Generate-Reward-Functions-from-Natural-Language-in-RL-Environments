{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering Adding:\n",
    "- Confidence intervals for performance metrics (Most papers only really seem to go this far)\n",
    "- Statistical significance tests between different approaches\n",
    "- Variance analysis across multiple runs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "\n",
    "import optuna\n",
    "print(optuna.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **This experiment is investigating the performance of an adaptive reward function to state of the art reward functions in environments with environmentally variable changes.**\n",
    "\n",
    "-> Is there a statisitcally significant improvement in performance over time in this varying environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "current_dir = os.getcwd()  \n",
    "project_root = str(Path(current_dir).parent.parent)\n",
    "sys.path.append(project_root)\n",
    "\n",
    "\n",
    "# Initialize environment and device\n",
    "from AdaptiveRewardFunctionLearning.Prompts.prompts import device, apiKey,modelName\n",
    "\n",
    "#Cu stomCartPoleEnv\n",
    "from RLEnvironment.env import CustomCartPoleEnv\n",
    "#RewardUpdateSystem\n",
    "from AdaptiveRewardFunctionLearning.RewardGeneration.rewardCritic import RewardUpdateSystem\n",
    "#DQLearningAgent\n",
    "from RLEnvironment.training.agent import DQLearningAgent\n",
    "from RLEnvironment.training.training import trainDQLearning\n",
    "\n",
    "#DynamicRewardFunction\n",
    "from AdaptiveRewardFunctionLearning.RewardGeneration.rewardCodeGeneration import stabilityReward, efficiencyReward, dynamicRewardFunction\n",
    "\n",
    "#import\n",
    "from AdaptiveRewardFunctionLearning.Visualisation.trainingTestFunctions import (\n",
    "    runEpisode,\n",
    "    detectJumps,\n",
    "    analyzeRewardSensibility,\n",
    "    performUpdate,\n",
    "    updateCompositeRewardFunction,\n",
    "    plotExperimentResults,\n",
    "    savePlot\n",
    ")\n",
    "\n",
    "# Import new reward functions\n",
    "from AdaptiveRewardFunctionLearning.RewardGeneration.cartpole_energy_reward import EnergyBasedRewardFunction\n",
    "\n",
    "\n",
    "# from AdaptiveRewardFunctionLearning.RewardGeneration.cartpole_meta_learning import meta_learning_cartpole\n",
    "# from AdaptiveRewardFunctionLearning.RewardGeneration.reward_meta_learning import RewardFunctionMetaLearner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**State of the Art Reward Functions with Reference to Papers**\n",
    "\n",
    "**Potential-based Reward Shaping (PBRS):**\n",
    "```python\n",
    "def potentialBasedRewardShaping(observation, action):\n",
    "    x, xDot, angle, angleDot = observation\n",
    "    gamma = 0.99  # Example discount factor\n",
    "\n",
    "    def phi(x, xDot, angle, angleDot):\n",
    "        # Example potential function\n",
    "        return -abs(x) - abs(angle)\n",
    "\n",
    "    current_potential = phi(x, xDot, angle, angleDot)\n",
    "    next_potential = phi(x + xDot, angle + angleDot, xDot, angleDot)  # Simplified next state\n",
    "    return float(gamma * next_potential - current_potential)\n",
    "```\n",
    "\n",
    "Paper: \"Potential-based Shaping in Model-based Reinforcement Learning\"\n",
    "\n",
    "Link: https://cdn.aaai.org/AAAI/2008/AAAI08-096.pdf\n",
    "\n",
    "\n",
    "**Parameterized Reward Shaping:**\n",
    "```python\n",
    "def parameterizedRewardShaping(observation, action):\n",
    "    x, xDot, angle, angleDot = observation\n",
    "    original_reward = 1.0  # Assuming default CartPole reward\n",
    "\n",
    "    def f(x, xDot, angle, angleDot):\n",
    "        # Example shaping reward function\n",
    "        return -abs(angle)\n",
    "\n",
    "    def z_phi(x, xDot, angle, angleDot):\n",
    "        # Example shaping weight function\n",
    "        return 0.5\n",
    "\n",
    "    shaping_reward = f(x, xDot, angle, angleDot)\n",
    "    shaping_weight = z_phi(x, xDot, angle, angleDot)\n",
    "    return float(original_reward + shaping_weight * shaping_reward)\n",
    "```\n",
    "\n",
    "Paper: \"Learning to Utilize Shaping Rewards: A New Approach of Reward Shaping\"\n",
    "\n",
    "Link: http://arxiv.org/pdf/2011.02669.pdf\n",
    "\n",
    "\n",
    "**Energy Based Reward Function - Physics Based**\n",
    "\n",
    "```python\n",
    "def energyBasedReward(observation, action):\n",
    "    x, xDot, angle, angleDot = observation\n",
    "    \n",
    "    # Calculate kinetic and potential energy components\n",
    "    kineticEnergy = 0.5 * (xDot**2 + angleDot**2)\n",
    "    potentialEnergy = 9.8 * (1 + cos(angle))  # g * h, where h depends on angle\n",
    "    \n",
    "    # Reward is inverse of total energy (less energy = more stable = better reward)\n",
    "    energyPenalty = -(kineticEnergy + potentialEnergy)\n",
    "    return float(1.0 + 0.1 * energyPenalty)  # Base reward plus energy term\n",
    "```\n",
    "\n",
    "Paper: \"Energy-Based Control for Safe Robot Learning\" (2019)\n",
    "\n",
    "Link: https://ieeexplore.ieee.org/document/8794207\n",
    "\n",
    "\n",
    "**Baseline Reward Function:**\n",
    "```python\n",
    "def baselineCartPoleReward(observation, action):\n",
    "    return 1.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Performance Experiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize reward functions and meta-learners\n",
    "energy_reward = EnergyBasedRewardFunction(mass_cart=1.0, mass_pole=0.1, length=0.5, gravity=9.8)\n",
    "# meta_reward = RewardFunctionMetaLearner(state_dim=4, action_dim=1)  # CartPole has 4 state dims, 1 action dim\n",
    "\n",
    "# def potentialBasedRewardShaping(observation, action):\n",
    "#     \"\"\"Advanced potential-based reward shaping using meta-learning\"\"\"\n",
    "#     reward_func = meta_reward.generate_reward_function()\n",
    "#     return float(reward_func(observation, action))\n",
    "\n",
    "# def parameterizedRewardShaping(observation, action):\n",
    "#     \"\"\"Meta-learning based parameterized reward shaping\"\"\"\n",
    "#     # Use meta-learning framework for parameter optimization\n",
    "#     learner = meta_learning_cartpole()\n",
    "#     return float(learner.parameterized_reward(observation, action))\n",
    "\n",
    "def energyBasedReward(observation, action):\n",
    "    \"\"\"Enhanced physics-based energy reward\"\"\"\n",
    "    return float(energy_reward.compute_reward(observation, action))\n",
    "\n",
    "\n",
    "def potentialBasedReward(observation, action):\n",
    "    \"\"\"Potential-based reward shaping for CartPole  - This one is not dynamic\"\"\"\n",
    "    x, x_dot, theta, theta_dot = observation\n",
    "    gamma = 0.99\n",
    "    \n",
    "    def potential(state):\n",
    "        # Potential function based on cart position and pole angle\n",
    "        # Higher potential for centered cart and upright pole\n",
    "        cart_potential = -(state[0] ** 2)  # Penalize distance from center\n",
    "        angle_potential = -((state[2] ** 2))  # Penalize angle from vertical\n",
    "        velocity_potential = -(state[1] ** 2)  # Penalize high velocities\n",
    "        ang_velocity_potential = -(state[3] ** 2)  # Penalize high angular velocities\n",
    "        \n",
    "        return cart_potential + 2*angle_potential + velocity_potential + ang_velocity_potential\n",
    "\n",
    "    current_potential = potential(observation)\n",
    "    next_potential = potential([x + x_dot, x_dot, theta + theta_dot, theta_dot])\n",
    "    \n",
    "    # PBRS formula: γΦ(s') - Φ(s)\n",
    "    shaped_reward = gamma * next_potential - current_potential\n",
    "    \n",
    "    return 1.0 + shaped_reward\n",
    "\n",
    "\n",
    "def baselineReward(observation, action):\n",
    "    \"\"\"Standard baseline reward\"\"\"\n",
    "    return 1.0"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def runPerformanceComparisonTest(\n    episodes=1000, \n    changeInterval=500, \n    lengthchanges=[0.5, 1.5],\n    mass_cart=1.0,\n    mass_pole=0.1,\n    initial_length=0.5,\n    gravity=9.8,\n    seed=42,\n    max_updates_per_component=3  # NEW parameter to limit API calls\n):\n    print(f\"Starting Performance Comparison Test with seed {seed}...\")\n    \n    # Set all random seeds\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    \n    def create_fresh_env():\n        # Helper function to create fresh environment with consistent settings\n        env = gym.make('CartPole-v1', max_episode_steps=20000, render_mode=None)\n        env.action_space.seed(seed)\n        env.observation_space.seed(seed)\n        env.reset(seed=seed)\n        env = CustomCartPoleEnv(env, numComponents=2)\n        env.setEnvironmentParameters(masscart=mass_cart, length=lengthchanges[0], gravity=gravity)\n        return env\n    \n    def create_dqn_agent(env, device):\n        return DQLearningAgent(\n            env=env, \n            stateSize=4, \n            actionSize=2, \n            device=device,\n            learningRate=0.0007,       # Balanced for adaptation and stability\n            discountFactor=0.99,       # Standard value works well\n            epsilon=1.0,\n            epsilonDecay=0.9997,       # Slower decay preserves exploration\n            epsilonMin=0.07,           # Higher minimum exploration rate\n            replayBufferSize=30000,    # Smaller to adapt faster to changes\n            batchSize=48,              # Moderate batch size\n            targetUpdateFreq=125       # More frequent updates\n        )\n    \n    # Initialize energy-based reward function\n    energy_reward = EnergyBasedRewardFunction(\n        mass_cart=mass_cart, \n        mass_pole=mass_pole, \n        length=initial_length, \n        gravity=gravity\n    )\n    \n    # Create initial environment\n    env = create_fresh_env()\n    \n    # Define reward function configurations\n    rewardfunctions = {\n        'adaptivereward': {\n            'agent': None,  # Will be created fresh for each test\n            'updatesystem': RewardUpdateSystem(apiKey, modelName, max_updates_per_run=max_updates_per_component),\n            'rewardfunction': None,\n            'update_method': 'llm'\n        },\n        'pbrs': {\n            'agent': None,  # Will be created fresh for each test\n            'updatesystem': None,\n            'rewardfunction': potentialBasedReward,\n            'update_method': None\n        },\n        'energy_based': {\n            'agent': None,  # Will be created fresh for each test\n            'updatesystem': energy_reward,\n            'rewardfunction': energy_reward.compute_reward,\n            'update_method': 'physics'\n        },\n        'baseline': {\n            'agent': None,  # Will be created fresh for each test\n            'updatesystem': None,\n            'rewardfunction': baselineReward,\n            'update_method': None\n        }\n    }\n\n    results = {}\n    test_order = ['adaptivereward', 'energy_based', 'baseline', 'pbrs']\n    \n    # Test each reward function\n    for rewardname in test_order:\n        print(f\"\\nTesting reward function: {rewardname}\")\n        \n        # Create fresh environment and agent for each test\n        env = create_fresh_env()\n        rewardfunctions[rewardname]['agent'] = create_dqn_agent(env, device)\n        rewardinfo = rewardfunctions[rewardname]\n        \n        # Reset length index for each test\n        currentlengthidx = 0\n        \n        if rewardname == 'adaptivereward':\n            # Initialize both components for adaptive reward\n            env.setComponentReward(1, stabilityReward)\n            env.setComponentReward(2, efficiencyReward)\n            rewardinfo['updatesystem'].lastUpdateEpisode = 0\n            print(f\"Limited to {max_updates_per_component} updates per component\")\n        else:\n            env.setRewardFunction(rewardinfo['rewardfunction'])\n        \n        episoderewards = []\n        episodebalancetimes = []\n        rewardchangeepisodes = []\n        \n        def onEpisodeEnd(env, updatesystem, episode, reward, steps):\n            nonlocal episoderewards, episodebalancetimes, rewardchangeepisodes, currentlengthidx\n            \n            # Record episode results\n            episoderewards.append(reward)\n            episodebalancetimes.append(steps)\n            \n            # Create metrics dictionary\n            metrics = {\n                'currentEpisode': episode,\n                'recentRewards': episoderewards[-100:] if len(episoderewards) > 100 else episoderewards,\n                'averageBalanceTime': np.mean(episodebalancetimes[-100:]) if episodebalancetimes else 0,\n                'balanceTimeVariance': np.var(episodebalancetimes[-100:]) if len(episodebalancetimes) > 1 else 0\n            }\n            \n            # Debug print for metrics\n            if episode % 1000 == 0:\n                print(f\"\\nMetrics Debug at Episode {episode}:\")\n                print(f\"Recent Average Reward: {np.mean(metrics['recentRewards']):.2f}\")\n                print(f\"Average Balance Time: {metrics['averageBalanceTime']:.2f}\")\n                print(f\"Balance Time Variance: {metrics['balanceTimeVariance']:.2f}\")\n                \n                if rewardname == 'adaptivereward' and hasattr(env, 'getCurrentWeights'):\n                    weights = env.getCurrentWeights()\n                    print(f\"Component Weights - Stability: {weights['stability']:.2f}, \"\n                          f\"Efficiency: {weights['efficiency']:.2f}\")\n                \n                # Print update statistics for adaptive reward\n                if rewardname == 'adaptivereward' and hasattr(updatesystem, 'update_counts'):\n                    print(f\"Update counts - Component 1: {updatesystem.update_counts.get(1, 0)}/{updatesystem.max_updates_per_run}, \"\n                          f\"Component 2: {updatesystem.update_counts.get(2, 0)}/{updatesystem.max_updates_per_run}\")\n            \n            # Handle LLM updates only for adaptive reward\n            if rewardname == 'adaptivereward' and updatesystem is not None:\n                if episode % 1000 == 0:\n                    print(f\"\\nEpisode {episode} - Time Since Last Update: {episode - updatesystem.lastUpdateEpisode}\")\n                \n                for component in range(1, 3):\n                    updatesystem.targetComponent = component\n                    if updatesystem.waitingTime(f'component_{component}', metrics, updatesystem.lastUpdateEpisode):\n                        current_func = env.rewardComponents[f'rewardFunction{component}']\n                        new_function, updated = updatesystem.validateAndUpdate(current_func)\n                        \n                        if updated:\n                            env.setComponentReward(component, new_function)\n                            rewardchangeepisodes.append(episode)\n                            updatesystem.lastUpdateEpisode = episode\n                            print(f\"✓ LLM update for component {component} at episode {episode}\")\n            \n            # Handle physics-based updates\n            elif rewardinfo['update_method'] == 'physics':\n                if episode % changeInterval == 0 and episode > 0:\n                    print(f\"\\nUpdating physics-based reward at episode {episode}\")\n                    updatesystem.length = lengthchanges[currentlengthidx]\n                    env.setRewardFunction(updatesystem.compute_reward)\n                    rewardchangeepisodes.append(episode)\n                    print(\"✓ Physics-based update completed\")\n            \n            # Environment changes\n            if episode % changeInterval == 0 and episode > 0:\n                currentlengthidx = (currentlengthidx + 1) % len(lengthchanges)\n                newlength = lengthchanges[currentlengthidx]\n                env.setEnvironmentParameters(length=newlength)\n                print(f\"\\nChanged pole length to: {newlength}m at episode {episode}\")\n        \n        # Train the agent\n        agent, env, rewards = trainDQLearning(\n            agent=rewardinfo['agent'],\n            env=env,\n            numEpisodes=episodes,\n            updateSystem=rewardinfo['updatesystem'],\n            onEpisodeEnd=onEpisodeEnd\n        )\n        \n        # Store results\n        results[rewardname] = {\n            'rewards': episoderewards,\n            'balancetimes': episodebalancetimes,\n            'rewardChanges': rewardchangeepisodes\n        }\n        \n        # Print final performance metrics\n        print(f\"\\nCompleted testing {rewardname}\")\n        print(f\"Final average reward: {np.mean(episoderewards[-100:]):.2f}\")\n        print(f\"Final average balance time: {np.mean(episodebalancetimes[-100:]):.2f}\")\n        \n        # Print update statistics for adaptive reward\n        if rewardname == 'adaptivereward' and 'updatesystem' in rewardinfo and hasattr(rewardinfo['updatesystem'], 'update_counts'):\n            print(f\"Final update counts - Component 1: {rewardinfo['updatesystem'].update_counts.get(1, 0)}/{rewardinfo['updatesystem'].max_updates_per_run}, \"\n                  f\"Component 2: {rewardinfo['updatesystem'].update_counts.get(2, 0)}/{rewardinfo['updatesystem'].max_updates_per_run}\")\n    \n    return results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Experiment\n",
    "\n",
    "changeInterval = 20000\n",
    "\n",
    "# results = runPerformanceComparisonTest(\n",
    "#     episodes=40000,  \n",
    "#     changeInterval=changeInterval,\n",
    "#     mass_cart=1.0,\n",
    "#     lengthchanges=[0.3, 0.9]  \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def visualizePerformanceComparison(results, changeInterval, folder_path):\n    \"\"\"\n    Create and save performance comparison visualizations\n    \"\"\"\n    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 15))\n    \n    # Color map for different reward functions\n    colors = ['b', 'g', 'r', 'c', 'm']\n    \n    # Plot rewards for each reward function with variance\n    for idx, (rewardname, rewardresults) in enumerate(results.items()):\n        rewards = pd.Series(rewardresults['rewards'])\n        \n        # Calculate rolling mean and standard deviation for rewards\n        window = 50\n        rolling_mean_rewards = rewards.rolling(window=window).mean()\n        rolling_std_rewards = rewards.rolling(window=window).std()\n        \n        # Plot mean line for rewards\n        ax1.plot(rolling_mean_rewards, \n                label=f'{rewardname}', \n                linewidth=2, \n                color=colors[idx])\n        \n        # Plot variance area for rewards\n        ax1.fill_between(\n            range(len(rewards)),\n            rolling_mean_rewards - rolling_std_rewards,\n            rolling_mean_rewards + rolling_std_rewards,\n            color=colors[idx],\n            alpha=0.2\n        )\n        \n        # Add vertical lines for environment changes (red)\n        change_episodes = range(changeInterval, len(rewards), changeInterval)\n        for ep in change_episodes:\n            ax1.axvline(x=ep, color='r', linestyle='--', alpha=0.3,\n                       label='Environment Change' if ep == change_episodes[0] else None)\n        \n        # Add vertical lines for reward function changes (green)\n        if 'rewardChanges' in rewardresults:\n            for ep in rewardresults['rewardChanges']:\n                ax1.axvline(x=ep, color='g', linestyle='--', alpha=0.3,\n                          label=f'{rewardname} Update' if ep == rewardresults['rewardChanges'][0] else None)\n    \n    ax1.set_title('Average Reward Over Time with Variance')\n    ax1.set_xlabel('Episode')\n    ax1.set_ylabel('Reward')\n    ax1.legend()\n    ax1.grid(True)\n    \n    # Plot balance times\n    for idx, (rewardname, rewardresults) in enumerate(results.items()):\n        balancetimes = pd.Series(rewardresults['balancetimes'])\n        \n        rolling_mean_balance = balancetimes.rolling(window=window).mean()\n        rolling_std_balance = balancetimes.rolling(window=window).std()\n        \n        ax2.plot(rolling_mean_balance,\n                label=f'{rewardname}', \n                linewidth=2, \n                color=colors[idx])\n        \n        ax2.fill_between(\n            range(len(balancetimes)),\n            rolling_mean_balance - rolling_std_balance,\n            rolling_mean_balance + rolling_std_balance,\n            color=colors[idx],\n            alpha=0.2\n        )\n        \n        # Add vertical lines for environment changes\n        for ep in change_episodes:\n            ax2.axvline(x=ep, color='r', linestyle='--', alpha=0.3,\n                       label='Environment Change' if ep == change_episodes[0] else None)\n        \n        # Add vertical lines for reward function changes\n        if 'rewardChanges' in rewardresults:\n            for ep in rewardresults['rewardChanges']:\n                ax2.axvline(x=ep, color='g', linestyle='--', alpha=0.3,\n                          label=f'{rewardname} Update' if ep == rewardresults['rewardChanges'][0] else None)\n    \n    ax2.set_title('Average Balance Time Over Episodes with Variance')\n    ax2.set_xlabel('Episode')\n    ax2.set_ylabel('Steps')\n    ax2.legend()\n    ax2.grid(True)\n    \n    # Plot environment parameters\n    env_param_history = []\n    for episode in range(len(next(iter(results.values()))['rewards'])):\n        idx = (episode // changeInterval) % 2\n        length = 0.3 if idx == 0 else 0.9  # Alternating between 0.3 and 0.9\n        env_param_history.append(length)\n    \n    ax3.plot(env_param_history, label='Pole Length', color='purple')\n    ax3.set_title('Environment Parameters Over Episodes')\n    ax3.set_xlabel('Episode')\n    ax3.set_ylabel('Pole Length (m)')\n    ax3.grid(True)\n    ax3.legend()\n    \n    plt.tight_layout()\n    \n    # Save plots\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    filepath = os.path.join(folder_path, f\"performance_comparison_{timestamp}.png\")\n    plt.savefig(filepath, bbox_inches='tight', dpi=300)\n    print(f\"Saved plot: performance_comparison_{timestamp}.png in {folder_path}\")\n    plt.close()\n    \n    # Create separate plots for each reward function\n    visualizeSeparateRewardFunctions(results, changeInterval, folder_path)\n\n\ndef visualizeSeparateRewardFunctions(results, changeInterval, folder_path):\n    \"\"\"\n    Create and save separate plots for each reward function\n    \"\"\"\n    # Color map for different reward functions\n    colors = {'adaptivereward': 'b', 'energy_based': 'g', 'baseline': 'r', 'pbrs': 'c'}\n    \n    # Create a 2x2 grid layout for separate plots\n    fig, axs = plt.subplots(2, 2, figsize=(16, 12))\n    axs = axs.flatten()\n    \n    # Plot each reward function in its own subplot\n    for idx, (rewardname, rewardresults) in enumerate(results.items()):\n        ax = axs[idx]\n        \n        # Get data\n        rewards = pd.Series(rewardresults['rewards'])\n        balancetimes = pd.Series(rewardresults['balancetimes'])\n        \n        # Calculate rolling mean and standard deviation\n        window = 50\n        rolling_mean_rewards = rewards.rolling(window=window).mean()\n        rolling_std_rewards = rewards.rolling(window=window).std()\n        rolling_mean_balance = balancetimes.rolling(window=window).mean()\n        \n        # Create twin axis for balance times\n        ax2 = ax.twinx()\n        \n        # Plot rewards\n        color = colors.get(rewardname, 'b')\n        ax.plot(rolling_mean_rewards, \n                label='Reward', \n                linewidth=2, \n                color=color)\n        \n        # Plot variance area for rewards\n        ax.fill_between(\n            range(len(rewards)),\n            rolling_mean_rewards - rolling_std_rewards,\n            rolling_mean_rewards + rolling_std_rewards,\n            color=color,\n            alpha=0.2\n        )\n        \n        # Plot balance times on second y-axis\n        ax2.plot(rolling_mean_balance,\n                label='Balance Time', \n                linewidth=2, \n                color='darkgreen',\n                linestyle='--')\n        \n        # Add vertical lines for environment changes (red)\n        change_episodes = range(changeInterval, len(rewards), changeInterval)\n        for ep in change_episodes:\n            ax.axvline(x=ep, color='r', linestyle='--', alpha=0.3,\n                      label='Environment Change' if ep == change_episodes[0] else None)\n        \n        # Add vertical lines for reward function changes (green)\n        if 'rewardChanges' in rewardresults and rewardresults['rewardChanges']:\n            for ep in rewardresults['rewardChanges']:\n                ax.axvline(x=ep, color='g', linestyle='--', alpha=0.3,\n                          label=f'Update' if ep == rewardresults['rewardChanges'][0] else None)\n        \n        # Add title and labels\n        ax.set_title(f'{rewardname} Performance', fontsize=14)\n        ax.set_xlabel('Episode', fontsize=10)\n        ax.set_ylabel('Reward', fontsize=10)\n        ax2.set_ylabel('Balance Time (steps)', fontsize=10)\n        \n        # Add legends for both axes\n        lines1, labels1 = ax.get_legend_handles_labels()\n        lines2, labels2 = ax2.get_legend_handles_labels()\n        ax.legend(lines1 + lines2, labels1 + labels2, loc='upper left', fontsize=8)\n        \n        ax.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    \n    # Save separate plots\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    filepath = os.path.join(folder_path, f\"separate_reward_functions_{timestamp}.png\")\n    plt.savefig(filepath, bbox_inches='tight', dpi=300)\n    print(f\"Saved plot: separate_reward_functions_{timestamp}.png in {folder_path}\")\n    plt.close()\n    \n    \ndef calculateStability(rewards):\n    \"\"\"\n    Calculate stability score based on reward variance in the last 100 episodes\n    Lower variance = higher stability\n    \"\"\"\n    if len(rewards) < 100:\n        return 0.0\n    \n    last_hundred = rewards[-100:]\n    mean_reward = np.mean(last_hundred)\n    if mean_reward == 0:\n        return 0.0\n        \n    # Calculate coefficient of variation (normalized standard deviation)\n    stability = 1 - (np.std(last_hundred) / mean_reward)\n    return max(0, min(1, stability))  # Normalize between 0 and 1\n\ndef calculateConvergenceTime(rewards, threshold=195, window=50):\n    \"\"\"\n    Calculate the number of episodes needed to reach and maintain a certain performance\n    threshold for a given window of episodes\n    \"\"\"\n    if len(rewards) < window:\n        return len(rewards)\n    \n    rolling_mean = pd.Series(rewards).rolling(window).mean()\n    \n    for episode in range(window, len(rewards)):\n        if rolling_mean[episode] >= threshold:\n            # Check if performance is maintained\n            maintained = all(avg >= threshold * 0.9 for avg in rolling_mean[episode:episode+window])\n            if maintained:\n                return episode\n    \n    return len(rewards)  # If never converged, return total episodes\n\ndef calculatePerformanceMetrics(results):\n    \"\"\"Calculate performance metrics for each reward type\"\"\"\n    metrics = {}\n    for rewardname, rewardresults in results.items():\n        metrics[rewardname] = {\n            'finalavgreward': np.mean(rewardresults['rewards'][-100:]),\n            'finalavgbalance': np.mean(rewardresults['balancetimes'][-100:]),\n            'convergencetime': calculateConvergenceTime(rewardresults['rewards']),\n            'stability': calculateStability(rewardresults['rewards'])\n        }\n    return pd.DataFrame(metrics).T\n    \n    \ndef saveMetricsTable(metrics, filename, folder_path):\n    \"\"\"Save metrics table to specified folder\"\"\"\n    # Create figure for metrics table\n    fig, ax = plt.subplots(figsize=(12, 4))\n    ax.axis('tight')\n    ax.axis('off')\n    \n    # Create table with formatted metrics\n    table = ax.table(\n        cellText=metrics.values.round(3),\n        colLabels=metrics.columns,\n        rowLabels=metrics.index,\n        cellLoc='center',\n        loc='center'\n    )\n    \n    # Adjust font size and scaling\n    table.auto_set_font_size(False)\n    table.set_fontsize(9)\n    table.scale(1.2, 1.5)\n    \n    plt.title(\"Performance Metrics Comparison\")\n    \n    # Save with timestamp\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    filepath = os.path.join(folder_path, f\"{filename}_{timestamp}.png\")\n    plt.savefig(filepath, bbox_inches='tight', dpi=300)\n    print(f\"Saved metrics table: {filename}_{timestamp}.png in {folder_path}\")\n    plt.close()"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def adaptation_metrics(results, changeInterval=5000):\n    \"\"\"\n    Calculate adaptation metrics for different reward approaches:\n    - Recovery time: Episodes needed to recover after an environment change\n    - Performance drop: Percentage drop in reward after environment change\n    - Adaptation effectiveness: How quickly and completely the agent adapts\n    \n    Args:\n        results: Dictionary containing results from different reward approaches\n        changeInterval: Number of episodes between environment changes\n    \n    Returns:\n        DataFrame with adaptation metrics for each reward approach\n    \"\"\"\n    adaptation_data = {}\n    \n    for rewardname, rewarddata in results.items():\n        rewards = rewarddata['rewards']\n        balancetimes = rewarddata['balancetimes']\n        \n        # Find the episodes where environment changes occurred\n        change_episodes = list(range(changeInterval, len(rewards), changeInterval))\n        \n        # Skip if there aren't enough episodes to analyze\n        if not change_episodes or change_episodes[0] >= len(rewards):\n            continue\n            \n        recovery_times = []\n        performance_drops = []\n        adaptation_effectiveness = []\n        \n        for change_ep in change_episodes:\n            # Skip if we don't have enough data after this change\n            if change_ep + changeInterval > len(rewards):\n                continue\n                \n            # Get data before change (baseline performance)\n            pre_change_window = 100  # Last 100 episodes before change\n            pre_change_start = max(0, change_ep - pre_change_window)\n            pre_change_rewards = rewards[pre_change_start:change_ep]\n            pre_change_avg = np.mean(pre_change_rewards) if pre_change_rewards else 0\n            \n            # Get data after change\n            post_change_window = min(changeInterval, len(rewards) - change_ep)\n            post_change_rewards = rewards[change_ep:change_ep + post_change_window]\n            \n            # Calculate immediate performance drop\n            initial_post_avg = np.mean(post_change_rewards[:20]) if len(post_change_rewards) >= 20 else np.mean(post_change_rewards)\n            performance_drop = max(0, (pre_change_avg - initial_post_avg) / pre_change_avg if pre_change_avg > 0 else 0)\n            performance_drops.append(performance_drop)\n            \n            # Calculate recovery time (episodes to reach 90% of pre-change performance)\n            recovery_threshold = 0.9 * pre_change_avg\n            recovery_ep = change_ep\n            for i, reward in enumerate(post_change_rewards):\n                # Use sliding window average for more stable measure\n                window_size = min(10, i + 1)\n                recent_avg = np.mean(post_change_rewards[i - window_size + 1:i + 1])\n                if recent_avg >= recovery_threshold:\n                    recovery_ep = change_ep + i\n                    break\n            \n            recovery_time = recovery_ep - change_ep\n            recovery_times.append(recovery_time)\n            \n            # Calculate adaptation effectiveness (area under recovery curve)\n            # Higher values mean faster and more complete recovery\n            if recovery_time > 0 and pre_change_avg > 0:\n                # Calculate area between actual performance and ideal performance\n                ideal_area = pre_change_avg * recovery_time\n                actual_area = sum(post_change_rewards[:recovery_time])\n                effectiveness = actual_area / ideal_area if ideal_area > 0 else 0\n                adaptation_effectiveness.append(effectiveness)\n        \n        # Store metrics\n        adaptation_data[rewardname] = {\n            'avg_recovery_time': np.mean(recovery_times) if recovery_times else float('nan'),\n            'avg_performance_drop': np.mean(performance_drops) if performance_drops else float('nan'),\n            'avg_adaptation_effectiveness': np.mean(adaptation_effectiveness) if adaptation_effectiveness else float('nan')\n        }\n    \n    return pd.DataFrame(adaptation_data).T\n\ndef analyze_adaptation_speed(results, changeInterval=5000):\n    \"\"\"\n    Analyze how quickly each approach adapts to environment changes,\n    creating a visualization of adaptation speed with confidence intervals.\n    \n    Args:\n        results: List of dictionaries containing multiple runs of results\n        changeInterval: Number of episodes between environment changes\n    \n    Returns:\n        Figure showing adaptation speed for each approach\n    \"\"\"\n    # Combine data from all runs\n    combined_data = {}\n    \n    # First, organize data by reward type and change episode\n    for run_results in results:\n        for rewardname, rewarddata in run_results.items():\n            if rewardname not in combined_data:\n                combined_data[rewardname] = {}\n                \n            rewards = rewarddata['rewards']\n            change_episodes = list(range(changeInterval, len(rewards), changeInterval))\n            \n            for change_ep in change_episodes:\n                if change_ep >= len(rewards):\n                    continue\n                    \n                # Only look at episodes after change, up to next change\n                post_window = min(changeInterval, len(rewards) - change_ep)\n                post_rewards = rewards[change_ep:change_ep + post_window]\n                \n                # Get performance before change as baseline\n                pre_window = 100\n                pre_start = max(0, change_ep - pre_window)\n                pre_rewards = rewards[pre_start:change_ep]\n                pre_avg = np.mean(pre_rewards) if pre_rewards else 0\n                \n                # Normalize post-change rewards as percentage of pre-change performance\n                normalized_rewards = []\n                for r in post_rewards:\n                    normalized = r / pre_avg if pre_avg > 0 else 0\n                    normalized_rewards.append(normalized)\n                \n                # Create or update change episode data\n                change_key = f\"change_{change_ep}\"\n                if change_key not in combined_data[rewardname]:\n                    combined_data[rewardname][change_key] = []\n                    \n                combined_data[rewardname][change_key].append(normalized_rewards)\n    \n    # Create visualization\n    fig, ax = plt.subplots(figsize=(14, 8))\n    \n    # Colors for different reward approaches\n    colors = {\n        'adaptivereward': 'blue',\n        'energy_based': 'green',\n        'baseline': 'red',\n        'pbrs': 'purple'\n    }\n    \n    # Analysis window after change (how many episodes to show)\n    analysis_window = 500\n    \n    # Plot each reward approach\n    for rewardname, change_data in combined_data.items():\n        # Average all change episodes for this reward approach\n        all_normalized = []\n        \n        for change_key, runs in change_data.items():\n            # First ensure all runs have same length by padding shorter ones\n            max_len = min(analysis_window, max(len(run) for run in runs))\n            padded_runs = []\n            for run in runs:\n                if len(run) < max_len:\n                    padded = run + [run[-1]] * (max_len - len(run))  # Pad with last value\n                else:\n                    padded = run[:max_len]\n                padded_runs.append(padded)\n            \n            # Now we can safely calculate means and CIs\n            all_normalized.extend(padded_runs)\n        \n        # Convert to numpy array for easier calculations\n        all_normalized = np.array(all_normalized)\n        \n        # Calculate mean and confidence intervals\n        means = np.mean(all_normalized, axis=0)\n        std_devs = np.std(all_normalized, axis=0)\n        ci = 1.96 * std_devs / np.sqrt(all_normalized.shape[0])  # 95% CI\n        \n        # Plot mean line\n        episodes = np.arange(len(means))\n        ax.plot(episodes, means, label=rewardname, color=colors.get(rewardname, 'black'), linewidth=2)\n        \n        # Plot confidence interval\n        ax.fill_between(episodes, means - ci, means + ci, color=colors.get(rewardname, 'black'), alpha=0.2)\n    \n    # Add horizontal line at 100% (pre-change performance)\n    ax.axhline(y=1.0, color='gray', linestyle='--', alpha=0.7, label='Pre-change level')\n    \n    # Add labels and legend\n    ax.set_title('Adaptation Speed After Environment Changes', fontsize=16)\n    ax.set_xlabel('Episodes After Change', fontsize=14)\n    ax.set_ylabel('Performance (% of pre-change level)', fontsize=14)\n    ax.grid(True, alpha=0.3)\n    ax.legend(fontsize=12)\n    \n    # Set y-axis limits with some padding\n    ax.set_ylim([0, max(1.5, ax.get_ylim()[1])])\n    \n    return fig\n\n# Visualize the results\n# visualizePerformanceComparison(results,changeInterval)\n\n\n# # Calculate and display the metrics\n# metrics = calculatePerformanceMetrics(results)\n# print(\"\\nPerformance Metrics:\")\n# print(metrics)\n    \n# Call the function\n# saveMetricsTable(metrics)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple runs to generate confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def createExperimentFolder():\n    \"\"\"Create a timestamped folder for experiment results\"\"\"\n    from datetime import datetime\n    import os\n    \n    # Create base experiment folder if it doesn't exist\n    if not os.path.exists(\"PerformanceExperiment\"):\n        os.makedirs(\"PerformanceExperiment\")\n    \n    # Create timestamped subfolder\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    experiment_folder = os.path.join(\"PerformanceExperiment\", f\"experiment_{timestamp}\")\n    os.makedirs(experiment_folder)\n    \n    return experiment_folder\n\ndef savePlot(fig, filename, folder_path):\n    \"\"\"Save plot to specified folder with timestamp\"\"\"\n    from datetime import datetime\n    import os\n    \n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    filepath = os.path.join(folder_path, f\"{filename}_{timestamp}.png\")\n    fig.savefig(filepath, bbox_inches='tight', dpi=300)\n    print(f\"Saved plot: {filename}_{timestamp}.png in {folder_path}\")\n\ndef create_reward_over_time_plot_with_variance(results, changeInterval, save_path=None, include_variance=True):\n    \"\"\"\n    Create a reward over time plot with optional variance bands.\n    Shows all reward functions on one plot with clear markers for environment changes.\n    \n    Args:\n        results: Dictionary containing results from each reward approach\n        changeInterval: Number of episodes between environment changes\n        save_path: Directory to save the visualization (optional)\n        include_variance: Whether to include variance bands (default: True)\n    \n    Returns:\n        Matplotlib figure object\n    \"\"\"\n    plt.figure(figsize=(14, 8))\n    \n    # Define distinguishable colors and line styles\n    colors = {\n        'adaptivereward': '#1f77b4',  # blue\n        'energy_based': '#2ca02c',    # green\n        'pbrs': '#ff7f0e',            # orange\n        'baseline': '#d62728'         # red\n    }\n    \n    line_styles = {\n        'adaptivereward': '-',\n        'energy_based': '-.',\n        'pbrs': '--',\n        'baseline': ':'\n    }\n    \n    # Plot each reward function\n    for rewardname, rewardresults in results.items():\n        rewards = rewardresults['rewards']\n        \n        # Apply smoother with larger window for cleaner visualization\n        window = 50\n        smoothed_rewards = pd.Series(rewards).rolling(window=window, min_periods=1).mean()\n        \n        if include_variance:\n            # Calculate rolling standard deviation for variance bands\n            rolling_std = pd.Series(rewards).rolling(window=window, min_periods=1).std()\n            \n            # Create confidence interval (1 standard deviation)\n            upper_bound = smoothed_rewards + rolling_std\n            lower_bound = smoothed_rewards - rolling_std\n            \n            # Fill the area between upper and lower bounds\n            plt.fill_between(\n                range(len(rewards)),\n                lower_bound,\n                upper_bound,\n                alpha=0.2,\n                color=colors.get(rewardname, 'black'),\n                label=f\"{rewardname} variance\" if rewardname == list(results.keys())[0] else None\n            )\n        \n        plt.plot(\n            range(len(rewards)), \n            smoothed_rewards,\n            label=rewardname,\n            color=colors.get(rewardname, 'black'),\n            linestyle=line_styles.get(rewardname, '-'),\n            linewidth=2.5\n        )\n    \n    # Add vertical lines for environment changes\n    change_episodes = list(range(changeInterval, len(next(iter(results.values()))['rewards']), changeInterval))\n    for ep in change_episodes:\n        plt.axvline(x=ep, color='red', linestyle='--', alpha=0.5,\n                   label='Environment Change' if ep == change_episodes[0] else None)\n    \n    # Add annotations for environment changes\n    for i, ep in enumerate(change_episodes):\n        param_value = 0.9 if i % 2 else 0.3  # Alternating pole length (customize as needed)\n        plt.annotate(\n            f\"Length: {param_value}m\",\n            xy=(ep, plt.gca().get_ylim()[1] * 0.95),\n            xytext=(ep + 50, plt.gca().get_ylim()[1] * 0.95),\n            arrowprops=dict(facecolor='black', shrink=0.05, width=1.5, headwidth=8),\n            fontsize=10,\n            bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8)\n        )\n    \n    # Add title and labels\n    title_suffix = \" with Variance Bands\" if include_variance else \"\"\n    plt.title(f'Reward Performance Comparison Across Different Approaches{title_suffix}', fontsize=16)\n    plt.xlabel('Episode', fontsize=14)\n    plt.ylabel('Average Reward (smoothed)', fontsize=14)\n    plt.grid(True, alpha=0.3)\n    \n    # Create a custom legend with larger markers\n    plt.legend(fontsize=12, loc='upper center', bbox_to_anchor=(0.5, -0.15), \n               ncol=3, frameon=True, fancybox=True, shadow=True)\n    \n    # Adjust margins\n    plt.tight_layout()\n    \n    # Save the plot if a path is provided\n    if save_path:\n        # Create the directory if it doesn't exist\n        os.makedirs(save_path, exist_ok=True)\n        \n        # Use a descriptive filename\n        variance_suffix = \"_with_variance\" if include_variance else \"\"\n        filename = f\"reward_over_time{variance_suffix}.png\"\n        \n        # Join the path correctly\n        filepath = os.path.join(save_path, filename)\n        \n        # Save with a good margin setting\n        plt.savefig(filepath, bbox_inches='tight', dpi=300)\n        print(f\"Saved plot to {filepath}\")\n    \n    # Display the plot\n    plt.show()\n    \n    return plt.gcf()  # Return the figure object\n\ndef create_aggregate_reward_plot(all_results, changeInterval, folder_path):\n    \"\"\"\n    Create an aggregate reward plot that shows the average performance across all runs\n    with confidence intervals\n    \"\"\"\n    # First, organize data by reward type\n    reward_data = {}\n    max_episodes = 0\n    \n    # Collect data from all runs\n    for run_results in all_results:\n        for reward_type, results in run_results.items():\n            if reward_type not in reward_data:\n                reward_data[reward_type] = []\n            \n            # Get reward data\n            rewards = pd.Series(results['rewards']).rolling(window=100, min_periods=1).mean()\n            reward_data[reward_type].append(rewards)\n            max_episodes = max(max_episodes, len(rewards))\n    \n    # Create figure\n    plt.figure(figsize=(16, 10))\n    \n    # Define colors and line styles\n    colors = {\n        'adaptivereward': '#1f77b4',  # blue\n        'energy_based': '#2ca02c',    # green\n        'pbrs': '#ff7f0e',            # orange\n        'baseline': '#d62728'         # red\n    }\n    \n    line_styles = {\n        'adaptivereward': '-',\n        'energy_based': '-.',\n        'pbrs': '--',\n        'baseline': ':'\n    }\n    \n    # Plot each reward type\n    for reward_type, runs in reward_data.items():\n        # Make sure all runs have the same length by padding shorter ones\n        padded_runs = []\n        for run in runs:\n            if len(run) < max_episodes:\n                # Pad with NaNs\n                padded = run.reindex(range(max_episodes), fill_value=np.nan)\n            else:\n                padded = run\n            padded_runs.append(padded)\n        \n        # Convert to numpy array for calculations\n        run_data = np.array([run.values for run in padded_runs])\n        \n        # Calculate mean and confidence intervals\n        mean_rewards = np.nanmean(run_data, axis=0)\n        if run_data.shape[0] > 1:  # Only calculate std if we have multiple runs\n            std_rewards = np.nanstd(run_data, axis=0)\n            ci = 1.96 * std_rewards / np.sqrt(run_data.shape[0])  # 95% CI\n        else:\n            ci = np.zeros_like(mean_rewards)\n        \n        # Plot mean line\n        plt.plot(\n            range(len(mean_rewards)),\n            mean_rewards,\n            label=reward_type,\n            color=colors.get(reward_type, 'black'),\n            linestyle=line_styles.get(reward_type, '-'),\n            linewidth=3\n        )\n        \n        # Plot confidence interval\n        plt.fill_between(\n            range(len(mean_rewards)),\n            mean_rewards - ci,\n            mean_rewards + ci,\n            color=colors.get(reward_type, 'black'),\n            alpha=0.2\n        )\n    \n    # Add vertical lines for environment changes\n    change_episodes = list(range(changeInterval, max_episodes, changeInterval))\n    for i, ep in enumerate(change_episodes):\n        # Skip if beyond our data\n        if ep >= max_episodes:\n            continue\n            \n        plt.axvline(\n            x=ep,\n            color='black',\n            linestyle='--',\n            alpha=0.5,\n            label='Environment Change' if i == 0 else None\n        )\n        \n        # Add annotation for the environment change\n        param_value = 0.9 if i % 2 else 0.3  # Alternating pole length\n        y_pos = plt.gca().get_ylim()[1] * 0.95\n        plt.annotate(\n            f\"Length: {param_value}m\",\n            xy=(ep, y_pos),\n            xytext=(ep + max_episodes * 0.02, y_pos),\n            arrowprops=dict(facecolor='black', shrink=0.05, width=1.5, headwidth=8),\n            fontsize=12,\n            bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8)\n        )\n    \n    # Add title and labels\n    plt.title('Aggregate Reward Performance Across All Runs (with 95% CI)', fontsize=18)\n    plt.xlabel('Episode', fontsize=16)\n    plt.ylabel('Average Reward (smoothed)', fontsize=16)\n    plt.grid(True, alpha=0.3)\n    \n    # Create a custom legend\n    plt.legend(\n        fontsize=14,\n        loc='upper center',\n        bbox_to_anchor=(0.5, -0.1),\n        ncol=4,\n        frameon=True,\n        fancybox=True,\n        shadow=True\n    )\n    \n    # Adjust margins\n    plt.tight_layout()\n    \n    # Save the plot\n    filepath = os.path.join(folder_path, f\"aggregate_reward_comparison.png\")\n    plt.savefig(filepath, bbox_inches='tight', dpi=300)\n    print(f\"Saved plot: aggregate_reward_comparison.png in {folder_path}\")\n    \n    # Show the plot\n    plt.show()\n    \n    return plt.gcf()\n    \ndef saveMetricsTable(metrics, filename, folder_path):\n    \"\"\"Save metrics table to specified folder\"\"\"\n    import matplotlib.pyplot as plt\n    from datetime import datetime\n    import os\n    \n    # Style the DataFrame for visualization\n    fig, ax = plt.subplots(figsize=(12, 4))\n    ax.axis('tight')\n    ax.axis('off')\n    table = ax.table(cellText=metrics.values,\n                    colLabels=metrics.columns,\n                    rowLabels=metrics.index,\n                    cellLoc='center',\n                    loc='center')\n    \n    # Adjust font size and scaling\n    table.auto_set_font_size(False)\n    table.set_fontsize(9)\n    table.scale(1.2, 1.5)\n    \n    plt.title(\"Performance Metrics Comparison\")\n    \n    # Save with timestamp\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    filepath = os.path.join(folder_path, f\"{filename}_{timestamp}.png\")\n    plt.savefig(filepath, bbox_inches='tight', dpi=300)\n    print(f\"Saved metrics table: {filename}_{timestamp}.png in {folder_path}\")\n    \n    # Show the plot\n    plt.show()\n    plt.close()\n\ndef run_statistical_tests(results):\n    \"\"\"Run statistical tests to compare performance of different reward approaches\"\"\"\n    # Dictionary to store test results\n    test_results = {}\n    \n    # Prepare data for tests\n    reward_data = []\n    \n    # Collect episode rewards for each approach\n    for reward_type, reward_info in results.items():\n        rewards = np.array(reward_info['rewards'])\n        balance_times = np.array(reward_info['balancetimes'])\n        \n        # Create a DataFrame row for each episode\n        for i in range(len(rewards)):\n            row = {\n                'reward_type': reward_type,\n                'episode': i,\n                'reward': rewards[i],\n                'balance_time': balance_times[i]\n            }\n            reward_data.append(row)\n    \n    # Convert to DataFrame\n    df = pd.DataFrame(reward_data)\n    \n    # Run tests for different metrics\n    metrics = ['reward', 'balance_time']\n    for metric in metrics:\n        # Run ANOVA\n        from scipy import stats\n        unique_types = df['reward_type'].unique()\n        \n        if len(unique_types) < 2:\n            test_results[metric] = {\n                'anova': {'F': np.nan, 'p': np.nan},\n                'pairwise': {}\n            }\n            continue\n            \n        # Extract data for each reward type\n        groups = [df[df['reward_type'] == t][metric].values for t in unique_types]\n        \n        # Run ANOVA\n        try:\n            f_stat, p_value = stats.f_oneway(*groups)\n            anova_result = {'F': f_stat, 'p': p_value}\n        except:\n            anova_result = {'F': np.nan, 'p': np.nan}\n        \n        # Run Tukey's HSD post-hoc test for pairwise comparisons\n        try:\n            from statsmodels.stats.multicomp import pairwise_tukeyhsd\n            tukey = pairwise_tukeyhsd(df[metric], df['reward_type'], alpha=0.05)\n            \n            # Extract pairwise results\n            pairwise_results = {}\n            for i, group1 in enumerate(tukey.groupsunique):\n                for j, group2 in enumerate(tukey.groupsunique):\n                    if i < j:  # Avoid duplicates\n                        key = f\"{group1} vs {group2}\"\n                        idx = np.where((tukey.data.group1 == group1) & (tukey.data.group2 == group2))[0]\n                        if len(idx) > 0:\n                            reject = tukey.reject[idx[0]]\n                            pairwise_results[key] = {\n                                'reject_null': bool(reject),\n                                'p_value': float(tukey.pvalues[idx[0]])\n                            }\n            \n            test_results[metric] = {\n                'anova': anova_result,\n                'pairwise': pairwise_results\n            }\n        except:\n            test_results[metric] = {\n                'anova': anova_result,\n                'pairwise': {}\n            }\n    \n    return test_results\n\ndef runMultipleExperiments(numRuns=4, episodes=40000, changeInterval=20000, max_updates_per_component=3):\n    \"\"\"\n    Run multiple experiments and save results in organized folders\n    \n    Args:\n        numRuns: Number of experiment runs to perform\n        episodes: Number of episodes per run\n        changeInterval: Number of episodes between environment changes\n        max_updates_per_component: Maximum number of API calls per reward component per run\n    \"\"\"\n    # Create main experiment folder\n    experiment_folder = createExperimentFolder()\n    \n    allResults = []\n    allMetrics = []\n    allAdaptationMetrics = []\n    aggregatedMetrics = {}\n    aggregatedAdaptation = {}\n    \n    # Create run folders\n    for run in range(numRuns):\n        print(f\"\\n=============== Starting Run {run + 1}/{numRuns} ===============\")\n        \n        # Create folder for this run\n        run_folder = os.path.join(experiment_folder, f\"run_{run + 1}\")\n        os.makedirs(run_folder)\n        \n        # Run experiment\n        results = runPerformanceComparisonTest(\n            episodes=episodes,\n            changeInterval=changeInterval,\n            mass_cart=1.0,\n            lengthchanges=[0.3, 0.9],\n            max_updates_per_component=max_updates_per_component\n        )\n        \n        # Calculate metrics for this run\n        metrics = calculatePerformanceMetrics(results)\n        adaptation_metrics_df = adaptation_metrics(results, changeInterval)\n        \n        # Store results\n        allResults.append(results)\n        allMetrics.append(metrics)\n        allAdaptationMetrics.append(adaptation_metrics_df)\n        \n        # Store metrics by reward type\n        for idx, row in metrics.iterrows():\n            if idx not in aggregatedMetrics:\n                aggregatedMetrics[idx] = []\n            aggregatedMetrics[idx].append(row.to_dict())\n        \n        # Store adaptation metrics by reward type\n        for idx, row in adaptation_metrics_df.iterrows():\n            if idx not in aggregatedAdaptation:\n                aggregatedAdaptation[idx] = []\n            aggregatedAdaptation[idx].append(row.to_dict())\n        \n        # Visualization section\n        print(\"\\n--- Generating visualizations for Run\", run + 1, \"---\")\n        \n        # 1. Performance comparison plots (original)\n        print(\"Creating performance comparison plots...\")\n        visualizePerformanceComparison(results, changeInterval, run_folder)\n        \n        # 2. Save metrics table\n        print(\"Creating metrics table...\")\n        saveMetricsTable(metrics, f\"metrics_run_{run + 1}\", run_folder)\n        \n        # 3. Create new reward plots with and without variance\n        print(\"Creating reward over time plots...\")\n        # With variance bands\n        reward_plot_with_variance = create_reward_over_time_plot_with_variance(\n            results, \n            changeInterval, \n            save_path=run_folder,\n            include_variance=True\n        )\n        plt.close()\n        \n        # Without variance bands\n        reward_plot_without_variance = create_reward_over_time_plot_with_variance(\n            results, \n            changeInterval, \n            save_path=run_folder,\n            include_variance=False\n        )\n        plt.close()\n    \n    # Calculate confidence intervals (95%)\n    confidenceIntervals = {}\n    resultsTable = pd.DataFrame()\n    \n    for rewardType, metrics_list in aggregatedMetrics.items():\n        metrics_df = pd.DataFrame(metrics_list)\n        means = metrics_df.mean()\n        cis = 1.96 * metrics_df.std() / np.sqrt(numRuns)\n        \n        # Create row for this reward type\n        resultsTable.loc[rewardType, 'Average Reward'] = f\"{means['finalavgreward']:.2f} ± {cis['finalavgreward']:.2f}\"\n        resultsTable.loc[rewardType, 'Average Balance'] = f\"{means['finalavgbalance']:.2f} ± {cis['finalavgbalance']:.2f}\"\n        resultsTable.loc[rewardType, 'Convergence Time'] = f\"{means['convergencetime']:.2f} ± {cis['convergencetime']:.2f}\"\n        resultsTable.loc[rewardType, 'Stability'] = f\"{means['stability']:.2f} ± {cis['stability']:.2f}\"\n        \n        # Store the raw values for possible further analysis\n        confidenceIntervals[rewardType] = {\n            'finalavgreward': {'mean': means['finalavgreward'], 'ci': cis['finalavgreward']},\n            'finalavgbalance': {'mean': means['finalavgbalance'], 'ci': cis['finalavgbalance']},\n            'convergencetime': {'mean': means['convergencetime'], 'ci': cis['convergencetime']},\n            'stability': {'mean': means['stability'], 'ci': cis['stability']}\n        }\n    \n    # Do the same for adaptation metrics\n    adaptationTable = pd.DataFrame()\n    adaptationIntervals = {}\n    \n    for rewardType, metrics_list in aggregatedAdaptation.items():\n        if not metrics_list:\n            continue\n            \n        metrics_df = pd.DataFrame(metrics_list)\n        means = metrics_df.mean()\n        cis = 1.96 * metrics_df.std() / np.sqrt(numRuns)\n        \n        # Create row for this reward type\n        for metric in metrics_df.columns:\n            if means[metric] == means[metric]:  # Check for NaN\n                adaptationTable.loc[rewardType, metric] = f\"{means[metric]:.2f} ± {cis[metric]:.2f}\"\n            else:\n                adaptationTable.loc[rewardType, metric] = \"N/A\"\n        \n        # Store raw values\n        adaptationIntervals[rewardType] = {\n            col: {'mean': means[col], 'ci': cis[col]} \n            for col in metrics_df.columns if means[col] == means[col]\n        }\n    \n    print(\"\\n--- Creating Aggregate Analysis ---\")\n    \n    # Run statistical tests across all runs\n    if len(allResults) > 1:\n        print(\"Running statistical tests...\")\n        # Combine results from all runs\n        combined_results = {}\n        for reward_type in allResults[0].keys():\n            combined_results[reward_type] = {\n                'rewards': [],\n                'balancetimes': [],\n                'rewardChanges': []\n            }\n            \n            for run_result in allResults:\n                if reward_type in run_result:\n                    combined_results[reward_type]['rewards'].extend(run_result[reward_type]['rewards'])\n                    combined_results[reward_type]['balancetimes'].extend(run_result[reward_type]['balancetimes'])\n                    if 'rewardChanges' in run_result[reward_type]:\n                        combined_results[reward_type]['rewardChanges'].extend(run_result[reward_type]['rewardChanges'])\n        \n        # Run statistical tests\n        stats_results = run_statistical_tests(combined_results)\n        \n        # Save statistical test results\n        with open(os.path.join(experiment_folder, \"statistical_tests.txt\"), \"w\") as f:\n            f.write(\"Statistical Test Results:\\n\")\n            for metric, tests in stats_results.items():\n                f.write(f\"\\n{metric.upper()} TESTS:\\n\")\n                f.write(f\"ANOVA: F={tests['anova']['F']:.4f}, p={tests['anova']['p']:.4f}\\n\")\n                f.write(\"Pairwise comparisons:\\n\")\n                for pair, result in tests['pairwise'].items():\n                    significant = \"Significant\" if result['reject_null'] else \"Not significant\"\n                    f.write(f\"  {pair}: {significant} (p={result['p_value']:.4f})\\n\")\n    \n    # Generate and save adaptation speed visualization\n    if numRuns > 1:\n        print(\"Creating adaptation speed visualization...\")\n        adaptation_fig = analyze_adaptation_speed(allResults, changeInterval)\n        savePlot(adaptation_fig, \"adaptation_speed\", experiment_folder)\n        \n        # Create aggregate reward plot\n        print(\"Creating aggregate reward plot...\")\n        aggregate_fig = create_aggregate_reward_plot(allResults, changeInterval, experiment_folder)\n        plt.close(aggregate_fig)\n    \n    # Save aggregate statistics\n    print(\"Saving aggregate statistics...\")\n    with open(os.path.join(experiment_folder, \"aggregate_statistics.txt\"), \"w\") as f:\n        f.write(f\"Experiment Configuration:\\n\")\n        f.write(f\"- Number of runs: {numRuns}\\n\")\n        f.write(f\"- Episodes per run: {episodes}\\n\")\n        f.write(f\"- Environment change interval: {changeInterval}\\n\")\n        f.write(f\"- Max updates per component per run: {max_updates_per_component}\\n\\n\")\n        \n        f.write(\"Aggregate Performance Statistics:\\n\")\n        for rewardType, metrics in confidenceIntervals.items():\n            f.write(f\"\\n{rewardType}:\\n\")\n            for metric, values in metrics.items():\n                f.write(f\"{metric}: {values['mean']:.2f} ± {values['ci']:.2f}\\n\")\n        \n        f.write(\"\\n\\nAggregate Adaptation Statistics:\\n\")\n        for rewardType, metrics in adaptationIntervals.items():\n            f.write(f\"\\n{rewardType}:\\n\")\n            for metric, values in metrics.items():\n                if values['mean'] == values['mean']:  # Check if not NaN\n                    f.write(f\"{metric}: {values['mean']:.2f} ± {values['ci']:.2f}\\n\")\n                else:\n                    f.write(f\"{metric}: N/A\\n\")\n    \n    # Save final results tables\n    print(\"Creating final metrics tables...\")\n    saveMetricsTable(resultsTable, \"final_results\", experiment_folder)\n    if not adaptationTable.empty:\n        saveMetricsTable(adaptationTable, \"adaptation_metrics\", experiment_folder)\n    \n    print(f\"\\nExperiment results saved in: {experiment_folder}\")\n    return confidenceIntervals, adaptationIntervals, allResults, resultsTable, adaptationTable"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Run a small test experiment with multiple runs\n\n# This will run a smaller experiment to demonstrate the new plots\nconfidenceIntervals, adaptationIntervals, allResults, resultsTable, adaptationTable = runMultipleExperiments(\n    numRuns=2,  # Run just 2 experiments to see the results faster\n    episodes=1000,  # Shorter episodes for testing\n    changeInterval=500,  # Environment changes every 500 episodes\n    max_updates_per_component=3  # Limit API calls per component\n)\n\n# Display the results\nprint(\"\\nFinal Performance Metrics (with confidence intervals):\")\nprint(resultsTable)\n\nprint(\"\\nAdaptation Metrics (with confidence intervals):\")\nprint(adaptationTable)\n\n# For a full experiment, use parameters like:\n# confidenceIntervals, adaptationIntervals, allResults, resultsTable, adaptationTable = runMultipleExperiments(\n#     numRuns=4,  # Run 4 complete experiments for statistical significance \n#     episodes=40000,  # Each experiment runs for 40,000 episodes\n#     changeInterval=10000,  # Environment changes every 10,000 episodes\n#     max_updates_per_component=3  # Limit to 3 updates per component per run\n# )"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}