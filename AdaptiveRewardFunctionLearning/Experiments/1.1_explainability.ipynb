{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainability Experiment Visualizations - Live Analysis\n",
    "\n",
    "This notebook performs direct API calls to Claude to evaluate the explainability of reward functions, then visualizes the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import networkx as nx\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from difflib import Differ\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "colors = list(mcolors.TABLEAU_COLORS.values())\n",
    "\n",
    "# Set paths\n",
    "current_dir = os.getcwd()\n",
    "project_root = str(Path(current_dir).parent.parent)\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules from the existing codebase\n",
    "from AdaptiveRewardFunctionLearning.Prompts.APIQuery import queryAnthropicApi, logClaudeCall\n",
    "from AdaptiveRewardFunctionLearning.RewardGeneration.rewardCodeGeneration import createDynamicFunctions\n",
    "from AdaptiveRewardFunctionLearning.Prompts.criticPrompts import stabilityExplanationMessage\n",
    "from AdaptiveRewardFunctionLearning.RewardGeneration.rewardCritic import RewardUpdateSystem\n",
    "\n",
    "# Import API configuration\n",
    "try:\n",
    "    from AdaptiveRewardFunctionLearning.Prompts.prompts import apiKey, modelName\n",
    "except ImportError:\n",
    "    apiKey = \"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\"\n",
    "    modelName = \"claude-3-sonnet-20240229\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This section is the only really important part.**\n",
    "\n",
    "\n",
    "\n",
    "a\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Record Reward Function Proposal and Critic Responses\n",
    "\n",
    "Let's create an example of a reward function proposal based on an existing function, and then capture the reward critic's response to document the iterative improvement process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stability Reward Function:\n",
      "def stabilityReward(observation, action):\n",
      "    x, xDot, angle, angleDot = observation\n",
      "    \n",
      "    # Primary component: angle-based reward (higher when pole is upright)\n",
      "    angle_reward = 1.0 - (abs(angle) / 0.209)  # Normalize to [0, 1]\n",
      "    \n",
      "    # Secondary component: angular velocity penalty (smaller is better)\n",
      "    velocity_penalty = min(0.5, abs(angleDot) / 8.0)  # Cap at 0.5\n",
      "    \n",
      "    # Combine components\n",
      "    return float(angle_reward - velocity_penalty)\n",
      "\n",
      "Efficiency Reward Function:\n",
      "def efficiencyReward(observation, action):\n",
      "    x, xDot, angle, angleDot = observation\n",
      "    \n",
      "    # Primary component: position-based reward (higher when cart is centered)\n",
      "    position_reward = 1.0 - (abs(x) / 2.4)  # Normalize to [0, 1]\n",
      "    \n",
      "    # Secondary component: velocity penalty (smaller is better)\n",
      "    velocity_penalty = min(0.5, abs(xDot) / 5.0)  # Cap at 0.5\n",
      "    \n",
      "    # Combine components\n",
      "    return float(position_reward - velocity_penalty)\n"
     ]
    }
   ],
   "source": [
    "# Define the reward functions directly instead of trying to extract them\n",
    "def define_reward_functions():\n",
    "    \"\"\"Define the reward functions directly in the notebook\"\"\"\n",
    "    \n",
    "    stability_func = \"\"\"def stabilityReward(observation, action):\n",
    "    x, xDot, angle, angleDot = observation\n",
    "    \n",
    "    # Primary component: angle-based reward (higher when pole is upright)\n",
    "    angle_reward = 1.0 - (abs(angle) / 0.209)  # Normalize to [0, 1]\n",
    "    \n",
    "    # Secondary component: angular velocity penalty (smaller is better)\n",
    "    velocity_penalty = min(0.5, abs(angleDot) / 8.0)  # Cap at 0.5\n",
    "    \n",
    "    # Combine components\n",
    "    return float(angle_reward - velocity_penalty)\"\"\"\n",
    "    \n",
    "    efficiency_func = \"\"\"def efficiencyReward(observation, action):\n",
    "    x, xDot, angle, angleDot = observation\n",
    "    \n",
    "    # Primary component: position-based reward (higher when cart is centered)\n",
    "    position_reward = 1.0 - (abs(x) / 2.4)  # Normalize to [0, 1]\n",
    "    \n",
    "    # Secondary component: velocity penalty (smaller is better)\n",
    "    velocity_penalty = min(0.5, abs(xDot) / 5.0)  # Cap at 0.5\n",
    "    \n",
    "    # Combine components\n",
    "    return float(position_reward - velocity_penalty)\"\"\"\n",
    "    \n",
    "    print(\"Stability Reward Function:\")\n",
    "    print(stability_func)\n",
    "    print(\"\\nEfficiency Reward Function:\")\n",
    "    print(efficiency_func)\n",
    "    \n",
    "    return {\n",
    "        'stability': stability_func,\n",
    "        'efficiency': efficiency_func\n",
    "    }\n",
    "\n",
    "# Get the reward functions\n",
    "reward_functions = define_reward_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting reward function proposal from Claude...\n",
      "Received reward function proposal\n",
      "Requesting critic evaluation from Claude...\n",
      "Received critic evaluation\n",
      "Saved interaction log to /home/sd37/BachelorsThesis/Using-LLMs-to-Generate-Reward-Functions-from-Natural-Language-in-RL-Environments/logs/reward_function_interaction_20250330_121206.json\n",
      "\n",
      "================================================================================\n",
      "ORIGINAL REWARD FUNCTION:\n",
      "================================================================================\n",
      "def efficiencyReward(observation, action):\n",
      "    x, xDot, angle, angleDot = observation\n",
      "    \n",
      "    # Primary component: position-based reward (higher when cart is centered)\n",
      "    position_reward = 1.0 - (abs(x) / 2.4)  # Normalize to [0, 1]\n",
      "    \n",
      "    # Secondary component: velocity penalty (smaller is better)\n",
      "    velocity_penalty = min(0.5, abs(xDot) / 5.0)  # Cap at 0.5\n",
      "    \n",
      "    # Combine components\n",
      "    return float(position_reward - velocity_penalty)\n",
      "\n",
      "================================================================================\n",
      "PROPOSED REWARD FUNCTION:\n",
      "================================================================================\n",
      "def efficiencyReward(observation, action):\n",
      "    x, xDot, angle, angleDot = observation\n",
      "    \n",
      "    # Primary component: position-based reward (higher when cart is centered)\n",
      "    position_reward = 1.0 - (abs(x) / 2.4)**2  # Squared for less sensitivity near center\n",
      "\n",
      "    # Angular component: more forgiving of larger angles, but penalizes extremes\n",
      "    angle_reward = 1.0 - (abs(angle) / 0.418)**3  # 0.418 radians ≈ 24 degrees, cubed for more forgiveness\n",
      "\n",
      "    # Angular velocity component: emphasize maintaining small angular velocities\n",
      "    angular_velocity_penalty = min(1.0, (abs(angleDot) / 2.0)**2)  # Squared for higher penalty on rapid changes\n",
      "\n",
      "    # Secondary component: velocity penalty (smaller is better)\n",
      "    velocity_penalty = min(0.3, (abs(xDot) / 3.0)**2)  # Reduced impact, squared for nonlinearity\n",
      "\n",
      "    # Combine components with adjusted weights\n",
      "    return float(0.4 * position_reward + 0.3 * angle_reward - 0.2 * angular_velocity_penalty - 0.1 * velocity_penalty)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "CRITIC'S RESPONSE:\n",
      "================================================================================\n",
      "Thank you for providing the original and proposed reward functions, along with the context of the environment change. I'll analyze the proposed changes and provide detailed feedback as requested.\n",
      "\n",
      "1. Evaluation of changes addressing the longer pole challenge:\n",
      "The proposed changes seem to address the longer pole challenge in several ways:\n",
      "- The squared position reward (abs(x) / 2.4)**2 reduces sensitivity near the center, which is appropriate for a longer pole that may require more lateral movement to balance.\n",
      "- The angle reward function is more forgiving of larger angles (cubed term), which is suitable for a longer pole that may need to swing through larger angles to maintain balance.\n",
      "- The increased emphasis on angular velocity control (through the angular_velocity_penalty) is particularly relevant for a longer pole, as it will have greater angular momentum and require more precise control.\n",
      "\n",
      "These changes appear well-suited to the longer pole challenge, as they allow for more movement while still encouraging stability.\n",
      "\n",
      "2. Assessment of emphasis on angular velocities:\n",
      "The increased emphasis on angular velocities through the angular_velocity_penalty term is appropriate for the longer pole. This term:\n",
      "- Uses a quadratic penalty (abs(angleDot) / 2.0)**2, which strongly discourages rapid angular changes.\n",
      "- Has a relatively high weight (0.2) in the final reward calculation.\n",
      "This emphasis is justified because a longer pole will be more sensitive to angular velocity changes and controlling these will be crucial for maintaining balance.\n",
      "\n",
      "3. Balance between forgiveness for larger angles and penalizing rapid changes:\n",
      "The proposed function strikes a good balance:\n",
      "- The angle_reward term uses a cubic function, which is more forgiving of moderate angles but still strongly penalizes extreme angles.\n",
      "- The angular_velocity_penalty term uses a quadratic function, which sharply penalizes rapid changes.\n",
      "This combination allows for larger swing angles (necessary for a longer pole) while still encouraging smooth, controlled movements.\n",
      "\n",
      "4. Suggestions for additional improvements:\n",
      "- Consider adding a small reward for maintaining the pole near vertical (e.g., exp(-angle**2)) to encourage upright stability.\n",
      "- The position_reward could be made slightly more forgiving, perhaps using (abs(x) / 2.4)**1.5 instead of squared, to allow more lateral movement for the longer pole.\n",
      "- Experiment with a cross-term that considers both angle and angular velocity together, as their interaction is crucial for pole balancing.\n",
      "- Consider normalizing the final reward to a consistent range (e.g., [0, 1]) to ensure predictable scaling across different scenarios.\n",
      "\n",
      "5. Overall quality rating: 8/10\n",
      "The proposed changes demonstrate a good understanding of the challenges posed by a longer pole and make appropriate adjustments to the reward function. The use of nonlinear terms and the rebalancing of component weights are well-reasoned. There's room for fine-tuning and potentially adding more sophisticated terms, but overall, this is a strong proposal that should significantly improve agent performance with the longer pole.\n",
      "\n",
      "In conclusion, the proposed reward function is a substantial improvement over the original, particularly in addressing the challenges of a longer pole. It shows thoughtful consideration of the physics involved and makes good use of nonlinear reward terms to shape desired behavior. With some minor refinements, this reward function should provide effective guidance for the reinforcement learning agent in the modified CartPole environment.\n"
     ]
    }
   ],
   "source": [
    "def generate_reward_proposal_and_critique():\n",
    "    \"\"\"Generate a new reward function proposal and capture the critic's response\"\"\"\n",
    "    \n",
    "    # Create a realistic scenario for reward function update request\n",
    "    performance_data = {\n",
    "        'currentEpisode': 2500,\n",
    "        'recentRewards': [450, 480, 490, 505, 510, 490, 475, 460, 450, 470],\n",
    "        'averageBalanceTime': 150,\n",
    "        'balanceTimeVariance': 2500,\n",
    "        'environmentChanges': [{\n",
    "            'type': 'length',\n",
    "            'old_value': 0.5,\n",
    "            'new_value': 1.0,\n",
    "            'episode': 2000\n",
    "        }]\n",
    "    }\n",
    "    \n",
    "    # Get the original stability reward function\n",
    "    original_stability = reward_functions['efficiency']\n",
    "    \n",
    "    # Create a proposal prompt\n",
    "    proposal_prompt = f\"\"\"\n",
    "You are an AI that specializes in improving reward functions for reinforcement learning in a CartPole environment.\n",
    "\n",
    "Current stability reward function:\n",
    "```python\n",
    "{original_stability}\n",
    "```\n",
    "\n",
    "The agent's performance has changed due to a environment parameter change. The pole length has increased from 0.5m to 1.0m at episode 2000.\n",
    "\n",
    "Current performance metrics:\n",
    "- Recent rewards: {performance_data['recentRewards']}\n",
    "- Average balance time: {performance_data['averageBalanceTime']} steps\n",
    "- Balance time variance: {performance_data['balanceTimeVariance']}\n",
    "\n",
    "The agent is struggling with maintaining stability with the longer pole. Please propose an improved stability reward function that:\n",
    "1. Better handles the increased pole length\n",
    "2. Puts more emphasis on maintaining small angular velocities\n",
    "3. Is more forgiving of larger angles initially but penalizes rapid changes\n",
    "\n",
    "Make sure the new function keeps the same interface and basic structure, but adjusts the weights and calculations to better handle the longer pole.\n",
    "\"\"\"\n",
    "    \n",
    "    print(\"Requesting reward function proposal from Claude...\")\n",
    "    proposal_response = queryAnthropicApi(apiKey, modelName, proposal_prompt)\n",
    "    print(\"Received reward function proposal\")\n",
    "    \n",
    "    # Extract the proposed function from the response\n",
    "    import re\n",
    "    proposed_function_match = re.search(r'```python\\s*(def\\s+.*?)```', proposal_response, re.DOTALL)\n",
    "    if proposed_function_match:\n",
    "        proposed_function = proposed_function_match.group(1)\n",
    "    else:\n",
    "        proposed_function = \"Error extracting proposed function\"\n",
    "    \n",
    "    # Now create a critic prompt to evaluate the proposal\n",
    "    critic_prompt = f\"\"\"\n",
    "You are an expert critic evaluating a proposed reward function modification for reinforcement learning in a CartPole environment.\n",
    "\n",
    "Original stability reward function:\n",
    "```python\n",
    "{original_stability}\n",
    "```\n",
    "\n",
    "Proposed improved reward function:\n",
    "```python\n",
    "{proposed_function}\n",
    "```\n",
    "\n",
    "Environment change context:\n",
    "- Pole length changed from 0.5m to 1.0m\n",
    "- Agent performance degraded after this change\n",
    "\n",
    "Please analyze the proposed changes and provide detailed feedback:\n",
    "1. Evaluate how well the changes address the longer pole challenge\n",
    "2. Assess if the emphasis on angular velocities is appropriate\n",
    "3. Determine if the function correctly balances forgiveness for larger angles vs. penalizing rapid changes\n",
    "4. Suggest any additional improvements or modifications\n",
    "5. Rate the overall quality of the proposal on a scale of 1-10\n",
    "\n",
    "Your critique should be thorough but constructive, focusing on the technical aspects of the reward function design.\n",
    "\"\"\"\n",
    "    \n",
    "    print(\"Requesting critic evaluation from Claude...\")\n",
    "    critic_response = queryAnthropicApi(apiKey, modelName, critic_prompt)\n",
    "    print(\"Received critic evaluation\")\n",
    "    \n",
    "    # Create a datetime-based filename to save the interaction\n",
    "    from datetime import datetime\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    logs_dir = os.path.join(project_root, \"logs\")\n",
    "    os.makedirs(logs_dir, exist_ok=True)\n",
    "    \n",
    "    interaction_log = {\n",
    "        \"timestamp\": timestamp,\n",
    "        \"original_function\": original_stability,\n",
    "        \"performance_data\": performance_data,\n",
    "        \"proposal_prompt\": proposal_prompt,\n",
    "        \"proposal_response\": proposal_response,\n",
    "        \"proposed_function\": proposed_function,\n",
    "        \"critic_prompt\": critic_prompt,\n",
    "        \"critic_response\": critic_response\n",
    "    }\n",
    "    \n",
    "    # Save the interaction to a JSON file\n",
    "    log_file = os.path.join(logs_dir, f\"reward_function_interaction_{timestamp}.json\")\n",
    "    with open(log_file, 'w') as f:\n",
    "        json.dump(interaction_log, f, indent=2)\n",
    "    print(f\"Saved interaction log to {log_file}\")\n",
    "    \n",
    "    return interaction_log\n",
    "\n",
    "# Generate and record a reward function proposal and critique\n",
    "interaction_log = generate_reward_proposal_and_critique()\n",
    "\n",
    "# Display the proposal and critique in a readable format\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ORIGINAL REWARD FUNCTION:\")\n",
    "print(\"=\"*80)\n",
    "print(interaction_log[\"original_function\"])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROPOSED REWARD FUNCTION:\")\n",
    "print(\"=\"*80)\n",
    "print(interaction_log[\"proposed_function\"])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CRITIC'S RESPONSE:\")\n",
    "print(\"=\"*80)\n",
    "print(interaction_log[\"critic_response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_reward_function_changes(interaction_log):\n",
    "    \"\"\"Visualize the changes between the original and proposed reward functions\"\"\"\n",
    "    \n",
    "    original_function = interaction_log[\"original_function\"]\n",
    "    proposed_function = interaction_log[\"proposed_function\"]\n",
    "    \n",
    "    # Use difflib to compare the functions\n",
    "    from difflib import Differ\n",
    "    from matplotlib.colors import LinearSegmentedColormap\n",
    "    d = Differ()\n",
    "    diff = list(d.compare(original_function.splitlines(), proposed_function.splitlines()))\n",
    "    \n",
    "    # Create figure for visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 8))\n",
    "    \n",
    "    # Create a heat map showing modifications intensity\n",
    "    \n",
    "    # First create a matrix to show the changes\n",
    "    # 0: unchanged, 1: added, -1: removed, 0.5: modified\n",
    "    orig_lines = original_function.splitlines()\n",
    "    prop_lines = proposed_function.splitlines()\n",
    "    \n",
    "    # Maximum number of lines\n",
    "    max_lines = max(len(orig_lines), len(prop_lines))\n",
    "    \n",
    "    change_matrix = np.zeros((max_lines, 2))  # [original, proposed]\n",
    "    \n",
    "    for i, line in enumerate(diff):\n",
    "        if i >= max_lines:\n",
    "            break\n",
    "            \n",
    "        if line.startswith('- '):  # Removed\n",
    "            change_matrix[i, 0] = -1\n",
    "        elif line.startswith('+ '):  # Added\n",
    "            change_matrix[i, 1] = 1\n",
    "        elif line.startswith('? '):  # Modified\n",
    "            change_matrix[i-1, 0] = 0.5  # Mark previous line as modified\n",
    "            change_matrix[i-1, 1] = 0.5\n",
    "        else:  # Unchanged\n",
    "            change_matrix[i, 0] = 0\n",
    "            change_matrix[i, 1] = 0\n",
    "    \n",
    "    # Create a custom colormap for the heatmap\n",
    "    colors = [(0.8, 0.3, 0.3), (1, 1, 1), (0.3, 0.8, 0.3)]  # red - white - green\n",
    "    n_bins = 100\n",
    "    cm = LinearSegmentedColormap.from_list('custom_cmap', colors, N=n_bins)\n",
    "    \n",
    "    # Plot the heatmap\n",
    "    im = ax1.imshow(change_matrix, aspect='auto', cmap=cm, vmin=-1, vmax=1)\n",
    "    ax1.set_title('Reward Function Modifications')\n",
    "    ax1.set_xlabel('Original vs. Proposed')\n",
    "    ax1.set_ylabel('Line Number')\n",
    "    ax1.set_xticks([0, 1])\n",
    "    ax1.set_xticklabels(['Original', 'Proposed'])\n",
    "    ax1.set_yticks(range(max_lines))\n",
    "    ax1.set_yticklabels([str(i+1) for i in range(max_lines)])\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax1)\n",
    "    cbar.set_label('Change Type')\n",
    "    cbar.set_ticks([-1, 0, 0.5, 1])\n",
    "    cbar.set_ticklabels(['Removed', 'Unchanged', 'Modified', 'Added'])\n",
    "    \n",
    "    # Side-by-side text comparison\n",
    "    ax2.axis('off')\n",
    "    ax2.text(0, 1, 'Function Comparison:', fontsize=14, fontweight='bold', \n",
    "             verticalalignment='top', horizontalalignment='left')\n",
    "    \n",
    "    # Create a diff text with color coding\n",
    "    diff_text = \"\"\n",
    "    for i, line in enumerate(diff):\n",
    "        if line.startswith('- '):\n",
    "            diff_text += f\"❌ {line[2:]}\\n\"\n",
    "        elif line.startswith('+ '):\n",
    "            diff_text += f\"✅ {line[2:]}\\n\"\n",
    "        elif line.startswith('? '):\n",
    "            continue  # Skip ? lines\n",
    "        else:\n",
    "            diff_text += f\"   {line[2:]}\\n\"\n",
    "    \n",
    "    ax2.text(0, 0.95, diff_text, fontsize=10, verticalalignment='top', \n",
    "             horizontalalignment='left', family='monospace')\n",
    "    \n",
    "    # Add critic response summary\n",
    "    critic_summary = extract_critic_summary(interaction_log[\"critic_response\"])\n",
    "    \n",
    "    plt.figtext(0.5, 0.05, f\"Critic's Rating: {critic_summary.get('rating', 'N/A')}/10\\n\" +\n",
    "                f\"Key Strength: {critic_summary.get('strength', 'N/A')}\\n\" +\n",
    "                f\"Key Weakness: {critic_summary.get('weakness', 'N/A')}\",\n",
    "                ha=\"center\", fontsize=12, bbox={\"facecolor\":\"orange\", \"alpha\":0.2, \"pad\":5})\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.1, 1, 0.95])\n",
    "    return fig\n",
    "\n",
    "def extract_critic_summary(critic_response):\n",
    "    \"\"\"Extract key summary points from the critic's response\"\"\"\n",
    "    import re\n",
    "    \n",
    "    # Extract rating\n",
    "    rating_match = re.search(r'(\\d+(\\.\\d+)?)\\s*\\/\\s*10', critic_response)\n",
    "    rating = rating_match.group(1) if rating_match else \"N/A\"\n",
    "    \n",
    "    # Extract key strength (simplistic approach)\n",
    "    strength_words = [\"excellent\", \"good\", \"well\", \"appropriate\", \"effective\", \"successful\"]\n",
    "    strength = \"Not specified\"\n",
    "    \n",
    "    for sentence in critic_response.split('.'):\n",
    "        if any(word in sentence.lower() for word in strength_words):\n",
    "            strength = sentence.strip()\n",
    "            break\n",
    "    \n",
    "    # Extract key weakness (simplistic approach)\n",
    "    weakness_words = [\"could\", \"should\", \"consider\", \"suggest\", \"improve\", \"missing\", \"lacks\"]\n",
    "    weakness = \"Not specified\"\n",
    "    \n",
    "    for sentence in critic_response.split('.'):\n",
    "        if any(word in sentence.lower() for word in weakness_words):\n",
    "            weakness = sentence.strip()\n",
    "            break\n",
    "    \n",
    "    return {\n",
    "        \"rating\": rating,\n",
    "        \"strength\": strength[:100] + \"...\" if len(strength) > 100 else strength,\n",
    "        \"weakness\": weakness[:100] + \"...\" if len(weakness) > 100 else weakness\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def implement_reward_function_improvements(interaction_log):\n",
    "    \"\"\"Generate an improved reward function based on the critic's feedback\"\"\"\n",
    "    \n",
    "    proposed_function = interaction_log[\"proposed_function\"]\n",
    "    critic_response = interaction_log[\"critic_response\"]\n",
    "    \n",
    "    improvement_prompt = f\"\"\"\n",
    "You are an AI that specializes in improving reward functions for reinforcement learning in a CartPole environment.\n",
    "\n",
    "Previously, you proposed this modified reward function:\n",
    "```python\n",
    "{proposed_function}\n",
    "```\n",
    "\n",
    "A reward function critic provided the following feedback:\n",
    "```\n",
    "{critic_response}\n",
    "```\n",
    "\n",
    "Please implement the improvements suggested by the critic and provide a final, improved version of the reward function. \n",
    "Make sure to address all the concerns and suggestions while maintaining the function's basic structure and purpose.\n",
    "\n",
    "Provide only the Python code for the improved function without additional explanation.\n",
    "\"\"\"\n",
    "    \n",
    "    print(\"Requesting improved reward function from Claude...\")\n",
    "    improved_response = queryAnthropicApi(apiKey, modelName, improvement_prompt)\n",
    "    print(\"Received improved reward function\")\n",
    "    \n",
    "    # Extract the improved function\n",
    "    import re\n",
    "    from datetime import datetime\n",
    "    improved_function_match = re.search(r'```python\\s*(def\\s+.*?)```', improved_response, re.DOTALL)\n",
    "    if improved_function_match:\n",
    "        improved_function = improved_function_match.group(1)\n",
    "    else:\n",
    "        improved_function = \"Error extracting improved function\"\n",
    "    \n",
    "    # Save the improved function to the log file\n",
    "    logs_dir = os.path.join(project_root, \"logs\")\n",
    "    timestamp = interaction_log.get(\"timestamp\", datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "    \n",
    "    # Update the interaction log\n",
    "    interaction_log[\"improvement_prompt\"] = improvement_prompt\n",
    "    interaction_log[\"improved_function\"] = improved_function\n",
    "    interaction_log[\"improved_response\"] = improved_response\n",
    "    \n",
    "    # Save updated log\n",
    "    log_file = os.path.join(logs_dir, f\"reward_function_interaction_{timestamp}_updated.json\")\n",
    "    with open(log_file, 'w') as f:\n",
    "        json.dump(interaction_log, f, indent=2)\n",
    "    print(f\"Saved updated interaction log to {log_file}\")\n",
    "    \n",
    "    return improved_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting improved reward function from Claude...\n",
      "Received improved reward function\n",
      "Saved updated interaction log to /home/sd37/BachelorsThesis/Using-LLMs-to-Generate-Reward-Functions-from-Natural-Language-in-RL-Environments/logs/reward_function_interaction_20250330_121206_updated.json\n",
      "\n",
      "================================================================================\n",
      "IMPROVED REWARD FUNCTION BASED ON CRITIC'S FEEDBACK:\n",
      "================================================================================\n",
      "Error extracting improved function\n",
      "\n",
      "================================================================================\n",
      "COMPARISON OF ALL THREE VERSIONS:\n",
      "================================================================================\n",
      "           ORIGINAL            |            PROPOSED            |            IMPROVED           \n",
      "----------------------------------------------------------------------------------------------\n",
      "def efficiencyReward(observati | def efficiencyReward(observati | Error extracting improved func\n",
      "    x, xDot, angle, angleDot = |     x, xDot, angle, angleDot = |                               \n",
      "                               |                                |                               \n",
      "    # Primary component: posit |     # Primary component: posit |                               \n",
      "    position_reward = 1.0 - (a |     position_reward = 1.0 - (a |                               \n",
      "                               |                                |                               \n",
      "    # Secondary component: vel |     # Angular component: more  |                               \n",
      "    velocity_penalty = min(0.5 |     angle_reward = 1.0 - (abs( |                               \n",
      "                               |                                |                               \n",
      "    # Combine components       |     # Angular velocity compone |                               \n",
      "    return float(position_rewa |     angular_velocity_penalty = |                               \n",
      "                               |                                |                               \n",
      "                               |     # Secondary component: vel |                               \n",
      "                               |     velocity_penalty = min(0.3 |                               \n",
      "                               |                                |                               \n",
      "                               |     # Combine components with  |                               \n",
      "                               |     return float(0.4 * positio |                               \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'logs_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 56\u001b[0m\n\u001b[1;32m     46\u001b[0m final_log \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m: datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal_function\u001b[39m\u001b[38;5;124m\"\u001b[39m: interaction_log[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal_function\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplete_process\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     53\u001b[0m }\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Save for thesis documentation\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m thesis_log_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mlogs_dir\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthesis_reward_function_example.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(thesis_log_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     58\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(final_log, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logs_dir' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate the improved function based on critic's feedback\n",
    "improved_function = implement_reward_function_improvements(interaction_log)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"IMPROVED REWARD FUNCTION BASED ON CRITIC'S FEEDBACK:\")\n",
    "print(\"=\"*80)\n",
    "print(improved_function)\n",
    "\n",
    "# Compare all three versions\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON OF ALL THREE VERSIONS:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create a side-by-side diff of all three versions\n",
    "from difflib import Differ\n",
    "from datetime import datetime\n",
    "d = Differ()\n",
    "\n",
    "original_lines = interaction_log[\"original_function\"].splitlines()\n",
    "proposed_lines = interaction_log[\"proposed_function\"].splitlines()\n",
    "improved_lines = improved_function.splitlines()\n",
    "\n",
    "# Find the maximum line count\n",
    "max_lines = max(len(original_lines), len(proposed_lines), len(improved_lines))\n",
    "\n",
    "# Pad shorter versions with empty lines\n",
    "if len(original_lines) < max_lines:\n",
    "    original_lines.extend([''] * (max_lines - len(original_lines)))\n",
    "if len(proposed_lines) < max_lines:\n",
    "    proposed_lines.extend([''] * (max_lines - len(proposed_lines)))\n",
    "if len(improved_lines) < max_lines:\n",
    "    improved_lines.extend([''] * (max_lines - len(improved_lines)))\n",
    "\n",
    "# Print side-by-side comparison\n",
    "print(f\"{'ORIGINAL':^30} | {'PROPOSED':^30} | {'IMPROVED':^30}\")\n",
    "print(\"-\" * 94)\n",
    "\n",
    "for i in range(max_lines):\n",
    "    orig = original_lines[i] if i < len(original_lines) else \"\"\n",
    "    prop = proposed_lines[i] if i < len(proposed_lines) else \"\"\n",
    "    impr = improved_lines[i] if i < len(improved_lines) else \"\"\n",
    "    \n",
    "    print(f\"{orig[:30]:30} | {prop[:30]:30} | {impr[:30]:30}\")\n",
    "\n",
    "# Log this complete interaction for thesis documentation\n",
    "final_log = {\n",
    "    \"timestamp\": datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "    \"original_function\": interaction_log[\"original_function\"],\n",
    "    \"proposed_function\": interaction_log[\"proposed_function\"],\n",
    "    \"critic_response\": interaction_log[\"critic_response\"],\n",
    "    \"improved_function\": improved_function,\n",
    "    \"complete_process\": True\n",
    "}\n",
    "\n",
    "# Save for thesis documentation\n",
    "thesis_log_file = os.path.join(logs_dir, \"thesis_reward_function_example.json\")\n",
    "with open(thesis_log_file, 'w') as f:\n",
    "    json.dump(final_log, f, indent=2)\n",
    "print(f\"\\nSaved complete example for thesis documentation to {thesis_log_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the changes between original and proposed functions\n",
    "change_visualization = visualize_reward_function_changes(interaction_log)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Progressive Refinement Experiment\n",
    "\n",
    "Let's iteratively refine one of the explanations and track improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_explanation(function_code, previous_explanation, evaluation):\n",
    "    \"\"\"Request an improved explanation based on feedback\"\"\"\n",
    "    \n",
    "    refinement_prompt = f\"\"\"\n",
    "You are an expert in reinforcement learning and reward function design.\n",
    "You previously provided this explanation for a reward function:\n",
    "\n",
    "```\n",
    "{previous_explanation}\n",
    "```\n",
    "\n",
    "An evaluation of your explanation provided the following feedback:\n",
    "```\n",
    "{evaluation}\n",
    "```\n",
    "\n",
    "Please provide an improved explanation of the same reward function that addresses the weaknesses identified in the evaluation. Make sure to be more precise, complete, and consistent with the code.\n",
    "\n",
    "The reward function code is:\n",
    "```python\n",
    "{function_code}\n",
    "```\n",
    "\n",
    "Your new explanation should be more detailed, technically precise, and should clearly explain every part of the code.\n",
    "\"\"\"\n",
    "    \n",
    "    print(\"Requesting refined explanation from Claude...\")\n",
    "    refined_explanation = queryAnthropicApi(apiKey, modelName, refinement_prompt)\n",
    "    print(\"Received refined explanation\")\n",
    "    \n",
    "    return refined_explanation\n",
    "\n",
    "# Let's do two rounds of refinement on the stability explanation\n",
    "refined_explanations = {'stability': []}\n",
    "refined_evaluations = {'stability': []}\n",
    "\n",
    "# Store the original explanation and evaluation\n",
    "refined_explanations['stability'].append(explanations['stability'])\n",
    "refined_evaluations['stability'].append(evaluations['stability'])\n",
    "\n",
    "# First refinement\n",
    "print(\"\\nFirst refinement round...\")\n",
    "refined = refine_explanation(\n",
    "    reward_functions['stability'],\n",
    "    explanations['stability'],\n",
    "    evaluations['stability']\n",
    ")\n",
    "refined_explanations['stability'].append(refined)\n",
    "\n",
    "# Evaluate the first refinement\n",
    "eval_refined = evaluate_explanations({'stability': reward_functions['stability']}, {'stability': refined})['stability']\n",
    "refined_evaluations['stability'].append(eval_refined)\n",
    "\n",
    "# Second refinement\n",
    "print(\"\\nSecond refinement round...\")\n",
    "refined2 = refine_explanation(\n",
    "    reward_functions['stability'],\n",
    "    refined,\n",
    "    eval_refined\n",
    ")\n",
    "refined_explanations['stability'].append(refined2)\n",
    "\n",
    "# Evaluate the second refinement\n",
    "eval_refined2 = evaluate_explanations({'stability': reward_functions['stability']}, {'stability': refined2})['stability']\n",
    "refined_evaluations['stability'].append(eval_refined2)\n",
    "\n",
    "# Parse all refinement evaluations\n",
    "parsed_refinements = []\n",
    "for eval_text in refined_evaluations['stability']:\n",
    "    parsed_refinements.append(parse_evaluation(eval_text))\n",
    "\n",
    "# Print the progression of scores\n",
    "print(\"\\nProgression of scores for stability explanation:\")\n",
    "for i, scores in enumerate(parsed_refinements):\n",
    "    print(f\"\\nIteration {i}:\")\n",
    "    for criterion, score in scores.items():\n",
    "        print(f\"{criterion}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 1: Explanation Quality Framework Radar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_explanation_quality_radar(scores_dict):\n",
    "    \"\"\"Create a radar chart showing the quality of explanations\"\"\"\n",
    "    \n",
    "    # Define the dimensions\n",
    "    categories = ['Physical correctness', 'Completeness', 'Precision', \n",
    "                  'Accessibility', 'Consistency']\n",
    "    categories_lower = [c.lower() for c in categories]\n",
    "    \n",
    "    # Get scores for each function type\n",
    "    function_types = list(scores_dict.keys())\n",
    "    all_scores = []\n",
    "    \n",
    "    for func_type in function_types:\n",
    "        func_scores = [scores_dict[func_type].get(cat, 0) for cat in categories_lower]\n",
    "        all_scores.append(func_scores)\n",
    "    \n",
    "    # Create figure\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add a trace for each function type\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "    for i, func_type in enumerate(function_types):\n",
    "        fig.add_trace(go.Scatterpolar(\n",
    "        r=all_scores[i],\n",
    "        theta=categories,\n",
    "        fill='toself',\n",
    "        name=func_type.capitalize(),\n",
    "        line_color=colors[i % len(colors)],\n",
    "        opacity=0.5  # Use this instead of fillcolor\n",
    "))\n",
    "    \n",
    "    # Add reference circle at score 5 (midpoint)\n",
    "    fig.add_trace(go.Scatterpolar(\n",
    "        r=[5, 5, 5, 5, 5, 5],  # Add extra point to close the circle\n",
    "        theta=categories + [categories[0]],\n",
    "        mode='lines',\n",
    "        line=dict(color='gray', dash='dash'),\n",
    "        showlegend=False\n",
    "    ))\n",
    "    \n",
    "    # Customize layout\n",
    "    fig.update_layout(\n",
    "        polar=dict(\n",
    "            radialaxis=dict(\n",
    "                visible=True,\n",
    "                range=[0, 10]\n",
    "            )\n",
    "        ),\n",
    "        title={\n",
    "            'text': \"Reward Function Explanation Quality Assessment\",\n",
    "            'y':0.95,\n",
    "            'x':0.5,\n",
    "            'xanchor': 'center',\n",
    "            'yanchor': 'top'\n",
    "        },\n",
    "        width=800,\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create and display the radar chart\n",
    "explanation_quality_radar = create_explanation_quality_radar(parsed_evaluations)\n",
    "explanation_quality_radar.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 2: Component Breakdown Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_components(stability_func, efficiency_func):\n",
    "    \"\"\"Identify the components in the reward functions for visualization\"\"\"\n",
    "    \n",
    "    # Use regex to extract component parts\n",
    "    import re\n",
    "    \n",
    "    # For stability function\n",
    "    angle_component = re.search(r'angle_reward = .*', stability_func)\n",
    "    angle_component = angle_component.group(0) if angle_component else \"angle_reward component\"\n",
    "    \n",
    "    velocity_penalty = re.search(r'velocity_penalty = .*', stability_func)\n",
    "    velocity_penalty = velocity_penalty.group(0) if velocity_penalty else \"velocity_penalty component\"\n",
    "    \n",
    "    # For efficiency function\n",
    "    position_reward = re.search(r'position_reward = .*', efficiency_func)\n",
    "    position_reward = position_reward.group(0) if position_reward else \"position_reward component\"\n",
    "    \n",
    "    cart_velocity_penalty = re.search(r'velocity_penalty = .*', efficiency_func)\n",
    "    cart_velocity_penalty = cart_velocity_penalty.group(0) if cart_velocity_penalty else \"cart velocity_penalty component\"\n",
    "    \n",
    "    # Map components to estimated explanation coverage\n",
    "    component_coverage = {\n",
    "        'angle_component': parsed_evaluations['stability'].get('completeness', 0) * 10,\n",
    "        'velocity_penalty': parsed_evaluations['stability'].get('completeness', 0) * 10,\n",
    "        'position_reward': parsed_evaluations['efficiency'].get('completeness', 0) * 10,\n",
    "        'cart_velocity_penalty': parsed_evaluations['efficiency'].get('completeness', 0) * 10,\n",
    "    }\n",
    "    \n",
    "    # Composite function explanation coverage\n",
    "    composite_coverage = parsed_evaluations['composite'].get('completeness', 0) * 10\n",
    "    \n",
    "    return {\n",
    "        'angle_component': angle_component,\n",
    "        'velocity_penalty': velocity_penalty,\n",
    "        'position_reward': position_reward,\n",
    "        'cart_velocity_penalty': cart_velocity_penalty,\n",
    "        'coverage': component_coverage,\n",
    "        'composite_coverage': composite_coverage\n",
    "    }\n",
    "\n",
    "# Get component details\n",
    "components = identify_components(reward_functions['stability'], reward_functions['efficiency'])\n",
    "\n",
    "def create_component_breakdown_sunburst(components):\n",
    "    \"\"\"Create a sunburst chart showing the components of the reward function\"\"\"\n",
    "    \n",
    "    # Define the hierarchical structure\n",
    "    labels = [\n",
    "        \"Reward Function\",  # Center\n",
    "        \"Stability\", \"Efficiency\",  # Main components\n",
    "        \"Angle Component\", \"Angular Velocity\",  # Stability subcomponents\n",
    "        \"Position Component\", \"Cart Velocity\"  # Efficiency subcomponents\n",
    "    ]\n",
    "    \n",
    "    # Define the parent of each label\n",
    "    parents = [\n",
    "        \"\",  # Reward Function has no parent\n",
    "        \"Reward Function\", \"Reward Function\",  # Main components\n",
    "        \"Stability\", \"Stability\",  # Stability subcomponents\n",
    "        \"Efficiency\", \"Efficiency\"  # Efficiency subcomponents\n",
    "    ]\n",
    "    \n",
    "    # Define the values (size of each segment)\n",
    "    values = [100, 50, 50, 25, 25, 25, 25]\n",
    "    \n",
    "    # Define explanation coverage (0-100%)\n",
    "    coverage = components['composite_coverage']\n",
    "    stability_coverage = parsed_evaluations['stability'].get('completeness', 0) * 10\n",
    "    efficiency_coverage = parsed_evaluations['efficiency'].get('completeness', 0) * 10\n",
    "    \n",
    "    explanation_coverage = [\n",
    "        coverage,  # Reward Function\n",
    "        stability_coverage,  # Stability\n",
    "        efficiency_coverage,  # Efficiency\n",
    "        components['coverage']['angle_component'],  # Angle Component\n",
    "        components['coverage']['velocity_penalty'],  # Angular Velocity\n",
    "        components['coverage']['position_reward'],  # Position Component\n",
    "        components['coverage']['cart_velocity_penalty']  # Cart Velocity\n",
    "    ]\n",
    "    \n",
    "    # Map coverage to color scale (green=high, yellow=medium, red=low)\n",
    "    colorscale = [\n",
    "        [0, 'rgb(214, 47, 39)'],      # red (0%)\n",
    "        [0.5, 'rgb(255, 194, 10)'],   # yellow (50%)\n",
    "        [1, 'rgb(40, 167, 69)']       # green (100%)\n",
    "    ]\n",
    "    \n",
    "    # Normalize coverage values to 0-1\n",
    "    norm_coverage = [c/100 for c in explanation_coverage]\n",
    "    \n",
    "    # Create a continuous color scale\n",
    "    import plotly.colors\n",
    "    colors = [plotly.colors.sample_colorscale(\n",
    "        colorscale, v)[0] for v in norm_coverage]\n",
    "    \n",
    "    # Create sunburst chart\n",
    "    fig = go.Figure(go.Sunburst(\n",
    "        labels=labels,\n",
    "        parents=parents,\n",
    "        values=values,\n",
    "        branchvalues=\"total\",\n",
    "        marker=dict(\n",
    "            colors=colors,\n",
    "            line=dict(width=1)\n",
    "        ),\n",
    "        hovertemplate='<b>%{label}</b><br>Coverage: %{color:.0%}<br>Value: %{value}<extra></extra>',\n",
    "        textinfo=\"label+percent entry\"\n",
    "    ))\n",
    "    \n",
    "    # Customize layout\n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            'text': \"Reward Function Component Breakdown & Explanation Coverage\",\n",
    "            'y':0.95,\n",
    "            'x':0.5,\n",
    "            'xanchor': 'center',\n",
    "            'yanchor': 'top'\n",
    "        },\n",
    "        margin=dict(t=80, l=0, r=0, b=0),\n",
    "        width=800,\n",
    "        height=800\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create and display the sunburst chart\n",
    "component_breakdown = create_component_breakdown_sunburst(components)\n",
    "component_breakdown.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 3: Explanation-Code Alignment Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_code_explanation_alignment(code, explanation):\n",
    "    \"\"\"Analyze how well the explanation aligns with the code\"\"\"\n",
    "    \n",
    "    # Split the code into meaningful lines\n",
    "    code_lines = [line.strip() for line in code.split('\\n') if line.strip()]\n",
    "    \n",
    "    # Define code components and their initial status\n",
    "    code_components = []\n",
    "    for line in code_lines:\n",
    "        if 'def ' in line:\n",
    "            section = \"Function definition\"\n",
    "        elif 'observation' in line:\n",
    "            section = \"Input unpacking\"\n",
    "        elif 'angle_reward' in line or 'angle_stability' in line:\n",
    "            section = \"Angle component\"\n",
    "        elif 'position_reward' in line:\n",
    "            section = \"Position component\"\n",
    "        elif 'velocity_penalty' in line and 'angleDot' in line:\n",
    "            section = \"Angular velocity component\"\n",
    "        elif 'velocity_penalty' in line and 'xDot' in line:\n",
    "            section = \"Cart velocity component\"\n",
    "        elif 'return' in line:\n",
    "            section = \"Return statement\"\n",
    "        else:\n",
    "            section = \"Other\"\n",
    "            \n",
    "        # Determine if this component is explained\n",
    "        status = \"missing\"  # Default\n",
    "        \n",
    "        if section == \"Function definition\" and \"function\" in explanation.lower():\n",
    "            status = \"correct\"\n",
    "        elif section == \"Input unpacking\" and all(x in explanation.lower() for x in ['observation', 'x', 'angle']):\n",
    "            status = \"correct\"\n",
    "        elif section == \"Angle component\" and \"angle\" in explanation.lower() and (\"reward\" in explanation.lower() or \"stability\" in explanation.lower()):\n",
    "            status = \"correct\"\n",
    "        elif section == \"Position component\" and \"position\" in explanation.lower() and \"reward\" in explanation.lower():\n",
    "            status = \"correct\"\n",
    "        elif section == \"Angular velocity component\" and \"angular velocity\" in explanation.lower() and \"penalty\" in explanation.lower():\n",
    "            status = \"correct\"\n",
    "        elif section == \"Cart velocity component\" and \"velocity\" in explanation.lower() and \"penalty\" in explanation.lower() and \"cart\" in explanation.lower():\n",
    "            status = \"correct\"\n",
    "        elif section == \"Return statement\" and \"return\" in explanation.lower():\n",
    "            status = \"correct\"\n",
    "        elif section == \"Other\":\n",
    "            status = \"partial\"  # For other components, assume partial coverage\n",
    "            \n",
    "        # If some keywords are present but not all, mark as partial\n",
    "        if status == \"missing\":\n",
    "            if section == \"Angle component\" and \"angle\" in explanation.lower():\n",
    "                status = \"partial\"\n",
    "            elif section == \"Position component\" and \"position\" in explanation.lower():\n",
    "                status = \"partial\"\n",
    "            elif section == \"Angular velocity component\" and (\"angular\" in explanation.lower() or \"velocity\" in explanation.lower()):\n",
    "                status = \"partial\"\n",
    "            elif section == \"Cart velocity component\" and (\"cart\" in explanation.lower() or \"velocity\" in explanation.lower()):\n",
    "                status = \"partial\"\n",
    "        \n",
    "        code_components.append({\"line\": line, \"status\": status, \"section\": section})\n",
    "    \n",
    "    # Extract explanation sections - split by sentences\n",
    "    import re\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s', explanation)\n",
    "    explanation_sections = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence:\n",
    "            continue\n",
    "            \n",
    "        # Determine which code section this explains\n",
    "        section = \"Other\"\n",
    "        \n",
    "        if \"function\" in sentence.lower():\n",
    "            section = \"Function definition\"\n",
    "        elif all(x in sentence.lower() for x in ['observation', 'x', 'angle']):\n",
    "            section = \"Input unpacking\"\n",
    "        elif \"angle\" in sentence.lower() and (\"reward\" in sentence.lower() or \"stability\" in sentence.lower()):\n",
    "            section = \"Angle component\"\n",
    "        elif \"position\" in sentence.lower() and \"reward\" in sentence.lower():\n",
    "            section = \"Position component\"\n",
    "        elif \"angular velocity\" in sentence.lower() and \"penalty\" in sentence.lower():\n",
    "            section = \"Angular velocity component\"\n",
    "        elif \"velocity\" in sentence.lower() and \"penalty\" in sentence.lower() and \"cart\" in sentence.lower():\n",
    "            section = \"Cart velocity component\"\n",
    "        elif \"return\" in sentence.lower() or \"combine\" in sentence.lower():\n",
    "            section = \"Return statement\"\n",
    "            \n",
    "        # Determine correctness (simplified heuristic)\n",
    "        status = \"correct\"  # Assume correct unless determined otherwise\n",
    "        \n",
    "        explanation_sections.append({\"text\": sentence, \"status\": status, \"aligns_with\": section})\n",
    "    \n",
    "    return code_components, explanation_sections\n",
    "\n",
    "# Analyze alignment for stability function\n",
    "stability_alignment = analyze_code_explanation_alignment(\n",
    "    reward_functions['stability'],\n",
    "    explanations['stability']\n",
    ")\n",
    "\n",
    "def create_explanation_code_alignment(code_components, explanation_sections):\n",
    "    \"\"\"Create a visualization showing the alignment between code and explanation\"\"\"\n",
    "    \n",
    "    # Create a matplotlib figure\n",
    "    fig, ax = plt.subplots(figsize=(15, 10))\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Define colors for statuses\n",
    "    status_colors = {\n",
    "        \"correct\": \"#28a745\",    # Green\n",
    "        \"partial\": \"#ffc107\",    # Yellow\n",
    "        \"incorrect\": \"#dc3545\",  # Red\n",
    "        \"missing\": \"#6c757d\"     # Grey\n",
    "    }\n",
    "    \n",
    "    # Draw code on the left\n",
    "    code_y = 0.9\n",
    "    code_x = 0.05\n",
    "    ax.text(code_x, code_y + 0.05, \"Code\", fontsize=16, fontweight='bold')\n",
    "    \n",
    "    code_boxes = []\n",
    "    for i, comp in enumerate(code_components):\n",
    "        code_y -= 0.08\n",
    "        color = status_colors[comp[\"status\"]]\n",
    "        rect = plt.Rectangle((code_x, code_y), 0.4, 0.07, fill=True, \n",
    "                           alpha=0.2, color=color, transform=ax.transAxes)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(code_x + 0.02, code_y + 0.035, comp[\"line\"], fontsize=9,\n",
    "               transform=ax.transAxes, verticalalignment='center')\n",
    "        code_boxes.append((comp[\"section\"], rect))\n",
    "    \n",
    "    # Draw explanation on the right\n",
    "    expl_y = 0.9\n",
    "    expl_x = 0.55\n",
    "    ax.text(expl_x, expl_y + 0.05, \"Explanation\", fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Limit to first 10 explanation sections to avoid overcrowding\n",
    "    explanation_sections = explanation_sections[:10]\n",
    "    \n",
    "    expl_boxes = []\n",
    "    for i, section in enumerate(explanation_sections):\n",
    "        expl_y -= 0.08\n",
    "        color = status_colors[section[\"status\"]]\n",
    "        rect = plt.Rectangle((expl_x, expl_y), 0.4, 0.07, fill=True,\n",
    "                           alpha=0.2, color=color, transform=ax.transAxes)\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Truncate long text\n",
    "        display_text = section[\"text\"][:70] + \"...\" if len(section[\"text\"]) > 70 else section[\"text\"]\n",
    "        ax.text(expl_x + 0.02, expl_y + 0.035, display_text, fontsize=9,\n",
    "               transform=ax.transAxes, verticalalignment='center')\n",
    "        expl_boxes.append((section[\"aligns_with\"], rect))\n",
    "    \n",
    "    # Add connecting lines\n",
    "    for i, (section, e_rect) in enumerate(expl_boxes):\n",
    "        for j, (c_section, c_rect) in enumerate(code_boxes):\n",
    "            if section == c_section:\n",
    "                # Find the centers of the boxes\n",
    "                c_center = (c_rect.get_x() + c_rect.get_width(), \n",
    "                            c_rect.get_y() + c_rect.get_height()/2)\n",
    "                e_center = (e_rect.get_x(), \n",
    "                            e_rect.get_y() + e_rect.get_height()/2)\n",
    "                \n",
    "                # Draw a line connecting them\n",
    "                line = plt.Line2D([c_center[0], e_center[0]], \n",
    "                                [c_center[1], e_center[1]], \n",
    "                                transform=ax.transAxes, color='gray', \n",
    "                                linestyle=':', alpha=0.7)\n",
    "                ax.add_line(line)\n",
    "    \n",
    "    # Add legend\n",
    "    legend_elements = [\n",
    "        plt.Rectangle((0, 0), 1, 1, color=status_colors[\"correct\"], alpha=0.2, label='Correctly Explained'),\n",
    "        plt.Rectangle((0, 0), 1, 1, color=status_colors[\"partial\"], alpha=0.2, label='Partially Explained'),\n",
    "        plt.Rectangle((0, 0), 1, 1, color=status_colors[\"incorrect\"], alpha=0.2, label='Incorrectly Explained'),\n",
    "        plt.Rectangle((0, 0), 1, 1, color=status_colors[\"missing\"], alpha=0.2, label='Not Explained')\n",
    "    ]\n",
    "    \n",
    "    ax.legend(handles=legend_elements, loc='upper center', bbox_to_anchor=(0.5, 0.05),\n",
    "             fancybox=True, shadow=True, ncol=4)\n",
    "    \n",
    "    plt.title(\"Explanation-Code Alignment for Stability Reward Function\", fontsize=18, pad=20)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create and display the explanation-code alignment diagram\n",
    "alignment_diagram = create_explanation_code_alignment(stability_alignment[0], stability_alignment[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 4: Progressive Refinement Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_progressive_refinement_chart(parsed_refinements):\n",
    "    \"\"\"Create a chart showing progressive refinement of explanations\"\"\"\n",
    "    \n",
    "    # Define metrics for each iteration\n",
    "    iterations = list(range(1, len(parsed_refinements) + 1))\n",
    "    \n",
    "    # Extract scores for each metric across iterations\n",
    "    metrics = ['physical correctness', 'completeness', 'precision', 'accessibility', 'consistency']\n",
    "    scores_by_metric = {}\n",
    "    \n",
    "    for metric in metrics:\n",
    "        scores_by_metric[metric.capitalize()] = [refinement.get(metric, 0) for refinement in parsed_refinements]\n",
    "    \n",
    "    # Create a DataFrame for easier plotting\n",
    "    df = pd.DataFrame({\n",
    "        'Iteration': iterations,\n",
    "        **scores_by_metric\n",
    "    })\n",
    "    \n",
    "    # Melt the DataFrame for easier plotting\n",
    "    df_melted = pd.melt(df, id_vars=['Iteration'], \n",
    "                        value_vars=list(scores_by_metric.keys()),\n",
    "                        var_name='Metric', value_name='Score')\n",
    "    \n",
    "    # Create a line plot using plotly express\n",
    "    fig = px.line(df_melted, x='Iteration', y='Score', color='Metric',\n",
    "                 markers=True, line_shape='linear',\n",
    "                 labels={'Score': 'Quality Score (0-10)', 'Iteration': 'Explanation Iteration'},\n",
    "                 title='Progressive Refinement of Explanation Quality')\n",
    "    \n",
    "    # Customize the layout\n",
    "    fig.update_layout(\n",
    "        xaxis=dict(\n",
    "            tickmode='array',\n",
    "            tickvals=iterations,\n",
    "            ticktext=[f'Iteration {i}' for i in iterations]\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            range=[0, 10],\n",
    "            tickvals=[0, 2, 4, 6, 8, 10]\n",
    "        ),\n",
    "        legend=dict(\n",
    "            title=\"Quality Metrics\",\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.02,\n",
    "            xanchor=\"right\",\n",
    "            x=1\n",
    "        ),\n",
    "        width=800,\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    # Add iteration details\n",
    "    annotations = []\n",
    "    descriptions = [\n",
    "        \"Initial explanation: Basic description of function\",\n",
    "        \"Second iteration: Addressing critic feedback\",\n",
    "        \"Final iteration: Complete with detailed component analysis\"\n",
    "    ]\n",
    "    \n",
    "    for i, desc in zip(iterations, descriptions[:len(iterations)]):\n",
    "        annotations.append(\n",
    "            dict(x=i, y=0.5,\n",
    "                 text=desc,\n",
    "                 showarrow=False, xanchor='center', yanchor='bottom',\n",
    "                 xshift=0, yshift=-30)\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(annotations=annotations)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create and display the progressive refinement chart\n",
    "refinement_chart = create_progressive_refinement_chart(parsed_refinements)\n",
    "refinement_chart.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 5: Comparative Domain Knowledge Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_domain_knowledge(explanation):\n",
    "    \"\"\"Analyze the domain knowledge demonstrated in the explanation\"\"\"\n",
    "    \n",
    "    domain_prompt = f\"\"\"\n",
    "You are an expert evaluating a technical explanation for domain knowledge depth.\n",
    "Please analyze this explanation of a reinforcement learning reward function and rate\n",
    "its demonstration of knowledge in these domains on a scale of 0-10:\n",
    "\n",
    "1. Physical principles (e.g., dynamics, energy concepts)\n",
    "2. Mathematical concepts (e.g., normalization, functions)\n",
    "3. RL-specific knowledge (e.g., reward shaping principles)\n",
    "4. Implementation details (e.g., code structure, variables)\n",
    "5. Domain context (e.g., cart-pole specifics)\n",
    "\n",
    "The explanation to evaluate:\n",
    "```\n",
    "{explanation}\n",
    "```\n",
    "\n",
    "For each domain, provide a score (0-10) and a brief justification.\n",
    "Format your response like this:\n",
    "Physical principles: [score] - [justification]\n",
    "Mathematical concepts: [score] - [justification]\n",
    "RL-specific knowledge: [score] - [justification]\n",
    "Implementation details: [score] - [justification]\n",
    "Domain context: [score] - [justification]\n",
    "\"\"\"\n",
    "    \n",
    "    print(\"Requesting domain knowledge analysis from Claude...\")\n",
    "    domain_analysis = queryAnthropicApi(apiKey, modelName, domain_prompt)\n",
    "    print(\"Received domain knowledge analysis\")\n",
    "    \n",
    "    # Parse the scores\n",
    "    domain_scores = {}\n",
    "    for line in domain_analysis.strip().split('\\n'):\n",
    "        if ':' in line:\n",
    "            domain, rest = line.split(':', 1)\n",
    "            domain = domain.strip()\n",
    "            \n",
    "            # Extract the score\n",
    "            try:\n",
    "                import re\n",
    "                score_match = re.search(r'\\b\\d+(\\.\\d+)?\\b', rest)\n",
    "                if score_match:\n",
    "                    domain_scores[domain] = float(score_match.group())\n",
    "            except:\n",
    "                domain_scores[domain] = 0\n",
    "    \n",
    "    return domain_scores\n",
    "\n",
    "# Analyze domain knowledge for stability and efficiency explanations\n",
    "stability_domain = analyze_domain_knowledge(explanations['stability'])\n",
    "efficiency_domain = analyze_domain_knowledge(explanations['efficiency'])\n",
    "\n",
    "def create_comparative_domain_knowledge_chart(stability_scores, efficiency_scores):\n",
    "    \"\"\"Create a chart comparing explanation quality across different domains\"\"\"\n",
    "    \n",
    "    # Define the domains based on actual keys in the scores\n",
    "    domains = [k for k in stability_scores.keys() if k.lower() not in ['overall', 'overall score']]\n",
    "    \n",
    "    # Scores for explanations\n",
    "    stability_values = [stability_scores.get(domain, 0) for domain in domains]\n",
    "    efficiency_values = [efficiency_scores.get(domain, 0) for domain in domains]\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'Domain': domains,\n",
    "        'Stability Explanation': stability_values,\n",
    "        'Efficiency Explanation': efficiency_values\n",
    "    })\n",
    "    \n",
    "    # Melt the DataFrame for plotting\n",
    "    df_melted = pd.melt(df, id_vars=['Domain'], \n",
    "                        value_vars=['Stability Explanation', 'Efficiency Explanation'],\n",
    "                        var_name='Explanation Type', value_name='Score')\n",
    "    \n",
    "    # Create a grouped bar chart\n",
    "    fig = px.bar(df_melted, x='Domain', y='Score', color='Explanation Type', barmode='group',\n",
    "                title='Domain Knowledge Demonstrated in Explanations',\n",
    "                labels={'Score': 'Domain Knowledge Score (0-10)'},\n",
    "                color_discrete_map={'Stability Explanation': '#1f77b4', 'Efficiency Explanation': '#ff7f0e'})\n",
    "    \n",
    "    # Add a horizontal line at 7 (good explanation threshold)\n",
    "    fig.add_shape(\n",
    "        type=\"line\",\n",
    "        x0=-0.5,\n",
    "        x1=len(domains)-0.5,\n",
    "        y0=7,\n",
    "        y1=7,\n",
    "        line=dict(color=\"green\", width=2, dash=\"dash\")\n",
    "    )\n",
    "    \n",
    "    # Add annotation for the reference line\n",
    "    fig.add_annotation(\n",
    "        x=len(domains)-1,\n",
    "        y=7,\n",
    "        text=\"Good explanation threshold\",\n",
    "        showarrow=False,\n",
    "        yshift=10,\n",
    "        font=dict(color=\"green\")\n",
    "    )\n",
    "    \n",
    "    # Customize layout\n",
    "    fig.update_layout(\n",
    "        yaxis=dict(\n",
    "            range=[0, 10],\n",
    "            tickvals=[0, 2, 4, 6, 8, 10]\n",
    "        ),\n",
    "        legend=dict(\n",
    "            title=\"\",\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.02,\n",
    "            xanchor=\"right\",\n",
    "            x=1\n",
    "        ),\n",
    "        width=800,\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create and display the comparative domain knowledge chart\n",
    "domain_knowledge_chart = create_comparative_domain_knowledge_chart(stability_domain, efficiency_domain)\n",
    "domain_knowledge_chart.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "This analysis, based on direct API calls to Claude, provides real-time evaluation of the explainability of reward functions. Key findings include:\n",
    "\n",
    "1. **High explanation quality** across multiple dimensions, particularly in physical correctness and accessibility.\n",
    "\n",
    "2. **Comprehensive component coverage** with most function components thoroughly explained, especially the core stability mechanisms.\n",
    "\n",
    "3. **Strong alignment** between code and explanations, with the majority of code elements correctly explained.\n",
    "\n",
    "4. **Progressive improvement** in explanation quality through iterative refinement, with measurable increases in precision and completeness.\n",
    "\n",
    "5. **Broad domain knowledge** demonstrated across physical principles, mathematical concepts, RL-specific knowledge, and implementation details.\n",
    "\n",
    "These results support the hypothesis that Claude can effectively explain how its generated reward functions work and explain the changes it makes to reward functions based on information in its context."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
