{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\samdd\\anaconda3\\envs\\gymenv\\lib\\site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\samdd\\anaconda3\\envs\\gymenv\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\samdd\\anaconda3\\envs\\gymenv\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\samdd\\anaconda3\\envs\\gymenv\\lib\\site-packages (from gym) (3.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\samdd\\anaconda3\\envs\\gymenv\\lib\\site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\samdd\\anaconda3\\envs\\gymenv\\lib\\site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\samdd\\anaconda3\\envs\\gymenv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\samdd\\anaconda3\\envs\\gymenv\\lib\\site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\samdd\\anaconda3\\envs\\gymenv\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\samdd\\anaconda3\\envs\\gymenv\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\samdd\\anaconda3\\envs\\gymenv\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\samdd\\anaconda3\\envs\\gymenv\\lib\\site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\samdd\\anaconda3\\envs\\gymenv\\lib\\site-packages (from matplotlib) (2.9.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\samdd\\anaconda3\\envs\\gymenv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gym numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline SoTA reward function\n",
    "def SoTARewardFunction(observation, action):\n",
    "    x, _, theta, _ = observation\n",
    "    \n",
    "    if abs(theta) > np.deg2rad(15) or abs(x) > 2.4:\n",
    "        return 0\n",
    "    \n",
    "    return 1\n",
    "\n",
    "# Example LLM-generated reward functions\n",
    "def LLMRewardFunction1API(observation, action):\n",
    "    x, _, angle, _ = observation\n",
    "    reward = 1.0 - abs(angle) - 0.5 * abs(x)\n",
    "    return max(0, reward)\n",
    "\n",
    "def LLMRewardFunction2API(observation, action):\n",
    "    x, _, angle, _ = observation\n",
    "    reward = 1.0\n",
    "    \n",
    "    if abs(x) > 2.4 or abs(angle) > 0.2:\n",
    "        reward = -10.0\n",
    "    else:\n",
    "        reward += 10.0 / (1.0 + abs(angle))\n",
    "        reward += 5.0 / (1.0 + abs(x))\n",
    "    \n",
    "    return int(reward)\n",
    "\n",
    "\n",
    "def LLMRewardFunction1(observation, action):\n",
    "    # Unpack state variables\n",
    "    x, x_dot, angle, angle_dot = observation\n",
    "    \n",
    "    # Define constants\n",
    "    x_threshold = 2.4  # Cart position limit\n",
    "    angle_threshold = 12 * np.pi / 180  # Angle limit (12 degrees)\n",
    "    \n",
    "    # Check if episode is done\n",
    "    done = bool(\n",
    "        abs(x) > x_threshold or\n",
    "        abs(angle) > angle_threshold\n",
    "    )\n",
    "    \n",
    "    if not done:\n",
    "        # Reward for pole angle (closer to vertical is better)\n",
    "        angle_reward = 1 - abs(angle) / angle_threshold\n",
    "        \n",
    "        # Reward for cart position (closer to center is better)\n",
    "        position_reward = 1 - abs(x) / x_threshold\n",
    "        \n",
    "        # Reward for keeping the pole stable\n",
    "        stability_reward = 1 / (1 + abs(angle_dot))\n",
    "        \n",
    "        # Combine rewards\n",
    "        reward = angle_reward + position_reward + stability_reward\n",
    "    else:\n",
    "        # Penalty for ending episode\n",
    "        reward = -10\n",
    "    \n",
    "    return reward\n",
    "\n",
    "\n",
    "def LLMRewardFunction2(observation, action):\n",
    "    # Unpack state variables\n",
    "    x, x_dot, angle, angle_dot = observation\n",
    "    \n",
    "    # System parameters (now hardcoded as they can't be passed in)\n",
    "    m = 0.1  # pendulum mass\n",
    "    M = 1.0  # cart mass\n",
    "    l = 0.5  # pendulum length\n",
    "    g = 9.8  # gravity\n",
    "    \n",
    "    # Calculate potential energy\n",
    "    PE = m * g * l * (1 - np.cos(angle))\n",
    "    \n",
    "    # Calculate kinetic energy\n",
    "    KE_cart = 0.5 * M * x_dot**2\n",
    "    KE_pendulum = 0.5 * m * ((x_dot**2) + (l * angle_dot**2) + \n",
    "                             (2 * x_dot * l * angle_dot * np.cos(angle)))\n",
    "    KE_total = KE_cart + KE_pendulum\n",
    "    \n",
    "    # Calculate total energy\n",
    "    E_total = PE + KE_total\n",
    "    \n",
    "    # Define reward components\n",
    "    energy_reward = -E_total  # Minimize total energy\n",
    "    stability_reward = -abs(angle)  # Minimize angle deviation\n",
    "    \n",
    "    # Combine rewards (adjusted weights since we can't measure control effort now)\n",
    "    reward = 0.7 * energy_reward + 0.3 * stability_reward\n",
    "    \n",
    "    # Check if episode is done\n",
    "    x_threshold = 2.4\n",
    "    angle_threshold = np.pi / 2\n",
    "    done = bool(\n",
    "        abs(x) > x_threshold or\n",
    "        abs(angle) > angle_threshold\n",
    "    )\n",
    "    \n",
    "    if done:\n",
    "        reward -= 100  # Large penalty for ending episode\n",
    "    \n",
    "    return reward\n",
    "\n",
    "\n",
    "#---\n",
    "\n",
    "# Train the agent using Q-learning\n",
    "def train(env, agent, reward_model, episodes=500):\n",
    "    rewards = []\n",
    "    for episode in range(episodes):\n",
    "        observation = env.reset()[0]\n",
    "        state = agent.discretize(observation)\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.choose_action(state)\n",
    "            next_observation, reward, done, _, _ = env.step(action)\n",
    "            adjusted_reward = reward_model(next_observation, reward)\n",
    "            next_state = agent.discretize(next_observation)\n",
    "            agent.update(state, action, adjusted_reward, next_state)\n",
    "            state = next_state\n",
    "            total_reward += adjusted_reward\n",
    "        rewards.append(total_reward)\n",
    "    return rewards\n",
    "\n",
    "\n",
    "\n",
    "# episodes = 10000\n",
    "# baselineRewards = train(env, agent, SoTARewardFunction, episodes)\n",
    "# LLM1Rewards = train(env, agent, LLMRewardFunction1, episodes)\n",
    "# LLM2Rewards = train(env, agent, LLMRewardFunction2, episodes)\n",
    "\n",
    "# # Plot results\n",
    "# plt.plot(np.arange(episodes), LLM2Rewards, label=\"2nd LLM-Generated Reward Model\")\n",
    "# plt.plot(np.arange(episodes), baselineRewards, label=\"Custom Baseline Reward Model\")\n",
    "# plt.plot(np.arange(episodes), LLM1Rewards, label=\"1st LLM-Generated Reward Model\")\n",
    "\n",
    "# plt.xlabel(\"Episodes\")\n",
    "# plt.ylabel(\"Total Reward\")\n",
    "\n",
    "# plt.title(\"Reward Comparison over Time\")\n",
    "\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: only length-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 45\u001b[0m\n\u001b[0;32m     43\u001b[0m episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label, reward_fn \u001b[38;5;129;01min\u001b[39;00m rewardFunctions:\n\u001b[1;32m---> 45\u001b[0m     rewards_dict[label] \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m#Total Reward Comparison\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 120\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(env, agent, reward_model, episodes)\u001b[0m\n\u001b[0;32m    118\u001b[0m adjusted_reward \u001b[38;5;241m=\u001b[39m reward_model(next_observation, reward)\n\u001b[0;32m    119\u001b[0m next_state \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mdiscretize(next_observation)\n\u001b[1;32m--> 120\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madjusted_reward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m    122\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m adjusted_reward\n",
      "Cell \u001b[1;32mIn[18], line 14\u001b[0m, in \u001b[0;36mupdate\u001b[1;34m(self, state, action, reward, next_state)\u001b[0m\n\u001b[0;32m     12\u001b[0m td_target \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscount_factor \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_table[next_state][best_next_action]\n\u001b[0;32m     13\u001b[0m td_error \u001b[38;5;241m=\u001b[39m td_target \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_table[state][action]\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m*\u001b[39m td_error\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "# Takes ~ 60 second to run\n",
    "from cartPoleShared import QLearningAgent\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "agent = QLearningAgent(env)\n",
    "\n",
    "\n",
    "#This is temporary until I fix the error with the update function not being defined inthe calss\n",
    "def update(self, state, action, reward, next_state):\n",
    "    best_next_action = np.argmax(self.q_table[next_state])\n",
    "    td_target = reward + self.discount_factor * self.q_table[next_state][best_next_action]\n",
    "    td_error = td_target - self.q_table[state][action]\n",
    "    self.q_table[state][action] += self.learning_rate * td_error\n",
    "\n",
    "QLearningAgent.update = update\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def movingAverageAndStd(data, windowSize=100):\n",
    "    average = np.convolve(data, np.ones(windowSize) / windowSize, mode='valid')\n",
    "    std = [np.std(data[i:i+windowSize]) for i in range(len(data) - windowSize + 1)]\n",
    "    return average, std\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# List of reward functions to abstract repeated calls\n",
    "rewardFunctions = [\n",
    "    (\"Baseline Reward Model\", SoTARewardFunction),\n",
    "    (\"LLM-Generated Reward Model 1 from API\", LLMRewardFunction1API),\n",
    "    (\"LLM-Generated Reward Model 2 from API\", LLMRewardFunction2API),\n",
    "    (\"LLM-Generated Reward Model 1 - Finetuned\", LLMRewardFunction1),\n",
    "    (\"LLM-Generated Reward Model 2 - Finetuned\", LLMRewardFunction2)\n",
    "\n",
    "]\n",
    "\n",
    "# Store results in a dictionary\n",
    "rewards_dict = {}\n",
    "\n",
    "# Run experiments for each reward function\n",
    "episodes = 10000\n",
    "for label, reward_fn in rewardFunctions:\n",
    "    rewards_dict[label] = train(env, agent, reward_fn, episodes)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "\n",
    "#Total Reward Comparison\n",
    "plt.subplot(2, 2, 1)\n",
    "for label, rewards in rewards_dict.items():\n",
    "    plt.plot(np.arange(episodes), rewards, label=label)\n",
    "\n",
    "\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "# 2.Running Average of Rewards\n",
    "\n",
    "# avgBaseline, stdBaseline = movingAverageAndStd(baselineRewards, windowSize=100)\n",
    "# avgLLM1, stdLLM1 = movingAverageAndStd(LLM1Rewards, windowSize=100)\n",
    "# avgLLM2, stdLLM2 = movingAverageAndStd(LLM2Rewards, windowSize=100)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "\n",
    "for label, rewards in rewards_dict.items():\n",
    "    avg, std = movingAverageAndStd(rewards, windowSize=100)\n",
    "    plt.plot(np.arange(len(avg)), avg, label=label)\n",
    "    plt.fill_between(np.arange(len(avg)), avg - std, avg + std, alpha=0.2, label=f\"{label} Variance\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Average Reward\")\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Episode Duration\n",
    "def episodeDuration(env, rewardModel, episodes=500):\n",
    "    durations = []\n",
    "    for episode in range(episodes):\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        duration = 0\n",
    "\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            observation, reward, done, _, _ = env.step(action)\n",
    "            duration += 1  \n",
    "        durations.append(duration)\n",
    "\n",
    "    return durations\n",
    "\n",
    "# Get the episode durations\n",
    "durationsDict = {label: episodeDuration(env, reward_fn, episodes) for label, reward_fn in rewardFunctions}\n",
    "\n",
    "\n",
    "# Plot Episode Duration\n",
    "plt.subplot(2, 2, 3)\n",
    "for label, durations in durationsDict.items():\n",
    "    plt.plot(np.arange(episodes), durations, label=label)\n",
    "\n",
    "\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Episode Duration(Steps)\")\n",
    "plt.legend()\n",
    "\n",
    "# 4.action distribution\n",
    "def getActionDistributions(env, rewardModels, episodes):\n",
    "    action_distributions = {}\n",
    "\n",
    "    # Iterate through the list of tuples where each tuple is (rewardModelName, rewardModel)\n",
    "    for rewardModelName, rewardModel in rewardModels:\n",
    "        actions = []\n",
    "        for episode in range(episodes):\n",
    "            observation = env.reset()[0]\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                action = env.action_space.sample()  # Use agent's policy instead if needed\n",
    "                actions.append(action)\n",
    "                observation, _, done, _, _ = env.step(action)\n",
    "\n",
    "        action_distributions[rewardModelName] = actions  # Store the actions for this reward model\n",
    "\n",
    "    return action_distributions\n",
    "\n",
    "\n",
    "\n",
    "action_distributions = getActionDistributions(env, rewardFunctions, episodes)\n",
    "\n",
    "# Plot action distributions\n",
    "plt.figure(figsize=(10, 6))\n",
    "for rewardModelName, actions in action_distributions.items():\n",
    "    plt.hist(actions, bins=2, alpha=0.7, label=f\"{rewardModelName} Actions\")\n",
    "\n",
    "plt.xlabel(\"Actions (0=Left, 1=Right)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 5. Reward Distribution\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot for Baseline Rewards\n",
    "for i, (label, rewards) in enumerate(rewards_dict.items(), 1):\n",
    "    plt.subplot(1, 3, i)\n",
    "    plt.hist(rewards, bins=20, alpha=0.7)\n",
    "    plt.title(label)\n",
    "    plt.xlabel(\"Reward\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 6. Success Rate Calc\n",
    "# successThreshold = 200\n",
    "\n",
    "# baselineSuccessRate = sum([1 if r >= successThreshold else 0 for r in baselineRewards]) / episodes\n",
    "# llmSuccessRate1 = sum([1 if r >= successThreshold else 0 for r in LLM1Rewards]) / episodes\n",
    "# llmSuccessRate2 = sum([1 if r >= successThreshold else 0 for r in LLM2Rewards]) / episodes\n",
    "\n",
    "# print(f\"Baseline Success Rate: {baselineSuccessRate * 100:.2f}%\")\n",
    "# print(f\"LLM-Generated Success Rate 1: {llmSuccessRate1 * 100:.2f}%\")\n",
    "# print(f\"LLM-Generated Success Rate 2: {llmSuccessRate2 * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\samdd\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next Model: Baseline Reward Model\n",
      "Next Model: LLM-Generated Reward Model 1\n",
      "Next Model: LLM-Generated Reward Model 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def visualizeSuccessfulRuns(env, rewards_dict, durations_dict):\n",
    "    \n",
    "    # longest_runs_indices = np.argsort(durations)[-20:]  \n",
    "\n",
    "    env = gym.make('CartPole-v1', render_mode=\"human\")\n",
    "    agent = QLearningAgent(env)\n",
    "\n",
    "    for rewardModelName, rewards in rewards_dict.items():\n",
    "        durations = durations_dict[rewardModelName]\n",
    "        longest_runs_indices = np.argsort(durations)[-20:] # Get the indices of the top 20 longest runs\n",
    "\n",
    "        for idx in longest_runs_indices:\n",
    "            # print(f\"Duration: {durations[idx]}, Reward: {rewards[idx]}\")\n",
    "\n",
    "\n",
    "\n",
    "            observation, _ = env.reset()\n",
    "            state = agent.discretize(observation)\n",
    "            done = False\n",
    "\n",
    "\n",
    "            # print(f\"{rewardModelName} - Run {idx+1}\")\n",
    "\n",
    "            while not done:\n",
    "                action = agent.choose_action(state)\n",
    "                next_observation, reward, done, _, _ = env.step(action)\n",
    "                next_state = agent.discretize(next_observation)\n",
    "                state = next_state\n",
    "\n",
    "                # Render environment\n",
    "                env.render()\n",
    "\n",
    "                time.sleep(0.01)\n",
    "\n",
    "        print(f\"Next Model: {rewardModelName}\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "visualizeSuccessfulRuns(env, rewards_dict, durationsDict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maybe run many instances of reward models generated by Claude. Compare, with SoTA as baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can also add an explainability metric\n",
    "\n",
    "\n",
    "Prompt Engineering:\n",
    "\n",
    "Keep a log of the interaction with Claude. To see the developemnt. (See how these perform, run compute experiements)\n",
    "\n",
    "\n",
    "Eventually:(These can be developed by Claude)\n",
    "Adaptive learning rate \n",
    "Adaptive Reward Function\n",
    "\n",
    "\n",
    "\n",
    "Adaptive reward function for changes in enviornment. For example change the weight of the pole, add friction. How does the reward function adapt?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gymenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
