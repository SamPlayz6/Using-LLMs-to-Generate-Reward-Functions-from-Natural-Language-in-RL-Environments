{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\samdd\\anaconda3\\envs\\gymenv\\lib\\site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\samdd\\anaconda3\\envs\\gymenv\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\samdd\\anaconda3\\envs\\gymenv\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\samdd\\anaconda3\\envs\\gymenv\\lib\\site-packages (from gym) (3.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\samdd\\anaconda3\\envs\\gymenv\\lib\\site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\samdd\\anaconda3\\envs\\gymenv\\lib\\site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\samdd\\anaconda3\\envs\\gymenv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\samdd\\anaconda3\\envs\\gymenv\\lib\\site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\samdd\\anaconda3\\envs\\gymenv\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\samdd\\anaconda3\\envs\\gymenv\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\samdd\\anaconda3\\envs\\gymenv\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\samdd\\anaconda3\\envs\\gymenv\\lib\\site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\samdd\\anaconda3\\envs\\gymenv\\lib\\site-packages (from matplotlib) (2.9.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\samdd\\anaconda3\\envs\\gymenv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gym numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline SoTA reward function\n",
    "def SoTARewardFunction(observation, action):\n",
    "    x, _, theta, _ = observation\n",
    "    \n",
    "    if abs(theta) > np.deg2rad(15) or abs(x) > 2.4:\n",
    "        return 0\n",
    "    \n",
    "    return 1\n",
    "\n",
    "# Example LLM-generated reward functions\n",
    "def LLMRewardFunction1API(observation, action):\n",
    "    x, _, angle, _ = observation\n",
    "    reward = 1.0 - abs(angle) - 0.5 * abs(x)\n",
    "    return max(0, reward)\n",
    "\n",
    "def LLMRewardFunction2API(observation, action):\n",
    "    x, _, angle, _ = observation\n",
    "    reward = 1.0\n",
    "    \n",
    "    if abs(x) > 2.4 or abs(angle) > 0.2:\n",
    "        reward = -10.0\n",
    "    else:\n",
    "        reward += 10.0 / (1.0 + abs(angle))\n",
    "        reward += 5.0 / (1.0 + abs(x))\n",
    "    \n",
    "    return int(reward)\n",
    "\n",
    "\n",
    "def LLMRewardFunction1(observation, action):\n",
    "    # Unpack state variables\n",
    "    x, x_dot, angle, angle_dot = observation\n",
    "    \n",
    "    # Define constants\n",
    "    x_threshold = 2.4  # Cart position limit\n",
    "    angle_threshold = 12 * np.pi / 180  # Angle limit (12 degrees)\n",
    "    \n",
    "    # Check if episode is done\n",
    "    done = bool(\n",
    "        abs(x) > x_threshold or\n",
    "        abs(angle) > angle_threshold\n",
    "    )\n",
    "    \n",
    "    if not done:\n",
    "        # Reward for pole angle (closer to vertical is better)\n",
    "        angle_reward = 1 - abs(angle) / angle_threshold\n",
    "        \n",
    "        # Reward for cart position (closer to center is better)\n",
    "        position_reward = 1 - abs(x) / x_threshold\n",
    "        \n",
    "        # Reward for keeping the pole stable\n",
    "        stability_reward = 1 / (1 + abs(angle_dot))\n",
    "        \n",
    "        # Combine rewards\n",
    "        reward = angle_reward + position_reward + stability_reward\n",
    "    else:\n",
    "        # Penalty for ending episode\n",
    "        reward = -10\n",
    "    \n",
    "    return reward\n",
    "\n",
    "\n",
    "def LLMRewardFunction2(observation, action):\n",
    "    # Unpack state variables\n",
    "    x, x_dot, angle, angle_dot = observation\n",
    "    \n",
    "    # System parameters (now hardcoded as they can't be passed in)\n",
    "    m = 0.1  # pendulum mass\n",
    "    M = 1.0  # cart mass\n",
    "    l = 0.5  # pendulum length\n",
    "    g = 9.8  # gravity\n",
    "    \n",
    "    # Calculate potential energy\n",
    "    PE = m * g * l * (1 - np.cos(angle))\n",
    "    \n",
    "    # Calculate kinetic energy\n",
    "    KE_cart = 0.5 * M * x_dot**2\n",
    "    KE_pendulum = 0.5 * m * ((x_dot**2) + (l * angle_dot**2) + \n",
    "                             (2 * x_dot * l * angle_dot * np.cos(angle)))\n",
    "    KE_total = KE_cart + KE_pendulum\n",
    "    \n",
    "    # Calculate total energy\n",
    "    E_total = PE + KE_total\n",
    "    \n",
    "    # Define reward components\n",
    "    energy_reward = -E_total  # Minimize total energy\n",
    "    stability_reward = -abs(angle)  # Minimize angle deviation\n",
    "    \n",
    "    # Combine rewards (adjusted weights since we can't measure control effort now)\n",
    "    reward = 0.7 * energy_reward + 0.3 * stability_reward\n",
    "    \n",
    "    # Check if episode is done\n",
    "    x_threshold = 2.4\n",
    "    angle_threshold = np.pi / 2\n",
    "    done = bool(\n",
    "        abs(x) > x_threshold or\n",
    "        abs(angle) > angle_threshold\n",
    "    )\n",
    "    \n",
    "    if done:\n",
    "        reward -= 100  # Large penalty for ending episode\n",
    "    \n",
    "    return reward\n",
    "\n",
    "\n",
    "#---\n",
    "\n",
    "# Train the agent using Q-learning\n",
    "def train(env, agent, reward_model, episodes=500):\n",
    "    rewards = []\n",
    "    for episode in range(episodes):\n",
    "        observation = env.reset()[0]\n",
    "        state = agent.discretize(observation)\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.choose_action(state)\n",
    "            next_observation, reward, done, _, _ = env.step(action)\n",
    "            adjusted_reward = reward_model(next_observation, reward)\n",
    "            next_state = agent.discretize(next_observation)\n",
    "            agent.update(state, action, adjusted_reward, next_state)\n",
    "            state = next_state\n",
    "            total_reward += adjusted_reward\n",
    "        rewards.append(total_reward)\n",
    "    return rewards\n",
    "\n",
    "\n",
    "\n",
    "# episodes = 10000\n",
    "# baselineRewards = train(env, agent, SoTARewardFunction, episodes)\n",
    "# LLM1Rewards = train(env, agent, LLMRewardFunction1, episodes)\n",
    "# LLM2Rewards = train(env, agent, LLMRewardFunction2, episodes)\n",
    "\n",
    "# # Plot results\n",
    "# plt.plot(np.arange(episodes), LLM2Rewards, label=\"2nd LLM-Generated Reward Model\")\n",
    "# plt.plot(np.arange(episodes), baselineRewards, label=\"Custom Baseline Reward Model\")\n",
    "# plt.plot(np.arange(episodes), LLM1Rewards, label=\"1st LLM-Generated Reward Model\")\n",
    "\n",
    "# plt.xlabel(\"Episodes\")\n",
    "# plt.ylabel(\"Total Reward\")\n",
    "\n",
    "# plt.title(\"Reward Comparison over Time\")\n",
    "\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 56\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label, reward_fn \u001b[38;5;129;01min\u001b[39;00m rewardFunctions:\n\u001b[0;32m     55\u001b[0m     env\u001b[38;5;241m.\u001b[39msetRewardFunction(reward_fn)  \u001b[38;5;66;03m# Update the reward function in the environment\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m     rewardsDict[label] \u001b[38;5;241m=\u001b[39m \u001b[43mtrainDQLearning\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m#Total Reward Comparison\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\samdd\\Desktop\\College\\4th Year\\FYProject\\SimpleCartPoleImplementation\\cartPoleShared.py:234\u001b[0m, in \u001b[0;36mtrainDQLearning\u001b[1;34m(agent, env, episodes)\u001b[0m\n\u001b[0;32m    231\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m--> 234\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchooseAction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m     nextObservation, reward, done, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m    236\u001b[0m     totalReward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "File \u001b[1;32mc:\\Users\\samdd\\Desktop\\College\\4th Year\\FYProject\\SimpleCartPoleImplementation\\cartPoleShared.py:188\u001b[0m, in \u001b[0;36mDQLearningAgent.chooseAction\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;66;03m# Exploitation: choose the action with max Q-value for current state\u001b[39;00m\n\u001b[1;32m--> 188\u001b[0m     stateTensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)  \u001b[38;5;66;03m# Ensure tensor is on the correct device\u001b[39;00m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    190\u001b[0m         qValues \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(stateTensor)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Takes ~ 60 second to run\n",
    "from cartPoleShared import DQLearningAgent, CustomCartPoleEnv ,torch, trainDQLearning\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "baseEnv = gym.make('CartPole-v1')\n",
    "env = CustomCartPoleEnv(baseEnv)\n",
    "\n",
    "# State size is based on the number of observations in CartPole (4)\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Instantiate the DQLearning agent with the new parameters\n",
    "agent = DQLearningAgent(env, stateSize=state_size, actionSize=action_size, device=device)\n",
    "\n",
    "\n",
    "\n",
    "#This is temporary until I fix the error with the update function not being defined inthe calss\n",
    "def update(self, state, action, reward, next_state):\n",
    "    best_next_action = np.argmax(self.q_table[next_state])\n",
    "    td_target = reward + self.discount_factor * self.q_table[next_state][best_next_action]\n",
    "    td_error = td_target - self.q_table[state][action]\n",
    "    self.q_table[state][action] += self.learning_rate * td_error\n",
    "\n",
    "DQLearningAgent.update = update\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def movingAverageAndStd(data, windowSize=100):\n",
    "    average = np.convolve(data, np.ones(windowSize) / windowSize, mode='valid')\n",
    "    std = [np.std(data[i:i+windowSize]) for i in range(len(data) - windowSize + 1)]\n",
    "    return average, std\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# List of reward functions to abstract repeated calls\n",
    "rewardFunctions = [\n",
    "    (\"Baseline Reward Model\", SoTARewardFunction),\n",
    "    (\"LLM-Generated Reward Model 1 from API\", LLMRewardFunction1API),\n",
    "    (\"LLM-Generated Reward Model 2 from API\", LLMRewardFunction2API),\n",
    "    (\"LLM-Generated Reward Model 1 - Finetuned\", LLMRewardFunction1),\n",
    "    (\"LLM-Generated Reward Model 2 - Finetuned\", LLMRewardFunction2)\n",
    "\n",
    "]\n",
    "\n",
    "# Store results in a dictionary\n",
    "rewardsDict = {}\n",
    "\n",
    "# Run experiments for each reward function\n",
    "episodes = 1000\n",
    "for label, reward_fn in rewardFunctions:\n",
    "    env.setRewardFunction(reward_fn)  # Update the reward function in the environment\n",
    "    rewardsDict[label] = trainDQLearning(agent, env, episodes)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "\n",
    "#Total Reward Comparison\n",
    "plt.subplot(2, 2, 1)\n",
    "for label, rewards in rewardsDict.items():\n",
    "    plt.plot(np.arange(episodes), rewards, label=label)\n",
    "\n",
    "\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "# 2.Running Average of Rewards\n",
    "\n",
    "# avgBaseline, stdBaseline = movingAverageAndStd(baselineRewards, windowSize=100)\n",
    "# avgLLM1, stdLLM1 = movingAverageAndStd(LLM1Rewards, windowSize=100)\n",
    "# avgLLM2, stdLLM2 = movingAverageAndStd(LLM2Rewards, windowSize=100)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "\n",
    "for label, rewards in rewardsDict.items():\n",
    "    avg, std = movingAverageAndStd(rewards, windowSize=100)\n",
    "    plt.plot(np.arange(len(avg)), avg, label=label)\n",
    "    plt.fill_between(np.arange(len(avg)), avg - std, avg + std, alpha=0.2, label=f\"{label} Variance\")\n",
    "\n",
    "    # print(\"------------------\")\n",
    "    # print(avg)\n",
    "    # print(std)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Average Reward\")\n",
    "\n",
    "\n",
    "plt.xlim(0, episodes)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Episode Duration\n",
    "def episodeDuration(env, rewardModel, episodes=500):\n",
    "    durations = []\n",
    "    for episode in range(episodes):\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        duration = 0\n",
    "\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            observation, reward, done, _, _ = env.step(action)\n",
    "            duration += 1  \n",
    "        durations.append(duration)\n",
    "\n",
    "    return durations\n",
    "\n",
    "# Get the episode durations\n",
    "durationsDict = {label: episodeDuration(env, reward_fn, episodes) for label, reward_fn in rewardFunctions}\n",
    "\n",
    "\n",
    "# Plot Episode Duration\n",
    "plt.subplot(2, 2, 3)\n",
    "for label, durations in durationsDict.items():\n",
    "    plt.plot(np.arange(episodes), durations, label=label)\n",
    "\n",
    "\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Episode Duration(Steps)\")\n",
    "plt.legend()\n",
    "\n",
    "# 4.action distribution\n",
    "# def getActionDistributions(env, rewardModels, episodes):\n",
    "#     actionDistributions = {}\n",
    "#     for rewardModelName, rewardModel in rewardModels:\n",
    "#         actions = []\n",
    "#         for episode in range(episodes):\n",
    "#             observation = env.reset()[0]\n",
    "#             done = False\n",
    "\n",
    "#             while not done:\n",
    "#                 action = env.action_space.sample() \n",
    "#                 actions.append(action)\n",
    "#                 observation, _, done, _, _ = env.step(action)\n",
    "\n",
    "#         actionDistributions[rewardModelName] = actions \n",
    "\n",
    "#     return actionDistributions\n",
    "\n",
    "\n",
    "\n",
    "# actionDistributions = getActionDistributions(env, rewardFunctions, episodes)\n",
    "\n",
    "# # Plot action distributions\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# for rewardModelName, actions in actionDistributions.items():\n",
    "#     plt.hist(actions, bins=2, alpha=0.7, label=f\"{rewardModelName} Actions\")\n",
    "\n",
    "# plt.xlabel(\"Actions (0=Left, 1=Right)\")\n",
    "# plt.ylabel(\"Frequency\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# 5. Reward Distribution\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot for Baseline Rewards\n",
    "for i, (label, rewards) in enumerate(rewardsDict.items(), 1):\n",
    "    plt.subplot(1, len(rewardsDict.items()), i)\n",
    "    plt.hist(rewards, bins=20, alpha=0.7)\n",
    "    plt.title(label)\n",
    "    plt.xlabel(\"Reward\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 6. Success Rate Calc\n",
    "# successThreshold = 200\n",
    "\n",
    "# baselineSuccessRate = sum([1 if r >= successThreshold else 0 for r in baselineRewards]) / episodes\n",
    "# llmSuccessRate1 = sum([1 if r >= successThreshold else 0 for r in LLM1Rewards]) / episodes\n",
    "# llmSuccessRate2 = sum([1 if r >= successThreshold else 0 for r in LLM2Rewards]) / episodes\n",
    "\n",
    "# print(f\"Baseline Success Rate: {baselineSuccessRate * 100:.2f}%\")\n",
    "# print(f\"LLM-Generated Success Rate 1: {llmSuccessRate1 * 100:.2f}%\")\n",
    "# print(f\"LLM-Generated Success Rate 2: {llmSuccessRate2 * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Reward Model - Visualizing Run 96\n",
      "Baseline Reward Model - Visualizing Run 33\n",
      "Baseline Reward Model - Visualizing Run 48\n",
      "Baseline Reward Model - Visualizing Run 25\n",
      "Baseline Reward Model - Visualizing Run 57\n",
      "Baseline Reward Model - Visualizing Run 74\n",
      "Baseline Reward Model - Visualizing Run 21\n",
      "Baseline Reward Model - Visualizing Run 28\n",
      "Baseline Reward Model - Visualizing Run 77\n",
      "Baseline Reward Model - Visualizing Run 52\n",
      "Baseline Reward Model - Visualizing Run 32\n",
      "Baseline Reward Model - Visualizing Run 39\n",
      "Baseline Reward Model - Visualizing Run 49\n",
      "Baseline Reward Model - Visualizing Run 71\n",
      "Baseline Reward Model - Visualizing Run 54\n",
      "Baseline Reward Model - Visualizing Run 90\n",
      "Baseline Reward Model - Visualizing Run 36\n",
      "Baseline Reward Model - Visualizing Run 53\n",
      "Baseline Reward Model - Visualizing Run 97\n",
      "Baseline Reward Model - Visualizing Run 10\n",
      "Next Model: Baseline Reward Model\n",
      "LLM-Generated Reward Model 1 from API - Visualizing Run 18\n",
      "LLM-Generated Reward Model 1 from API - Visualizing Run 79\n",
      "LLM-Generated Reward Model 1 from API - Visualizing Run 62\n",
      "LLM-Generated Reward Model 1 from API - Visualizing Run 37\n",
      "LLM-Generated Reward Model 1 from API - Visualizing Run 32\n",
      "LLM-Generated Reward Model 1 from API - Visualizing Run 65\n",
      "LLM-Generated Reward Model 1 from API - Visualizing Run 33\n",
      "LLM-Generated Reward Model 1 from API - Visualizing Run 36\n",
      "LLM-Generated Reward Model 1 from API - Visualizing Run 4\n",
      "LLM-Generated Reward Model 1 from API - Visualizing Run 44\n",
      "LLM-Generated Reward Model 1 from API - Visualizing Run 72\n",
      "LLM-Generated Reward Model 1 from API - Visualizing Run 25\n",
      "LLM-Generated Reward Model 1 from API - Visualizing Run 19\n",
      "LLM-Generated Reward Model 1 from API - Visualizing Run 20\n",
      "LLM-Generated Reward Model 1 from API - Visualizing Run 67\n",
      "LLM-Generated Reward Model 1 from API - Visualizing Run 89\n",
      "LLM-Generated Reward Model 1 from API - Visualizing Run 26\n",
      "LLM-Generated Reward Model 1 from API - Visualizing Run 1\n",
      "LLM-Generated Reward Model 1 from API - Visualizing Run 84\n",
      "LLM-Generated Reward Model 1 from API - Visualizing Run 55\n",
      "Next Model: LLM-Generated Reward Model 1 from API\n",
      "LLM-Generated Reward Model 2 from API - Visualizing Run 22\n",
      "LLM-Generated Reward Model 2 from API - Visualizing Run 17\n",
      "LLM-Generated Reward Model 2 from API - Visualizing Run 65\n",
      "LLM-Generated Reward Model 2 from API - Visualizing Run 34\n",
      "LLM-Generated Reward Model 2 from API - Visualizing Run 92\n",
      "LLM-Generated Reward Model 2 from API - Visualizing Run 83\n",
      "LLM-Generated Reward Model 2 from API - Visualizing Run 48\n",
      "LLM-Generated Reward Model 2 from API - Visualizing Run 18\n",
      "LLM-Generated Reward Model 2 from API - Visualizing Run 33\n",
      "LLM-Generated Reward Model 2 from API - Visualizing Run 54\n",
      "LLM-Generated Reward Model 2 from API - Visualizing Run 30\n",
      "LLM-Generated Reward Model 2 from API - Visualizing Run 81\n",
      "LLM-Generated Reward Model 2 from API - Visualizing Run 70\n",
      "LLM-Generated Reward Model 2 from API - Visualizing Run 55\n",
      "LLM-Generated Reward Model 2 from API - Visualizing Run 84\n",
      "LLM-Generated Reward Model 2 from API - Visualizing Run 9\n",
      "LLM-Generated Reward Model 2 from API - Visualizing Run 76\n",
      "LLM-Generated Reward Model 2 from API - Visualizing Run 16\n",
      "LLM-Generated Reward Model 2 from API - Visualizing Run 13\n",
      "LLM-Generated Reward Model 2 from API - Visualizing Run 86\n",
      "Next Model: LLM-Generated Reward Model 2 from API\n",
      "LLM-Generated Reward Model 1 - Finetuned - Visualizing Run 38\n",
      "LLM-Generated Reward Model 1 - Finetuned - Visualizing Run 43\n",
      "LLM-Generated Reward Model 1 - Finetuned - Visualizing Run 3\n",
      "LLM-Generated Reward Model 1 - Finetuned - Visualizing Run 51\n",
      "LLM-Generated Reward Model 1 - Finetuned - Visualizing Run 57\n",
      "LLM-Generated Reward Model 1 - Finetuned - Visualizing Run 93\n",
      "LLM-Generated Reward Model 1 - Finetuned - Visualizing Run 81\n",
      "LLM-Generated Reward Model 1 - Finetuned - Visualizing Run 34\n",
      "LLM-Generated Reward Model 1 - Finetuned - Visualizing Run 88\n",
      "LLM-Generated Reward Model 1 - Finetuned - Visualizing Run 98\n",
      "LLM-Generated Reward Model 1 - Finetuned - Visualizing Run 56\n",
      "LLM-Generated Reward Model 1 - Finetuned - Visualizing Run 74\n",
      "LLM-Generated Reward Model 1 - Finetuned - Visualizing Run 13\n",
      "LLM-Generated Reward Model 1 - Finetuned - Visualizing Run 48\n",
      "LLM-Generated Reward Model 1 - Finetuned - Visualizing Run 77\n",
      "LLM-Generated Reward Model 1 - Finetuned - Visualizing Run 33\n",
      "LLM-Generated Reward Model 1 - Finetuned - Visualizing Run 2\n",
      "LLM-Generated Reward Model 1 - Finetuned - Visualizing Run 91\n",
      "LLM-Generated Reward Model 1 - Finetuned - Visualizing Run 75\n",
      "LLM-Generated Reward Model 1 - Finetuned - Visualizing Run 84\n",
      "Next Model: LLM-Generated Reward Model 1 - Finetuned\n",
      "LLM-Generated Reward Model 2 - Finetuned - Visualizing Run 12\n",
      "LLM-Generated Reward Model 2 - Finetuned - Visualizing Run 6\n",
      "LLM-Generated Reward Model 2 - Finetuned - Visualizing Run 82\n",
      "LLM-Generated Reward Model 2 - Finetuned - Visualizing Run 59\n",
      "LLM-Generated Reward Model 2 - Finetuned - Visualizing Run 13\n",
      "LLM-Generated Reward Model 2 - Finetuned - Visualizing Run 96\n",
      "LLM-Generated Reward Model 2 - Finetuned - Visualizing Run 88\n",
      "LLM-Generated Reward Model 2 - Finetuned - Visualizing Run 71\n",
      "LLM-Generated Reward Model 2 - Finetuned - Visualizing Run 51\n",
      "LLM-Generated Reward Model 2 - Finetuned - Visualizing Run 49\n",
      "LLM-Generated Reward Model 2 - Finetuned - Visualizing Run 39\n",
      "LLM-Generated Reward Model 2 - Finetuned - Visualizing Run 24\n",
      "LLM-Generated Reward Model 2 - Finetuned - Visualizing Run 21\n",
      "LLM-Generated Reward Model 2 - Finetuned - Visualizing Run 22\n",
      "LLM-Generated Reward Model 2 - Finetuned - Visualizing Run 94\n",
      "LLM-Generated Reward Model 2 - Finetuned - Visualizing Run 36\n",
      "LLM-Generated Reward Model 2 - Finetuned - Visualizing Run 86\n",
      "LLM-Generated Reward Model 2 - Finetuned - Visualizing Run 99\n",
      "LLM-Generated Reward Model 2 - Finetuned - Visualizing Run 10\n",
      "LLM-Generated Reward Model 2 - Finetuned - Visualizing Run 50\n",
      "Next Model: LLM-Generated Reward Model 2 - Finetuned\n"
     ]
    }
   ],
   "source": [
    "def visualizeSuccessfulRuns(env, rewards_dict, durations_dict):\n",
    "    # Initialize the environment for visualization\n",
    "    env = gym.make('CartPole-v1', render_mode=\"human\")\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    # Initialize the DQLearningAgent\n",
    "    agent = DQLearningAgent(env, state_size, action_size, device)\n",
    "\n",
    "    # Set epsilon to a very low value to prioritize exploitation during visualization\n",
    "    agent.epsilon = 0.01\n",
    "\n",
    "    for rewardModelName, rewards in rewards_dict.items():\n",
    "        durations = durations_dict[rewardModelName]\n",
    "        longest_runs_indices = np.argsort(durations)[-20:]  # Get the indices of the top 20 longest runs\n",
    "\n",
    "        for idx in longest_runs_indices:\n",
    "            print(f\"{rewardModelName} - Visualizing Run {idx+1}\")\n",
    "\n",
    "            observation, _ = env.reset()\n",
    "            state = observation\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                action = agent.chooseAction(state)\n",
    "                next_observation, reward, done, _, _ = env.step(action)\n",
    "                next_state = next_observation\n",
    "                state = next_state\n",
    "\n",
    "                # Render environment\n",
    "                env.render()\n",
    "\n",
    "                time.sleep(0.01)\n",
    "\n",
    "        print(f\"Next Model: {rewardModelName}\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "# Call the visualization function\n",
    "visualizeSuccessfulRuns(env, rewardsDict, durationsDict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maybe run many instances of reward models generated by Claude. Compare, with SoTA as baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Can also add an explainability metric**\n",
    "\n",
    "\n",
    "Prompt Engineering:\n",
    "Keep a log of the interaction with Claude. To see the developemnt. (See how these perform, run compute experiements)\n",
    "\n",
    "\n",
    "Eventually:(These can be developed by Claude)\n",
    "Adaptive learning rate \n",
    "Adaptive Reward Function - (For example change the weight of the pole, add friction. How does the reward function adapt?)\n",
    "\n",
    "\n",
    "\n",
    "-- Meeting 17/10/24 --\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Adaptive Learning Rate Work:\n",
    "\n",
    "Gradient of the reward function -> Informs the learning rate\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "YOU NEED TO:\n",
    "\n",
    "- Have a look at the work that already exists here. (e.g. Have a look at A2C with reward functions)\n",
    "\n",
    "- Run long simulations. Explain what is happenig in simulations (Why are there peaks, what is happening with the variance in the model? - Use Claudes Explanations)\n",
    "\n",
    "- Alter all code to run with Deep Q-Learning instead of Q-Learning\n",
    "\n",
    "\n",
    "Compare and contrast work already done. Look for space where I can contribute. What can I do that is different. Also whata type of work is already being done?:\n",
    "https://github.com/360ZMEM/LLMRsearcher-code\n",
    "https://360zmem.github.io/LLMRsearcher/\n",
    "\n",
    "https://github.com/schashni/Eureka \n",
    "\n",
    "\n",
    "**NO MORE PROGRAMMING UNTIL I HAVE A STRONG IDEA OF THE DOMAIN OF THE FIELD*\n",
    "\n",
    "\n",
    "\n",
    "-- \n",
    "\n",
    "You can create a virtual environment on the Lab PC to run your simulations.\n",
    "Use PuTTY & WinSCP\n",
    "\n",
    "Need to change to Deep Q-Learning or other learning method that utilizes GPUs.\n",
    "Also can implement parallel simulations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
