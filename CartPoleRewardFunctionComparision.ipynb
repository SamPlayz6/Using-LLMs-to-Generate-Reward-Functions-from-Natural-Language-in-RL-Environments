{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in /home/sd37/.local/lib/python3.10/site-packages (1.0.0)\n",
      "Requirement already satisfied: numpy in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (2.1.2)\n",
      "Requirement already satisfied: matplotlib in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (3.9.2)\n",
      "Requirement already satisfied: anthropic in /home/sd37/.local/lib/python3.10/site-packages (0.37.1)\n",
      "Requirement already satisfied: torch in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (2.5.0)\n",
      "Requirement already satisfied: pygame in /home/sd37/.local/lib/python3.10/site-packages (2.6.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from gymnasium) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/sd37/.local/lib/python3.10/site-packages (from gymnasium) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/sd37/.local/lib/python3.10/site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from matplotlib) (2.9.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from anthropic) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/sd37/.local/lib/python3.10/site-packages (from anthropic) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/sd37/.local/lib/python3.10/site-packages (from anthropic) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/sd37/.local/lib/python3.10/site-packages (from anthropic) (0.6.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/sd37/.local/lib/python3.10/site-packages (from anthropic) (2.9.2)\n",
      "Requirement already satisfied: sniffio in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from anthropic) (1.3.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.0 in /home/sd37/.local/lib/python3.10/site-packages (from anthropic) (0.20.1)\n",
      "Requirement already satisfied: filelock in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: networkx in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/sd37/.local/lib/python3.10/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
      "Requirement already satisfied: exceptiongroup in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from anyio<5,>=3.5.0->anthropic) (1.2.2)\n",
      "Requirement already satisfied: certifi in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from httpx<1,>=0.23.0->anthropic) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /home/sd37/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->anthropic) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/sd37/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/sd37/.local/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/sd37/.local/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->anthropic) (2.23.4)\n",
      "Requirement already satisfied: six>=1.5 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/sd37/.local/lib/python3.10/site-packages (from tokenizers>=0.13.0->anthropic) (0.26.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (6.0)\n",
      "Requirement already satisfied: requests in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (4.66.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gymnasium numpy matplotlib anthropic torch pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: typing_extensions in /home/sd37/.local/lib/python3.10/site-packages (4.12.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install typing_extensions --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available. Using: NVIDIA GeForce RTX 3090\n",
      "CUDA available: True\n",
      "Number of GPUs: 1\n",
      "Device Name: NVIDIA GeForce RTX 3090\n",
      "/home/sd37/.conda/envs/thesis/bin/python\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available. Using:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available. Using CPU.\")\n",
    "    \n",
    "    \n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device Name:\", torch.cuda.get_device_name(0))\n",
    "    \n",
    "    \n",
    "    \n",
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline SoTA reward function\n",
    "def SoTARewardFunction(observation, action):\n",
    "    x, _, theta, _ = observation\n",
    "    \n",
    "    if abs(theta) > np.deg2rad(15) or abs(x) > 2.4:\n",
    "        return 0\n",
    "    \n",
    "    return 1\n",
    "\n",
    "# Example LLM-generated reward functions\n",
    "def LLMRewardFunction1API(observation, action):\n",
    "    x, _, angle, _ = observation\n",
    "    reward = 1.0 - abs(angle) - 0.5 * abs(x)\n",
    "    return max(0, reward)\n",
    "\n",
    "def LLMRewardFunction2API(observation, action):\n",
    "    x, _, angle, _ = observation\n",
    "    reward = 1.0\n",
    "    \n",
    "    if abs(x) > 2.4 or abs(angle) > 0.2:\n",
    "        reward = -10.0\n",
    "    else:\n",
    "        reward += 10.0 / (1.0 + abs(angle))\n",
    "        reward += 5.0 / (1.0 + abs(x))\n",
    "    \n",
    "    return int(reward)\n",
    "\n",
    "\n",
    "def LLMRewardFunction1(observation, action):\n",
    "    # Unpack state variables\n",
    "    x, x_dot, angle, angle_dot = observation\n",
    "    \n",
    "    # Define constants\n",
    "    x_threshold = 2.4  # Cart position limit\n",
    "    angle_threshold = 12 * np.pi / 180  # Angle limit (12 degrees)\n",
    "    \n",
    "    # Check if episode is done\n",
    "    done = bool(\n",
    "        abs(x) > x_threshold or\n",
    "        abs(angle) > angle_threshold\n",
    "    )\n",
    "    \n",
    "    if not done:\n",
    "        # Reward for pole angle (closer to vertical is better)\n",
    "        angle_reward = 1 - abs(angle) / angle_threshold\n",
    "        \n",
    "        # Reward for cart position (closer to center is better)\n",
    "        position_reward = 1 - abs(x) / x_threshold\n",
    "        \n",
    "        # Reward for keeping the pole stable\n",
    "        stability_reward = 1 / (1 + abs(angle_dot))\n",
    "        \n",
    "        # Combine rewards\n",
    "        reward = angle_reward + position_reward + stability_reward\n",
    "    else:\n",
    "        # Penalty for ending episode\n",
    "        reward = -10\n",
    "    \n",
    "    return reward\n",
    "\n",
    "\n",
    "def LLMRewardFunction2(observation, action):\n",
    "    # Unpack state variables\n",
    "    x, x_dot, angle, angle_dot = observation\n",
    "    \n",
    "    # System parameters (now hardcoded as they can't be passed in)\n",
    "    m = 0.1  # pendulum mass\n",
    "    M = 1.0  # cart mass\n",
    "    l = 0.5  # pendulum length\n",
    "    g = 9.8  # gravity\n",
    "    \n",
    "    # Calculate potential energy\n",
    "    PE = m * g * l * (1 - np.cos(angle))\n",
    "    \n",
    "    # Calculate kinetic energy\n",
    "    KE_cart = 0.5 * M * x_dot**2\n",
    "    KE_pendulum = 0.5 * m * ((x_dot**2) + (l * angle_dot**2) + \n",
    "                             (2 * x_dot * l * angle_dot * np.cos(angle)))\n",
    "    KE_total = KE_cart + KE_pendulum\n",
    "    \n",
    "    # Calculate total energy\n",
    "    E_total = PE + KE_total\n",
    "    \n",
    "    # Define reward components\n",
    "    energy_reward = -E_total  # Minimize total energy\n",
    "    stability_reward = -abs(angle)  # Minimize angle deviation\n",
    "    \n",
    "    # Combine rewards (adjusted weights since we can't measure control effort now)\n",
    "    reward = 0.7 * energy_reward + 0.3 * stability_reward\n",
    "    \n",
    "    # Check if episode is done\n",
    "    x_threshold = 2.4\n",
    "    angle_threshold = np.pi / 2\n",
    "    done = bool(\n",
    "        abs(x) > x_threshold or\n",
    "        abs(angle) > angle_threshold\n",
    "    )\n",
    "    \n",
    "    if done:\n",
    "        reward -= 100  # Large penalty for ending episode\n",
    "    \n",
    "    return reward\n",
    "\n",
    "\n",
    "#---\n",
    "\n",
    "# Train the agent using Q-learning\n",
    "def train(env, agent, reward_model, episodes=500):\n",
    "    rewards = []\n",
    "    for episode in range(episodes):\n",
    "        observation = env.reset()[0]\n",
    "        state = agent.discretize(observation)\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.choose_action(state)\n",
    "            next_observation, reward, done, _, _ = env.step(action)\n",
    "            adjusted_reward = reward_model(next_observation, reward)\n",
    "            next_state = agent.discretize(next_observation)\n",
    "            agent.update(state, action, adjusted_reward, next_state)\n",
    "            state = next_state\n",
    "            total_reward += adjusted_reward\n",
    "        rewards.append(total_reward)\n",
    "    return rewards\n",
    "\n",
    "\n",
    "\n",
    "# episodes = 10000\n",
    "# baselineRewards = train(env, agent, SoTARewardFunction, episodes)\n",
    "# LLM1Rewards = train(env, agent, LLMRewardFunction1, episodes)\n",
    "# LLM2Rewards = train(env, agent, LLMRewardFunction2, episodes)\n",
    "\n",
    "# # Plot results\n",
    "# plt.plot(np.arange(episodes), LLM2Rewards, label=\"2nd LLM-Generated Reward Model\")\n",
    "# plt.plot(np.arange(episodes), baselineRewards, label=\"Custom Baseline Reward Model\")\n",
    "# plt.plot(np.arange(episodes), LLM1Rewards, label=\"1st LLM-Generated Reward Model\")\n",
    "\n",
    "# plt.xlabel(\"Episodes\")\n",
    "# plt.ylabel(\"Total Reward\")\n",
    "\n",
    "# plt.title(\"Reward Comparison over Time\")\n",
    "\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 56\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label, reward_fn \u001b[38;5;129;01min\u001b[39;00m rewardFunctions:\n\u001b[1;32m     55\u001b[0m     env\u001b[38;5;241m.\u001b[39msetRewardFunction(reward_fn)  \u001b[38;5;66;03m# Update the reward function in the environment\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m     rewardsDict[label] \u001b[38;5;241m=\u001b[39m \u001b[43mtrainDQLearning\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m#Total Reward Comparison\u001b[39;00m\n",
      "File \u001b[0;32m~/BachelorsThesis/Using-LLMs-to-Generate-Reward-Functions-from-Natural-Language-in-RL-Environments/cartPoleShared.py:237\u001b[0m, in \u001b[0;36mtrainDQLearning\u001b[0;34m(agent, env, episodes)\u001b[0m\n\u001b[1;32m    235\u001b[0m rewards \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(episodes):\n\u001b[0;32m--> 237\u001b[0m     observation \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    238\u001b[0m     totalReward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    239\u001b[0m     done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/BachelorsThesis/Using-LLMs-to-Generate-Reward-Functions-from-Natural-Language-in-RL-Environments/cartPoleShared.py:190\u001b[0m, in \u001b[0;36mchooseAction\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchooseAction\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon:\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;66;03m# Exploration: choose a random action\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;66;03m# Exploitation: choose the action with max Q-value for current state\u001b[39;00m\n\u001b[1;32m    193\u001b[0m         stateTensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(state, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)  \u001b[38;5;66;03m# Ensure tensor is on the correct device\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Takes ~ 60 second to run\n",
    "from cartPoleShared import DQLearningAgent, CustomCartPoleEnv ,torch, trainDQLearning\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "baseEnv = gym.make('CartPole-v1')\n",
    "env = CustomCartPoleEnv(baseEnv)\n",
    "\n",
    "# State size is based on the number of observations in CartPole (4)\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Instantiate the DQLearning agent with the new parameters\n",
    "agent = DQLearningAgent(env, stateSize=state_size, actionSize=action_size, device=device)\n",
    "\n",
    "\n",
    "\n",
    "#This is temporary until I fix the error with the update function not being defined inthe calss\n",
    "def update(self, state, action, reward, next_state):\n",
    "    best_next_action = np.argmax(self.q_table[next_state])\n",
    "    td_target = reward + self.discount_factor * self.q_table[next_state][best_next_action]\n",
    "    td_error = td_target - self.q_table[state][action]\n",
    "    self.q_table[state][action] += self.learning_rate * td_error\n",
    "\n",
    "DQLearningAgent.update = update\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def movingAverageAndStd(data, windowSize=100):\n",
    "    average = np.convolve(data, np.ones(windowSize) / windowSize, mode='valid')\n",
    "    std = [np.std(data[i:i+windowSize]) for i in range(len(data) - windowSize + 1)]\n",
    "    return average, std\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# List of reward functions to abstract repeated calls\n",
    "rewardFunctions = [\n",
    "    (\"Baseline Reward Model\", SoTARewardFunction),\n",
    "    (\"LLM-Generated Reward Model 1 from API\", LLMRewardFunction1API),\n",
    "    (\"LLM-Generated Reward Model 2 from API\", LLMRewardFunction2API),\n",
    "    (\"LLM-Generated Reward Model 1 - Finetuned\", LLMRewardFunction1),\n",
    "    (\"LLM-Generated Reward Model 2 - Finetuned\", LLMRewardFunction2)\n",
    "\n",
    "]\n",
    "\n",
    "# Store results in a dictionary\n",
    "rewardsDict = {}\n",
    "\n",
    "# Run experiments for each reward function\n",
    "episodes = 1000\n",
    "for label, reward_fn in rewardFunctions:\n",
    "    env.setRewardFunction(reward_fn)  # Update the reward function in the environment\n",
    "    rewardsDict[label] = trainDQLearning(agent, env, episodes)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "\n",
    "#Total Reward Comparison\n",
    "plt.subplot(2, 2, 1)\n",
    "for label, rewards in rewardsDict.items():\n",
    "    plt.plot(np.arange(episodes), rewards, label=label)\n",
    "\n",
    "\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "# 2.Running Average of Rewards\n",
    "\n",
    "# avgBaseline, stdBaseline = movingAverageAndStd(baselineRewards, windowSize=100)\n",
    "# avgLLM1, stdLLM1 = movingAverageAndStd(LLM1Rewards, windowSize=100)\n",
    "# avgLLM2, stdLLM2 = movingAverageAndStd(LLM2Rewards, windowSize=100)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "\n",
    "for label, rewards in rewardsDict.items():\n",
    "    avg, std = movingAverageAndStd(rewards, windowSize=50)\n",
    "    plt.plot(np.arange(len(avg)), avg, label=label)\n",
    "    plt.fill_between(np.arange(len(avg)), avg - std, avg + std, alpha=0.2, label=f\"{label} Variance\")\n",
    "\n",
    "    # print(\"------------------\")\n",
    "    # print(avg)\n",
    "    # print(std)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Average Reward\")\n",
    "\n",
    "\n",
    "plt.xlim(0, episodes)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Episode Duration\n",
    "def episodeDuration(env, rewardModel, episodes=500):\n",
    "    durations = []\n",
    "    for episode in range(episodes):\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        duration = 0\n",
    "\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            observation, reward, done, _, _ = env.step(action)\n",
    "            duration += 1  \n",
    "        durations.append(duration)\n",
    "\n",
    "    return durations\n",
    "\n",
    "# Get the episode durations\n",
    "durationsDict = {label: episodeDuration(env, reward_fn, episodes) for label, reward_fn in rewardFunctions}\n",
    "\n",
    "\n",
    "# Plot Episode Duration\n",
    "plt.subplot(2, 2, 3)\n",
    "for label, durations in durationsDict.items():\n",
    "    plt.plot(np.arange(episodes), durations, label=label)\n",
    "\n",
    "\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Episode Duration(Steps)\")\n",
    "plt.legend()\n",
    "\n",
    "# 4.action distribution\n",
    "# def getActionDistributions(env, rewardModels, episodes):\n",
    "#     actionDistributions = {}\n",
    "#     for rewardModelName, rewardModel in rewardModels:\n",
    "#         actions = []\n",
    "#         for episode in range(episodes):\n",
    "#             observation = env.reset()[0]\n",
    "#             done = False\n",
    "\n",
    "#             while not done:\n",
    "#                 action = env.action_space.sample() \n",
    "#                 actions.append(action)\n",
    "#                 observation, _, done, _, _ = env.step(action)\n",
    "\n",
    "#         actionDistributions[rewardModelName] = actions \n",
    "\n",
    "#     return actionDistributions\n",
    "\n",
    "\n",
    "\n",
    "# actionDistributions = getActionDistributions(env, rewardFunctions, episodes)\n",
    "\n",
    "# # Plot action distributions\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# for rewardModelName, actions in actionDistributions.items():\n",
    "#     plt.hist(actions, bins=2, alpha=0.7, label=f\"{rewardModelName} Actions\")\n",
    "\n",
    "# plt.xlabel(\"Actions (0=Left, 1=Right)\")\n",
    "# plt.ylabel(\"Frequency\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# 5. Reward Distribution\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot for Baseline Rewards\n",
    "for i, (label, rewards) in enumerate(rewardsDict.items(), 1):\n",
    "    plt.subplot(1, len(rewardsDict.items()), i)\n",
    "    plt.hist(rewards, bins=20, alpha=0.7)\n",
    "    plt.title(label)\n",
    "    plt.xlabel(\"Reward\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 6. Success Rate Calc\n",
    "# successThreshold = 200\n",
    "\n",
    "# baselineSuccessRate = sum([1 if r >= successThreshold else 0 for r in baselineRewards]) / episodes\n",
    "# llmSuccessRate1 = sum([1 if r >= successThreshold else 0 for r in LLM1Rewards]) / episodes\n",
    "# llmSuccessRate2 = sum([1 if r >= successThreshold else 0 for r in LLM2Rewards]) / episodes\n",
    "\n",
    "# print(f\"Baseline Success Rate: {baselineSuccessRate * 100:.2f}%\")\n",
    "# print(f\"LLM-Generated Success Rate 1: {llmSuccessRate1 * 100:.2f}%\")\n",
    "# print(f\"LLM-Generated Success Rate 2: {llmSuccessRate2 * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeSuccessfulRuns(env, rewards_dict, durations_dict):\n",
    "    # Initialize the environment for visualization\n",
    "    env = gym.make('CartPole-v1', render_mode=\"human\")\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    # Initialize the DQLearningAgent\n",
    "    agent = DQLearningAgent(env, state_size, action_size, device)\n",
    "\n",
    "    # Set epsilon to a very low value to prioritize exploitation during visualization\n",
    "    agent.epsilon = 0.01\n",
    "\n",
    "    for rewardModelName, rewards in rewards_dict.items():\n",
    "        durations = durations_dict[rewardModelName]\n",
    "        longest_runs_indices = np.argsort(durations)[-20:]  # Get the indices of the top 20 longest runs\n",
    "\n",
    "        for idx in longest_runs_indices:\n",
    "            print(f\"{rewardModelName} - Visualizing Run {idx+1}\")\n",
    "\n",
    "            observation, _ = env.reset()\n",
    "            state = observation\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                action = agent.chooseAction(state)\n",
    "                next_observation, reward, done, _, _ = env.step(action)\n",
    "                next_state = next_observation\n",
    "                state = next_state\n",
    "\n",
    "                # Render environment\n",
    "                env.render()\n",
    "\n",
    "                time.sleep(0.01)\n",
    "\n",
    "        print(f\"Next Model: {rewardModelName}\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "# Call the visualization function\n",
    "visualizeSuccessfulRuns(env, rewardsDict, durationsDict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maybe run many instances of reward models generated by Claude. Compare, with SoTA as baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Can also add an explainability metric**\n",
    "\n",
    "\n",
    "Prompt Engineering:\n",
    "Keep a log of the interaction with Claude. To see the developemnt. (See how these perform, run compute experiements)\n",
    "\n",
    "\n",
    "Eventually:(These can be developed by Claude)\n",
    "Adaptive learning rate \n",
    "Adaptive Reward Function - (For example change the weight of the pole, add friction. How does the reward function adapt?)\n",
    "\n",
    "\n",
    "\n",
    "-- Meeting 17/10/24 --\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Adaptive Learning Rate Work:\n",
    "\n",
    "Gradient of the reward function -> Informs the learning rate\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "YOU NEED TO:\n",
    "\n",
    "- Have a look at the work that already exists here. (e.g. Have a look at A2C with reward functions)\n",
    "\n",
    "- Run long simulations. Explain what is happenig in simulations (Why are there peaks, what is happening with the variance in the model? - Use Claudes Explanations)\n",
    "\n",
    "- Alter all code to run with Deep Q-Learning instead of Q-Learning\n",
    "\n",
    "\n",
    "Compare and contrast work already done. Look for space where I can contribute. What can I do that is different. Also whata type of work is already being done?:\n",
    "https://github.com/360ZMEM/LLMRsearcher-code\n",
    "https://360zmem.github.io/LLMRsearcher/\n",
    "\n",
    "https://github.com/schashni/Eureka \n",
    "\n",
    "\n",
    "**NO MORE PROGRAMMING UNTIL I HAVE A STRONG IDEA OF THE DOMAIN OF THE FIELD*\n",
    "\n",
    "\n",
    "\n",
    "-- \n",
    "\n",
    "You can create a virtual environment on the Lab PC to run your simulations.\n",
    "Use PuTTY & WinSCP\n",
    "\n",
    "Need to change to Deep Q-Learning or other learning method that utilizes GPUs.\n",
    "Also can implement parallel simulations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
