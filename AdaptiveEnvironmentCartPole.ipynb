{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a CartPole environment where environment variables are altered throughout the training runs with an adaptive reward function to account for environment changes.\n",
    "\n",
    "Adaptive reward function implemented by Claude. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available. Using: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "# Import necessary packages\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Import your shared methods and classes from cartPoleShared.py\n",
    "from cartPoleShared import DQLearningAgent, CustomCartPoleEnv, queryAnthropicApi, logClaudeCall, extractFunctionCode, torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available. Using:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available. Using CPU.\")\n",
    "\n",
    "\n",
    "# Define Claude API constants\n",
    "apiKey=\"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\"\n",
    "modelName = \"claude-3-5-sonnet-20240620\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the environment and agent\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "customEnv = CustomCartPoleEnv(env)\n",
    "\n",
    "# Updated to use DQLearningAgent\n",
    "stateSize = env.observation_space.shape[0]\n",
    "actionSize = env.action_space.n\n",
    "agent = DQLearningAgent(customEnv, stateSize=stateSize, actionSize=actionSize, device=device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The environment is modified periodically, specifically every 100 episodes.\n",
    "\n",
    "The parameters being changed are:\n",
    "- Mass of the Cart (masscart): Adjusted randomly between 0.5 and 2.0.\n",
    "- Length of the Pole (length): Adjusted randomly between 0.5 and 2.0.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*Improvements:*\n",
    "- Add Envronment changes once the agent performs to a certain level.\n",
    "- Optimize the reward function generation part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of episodes to train\n",
    "episodes = 1000\n",
    "updateInterval = 100  # Change environment every 100 episodes\n",
    "\n",
    "# Train the agent and update environment/reward functions periodically\n",
    "def trainAgentWithDynamicUpdates(agent, env, episodes = episodes, updateInterval = updateInterval):\n",
    "    rewards = []\n",
    "    currentRewardFunction = \"def dynamicRewardFunction(observation, action):\\n    return 0\"  # Initial dummy reward function\n",
    "    currentExplanation = \"This is the initial reward function with no adaptation.\"\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        # Reset environment for each episode\n",
    "        observation = env.reset()[0]\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Agent chooses action and steps in environment\n",
    "            action = agent.chooseAction(observation)\n",
    "            next_observation, reward, done, _, _ = env.step(action)\n",
    "            agent.remember(observation, action, reward, next_observation, done)\n",
    "            observation = next_observation\n",
    "            total_reward += reward\n",
    "\n",
    "        # After each episode, perform training from the replay buffer\n",
    "        agent.replay(batchSize=32)\n",
    "\n",
    "        # Append total rewards for analysis\n",
    "        rewards.append(total_reward)\n",
    "\n",
    "        # Hardcoded environment changes every update interval\n",
    "        if episode % updateInterval == 0 and episode > 0:\n",
    "            new_masscart = np.random.uniform(0.5, 2.0)  # Randomize cart mass\n",
    "            new_length = np.random.uniform(0.5, 2.0)  # Randomize pole length\n",
    "            env.setEnvironmentParameters(masscart=new_masscart, length=new_length)\n",
    "            print(f\"Environment updated at episode {episode}. New masscart: {new_masscart}, New length: {new_length}\")\n",
    "\n",
    "            # Query Claude to get a new reward function with environmental changes and previous context\n",
    "            reward_prompt = f\"\"\"\n",
    "            You are a Python code outputter. Your task is to adapt the existing reward function below based on the given environment changes.\n",
    "            I want your output to only be Python code with one function definition.\n",
    "            Do not include any other text before or after the function definition.\n",
    "\n",
    "            Current Reward Function:\n",
    "            {currentRewardFunction}\n",
    "\n",
    "            Environment Changes:\n",
    "            - The cart mass is now {new_masscart}\n",
    "            - The pole length is now {new_length}\n",
    "\n",
    "            Current Explanation:\n",
    "            {currentExplanation}\n",
    "\n",
    "            Please adapt the reward function to suit the new environment. The inputs are observation and action in that order.\n",
    "            The input observation can be broken down as follows: x, _, angle, _ = observation.\n",
    "\n",
    "            Important Requirements:\n",
    "            1. The output reward function must always return a **numeric (float)** reward value.\n",
    "            2. Ensure **all possible conditions are handled** such that the function never returns None.\n",
    "            3. If an error occurs or the reward cannot be computed, the function must **return a default value of 0**.\n",
    "            4. The reward should be non-negative, so ensure the reward is at least 0 (use `max(reward, 0)` if necessary).\n",
    "            5. Add proper **exception handling** to guarantee that the function always returns a valid numeric value, even if unexpected situations occur.\n",
    "\n",
    "            Ensure the reward function reflects the changes in the environment appropriately:\n",
    "            \"\"\"\n",
    "            newRewardFunctionResponse = queryAnthropicApi(apiKey, modelName, [{\"role\": \"user\", \"content\": reward_prompt}])\n",
    "\n",
    "            # Example usage within the training loop\n",
    "            logClaudeCall(reward_prompt, newRewardFunctionResponse, reward_prompt, \"Explanation for reward function\", logFile='dynamicRewardFunctionAPICalls.jsonl')\n",
    "\n",
    "            # Extract the function code from the response\n",
    "            try:\n",
    "                # Extract the actual Python function from the response using the updated function\n",
    "                functionCode = extractFunctionCode(newRewardFunctionResponse)\n",
    "\n",
    "                # Update the reward function in the environment\n",
    "                customEnv.updateRewardFunction(functionCode)\n",
    "                print(\"Updated reward function from LLM after environment change.\")\n",
    "\n",
    "            except ValueError as e:\n",
    "                # If extraction or execution fails, print the error message\n",
    "                print(f\"Error updating reward function: {e}\")\n",
    "                print(\"Response from API:\", newRewardFunctionResponse)\n",
    "\n",
    "                # Optionally, ensure the fallback to the default reward function\n",
    "                customEnv.setRewardFunction(customEnv.angleBasedReward)\n",
    "                print(\"Falling back to default reward function.\")\n",
    "\n",
    "    return rewards\n",
    "\n",
    "# Train the agent - Can see the changing of reward functions (Bifurcations)\n",
    "# rewards = trainAgentWithDynamicUpdates(agent, customEnv, episodes, update_interval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment parameters updated: masscart=1.4652910064514981, length=1.298929722309374, gravity=9.8\n",
      "Environment updated at episode 100. New masscart: 1.4652910064514981, New length: 1.298929722309374\n",
      "updateReward Function: def dynamicRewardFunction(observation, action):\n",
      "    try:\n",
      "        x, _, angle, _ = observation\n",
      "        \n",
      "        # Adjust reward based on new cart mass and pole length\n",
      "        cart_mass = 1.4652910064514981\n",
      "        pole_length = 1.298929722309374\n",
      "        \n",
      "        # Calculate reward components\n",
      "        position_reward = 1.0 - abs(x) / 2.4  # Normalize position to [0, 1]\n",
      "        angle_reward = 1.0 - abs(angle) / 0.209  # Normalize angle to [0, 1]\n",
      "        \n",
      "        # Adjust rewards based on new parameters\n",
      "        mass_factor = 1.0 / cart_mass\n",
      "        length_factor = pole_length / 0.5  # Assuming 0.5 was the original length\n",
      "        \n",
      "        # Combine rewards\n",
      "        reward = (position_reward * mass_factor + angle_reward * length_factor) / 2\n",
      "        \n",
      "        # Ensure reward is non-negative\n",
      "        return max(reward, 0)\n",
      "    except Exception:\n",
      "        # Return default value if any error occurs\n",
      "        return 0\n",
      "Reward function updated dynamically from LLM.\n",
      "Updated reward function from LLM after environment change.\n",
      "Environment parameters updated: masscart=0.7831708640565412, length=1.2583492552316367, gravity=9.8\n",
      "Environment updated at episode 200. New masscart: 0.7831708640565412, New length: 1.2583492552316367\n",
      "updateReward Function: def dynamicRewardFunction(observation, action):\n",
      "    try:\n",
      "        x, _, angle, _ = observation\n",
      "        cart_mass = 0.7831708640565412\n",
      "        pole_length = 1.2583492552316367\n",
      "        \n",
      "        # Penalize based on distance from center\n",
      "        position_reward = 1.0 / (1.0 + abs(x))\n",
      "        \n",
      "        # Penalize based on angle from vertical\n",
      "        angle_reward = 1.0 / (1.0 + abs(angle))\n",
      "        \n",
      "        # Reward for maintaining balance\n",
      "        balance_reward = 10.0 if abs(angle) < 0.1 and abs(x) < 0.5 else 0.0\n",
      "        \n",
      "        # Adjust reward based on cart mass and pole length\n",
      "        mass_length_factor = 1.0 / (cart_mass * pole_length)\n",
      "        \n",
      "        total_reward = (position_reward + angle_reward + balance_reward) * mass_length_factor\n",
      "        \n",
      "        return max(total_reward, 0.0)\n",
      "    except Exception:\n",
      "        return 0.0\n",
      "Reward function updated dynamically from LLM.\n",
      "Updated reward function from LLM after environment change.\n",
      "Environment parameters updated: masscart=0.7005181162480789, length=0.6543733970290202, gravity=9.8\n",
      "Environment updated at episode 300. New masscart: 0.7005181162480789, New length: 0.6543733970290202\n",
      "updateReward Function: def dynamicRewardFunction(observation, action):\n",
      "    try:\n",
      "        x, _, angle, _ = observation\n",
      "        cart_mass = 0.7005181162480789\n",
      "        pole_length = 0.6543733970290202\n",
      "        \n",
      "        # Calculate reward based on cart position and pole angle\n",
      "        position_reward = 1.0 - abs(x) / 2.4  # Normalize position reward\n",
      "        angle_reward = 1.0 - abs(angle) / 0.209  # Normalize angle reward (12 degrees)\n",
      "        \n",
      "        # Adjust rewards based on new cart mass and pole length\n",
      "        mass_factor = 1 / cart_mass\n",
      "        length_factor = pole_length\n",
      "        \n",
      "        # Combine rewards with adjustments\n",
      "        reward = (position_reward + angle_reward) * mass_factor * length_factor\n",
      "        \n",
      "        return max(reward, 0)  # Ensure non-negative reward\n",
      "    except Exception:\n",
      "        return 0  # Return default value if any error occurs\n",
      "Reward function updated dynamically from LLM.\n",
      "Updated reward function from LLM after environment change.\n",
      "Environment parameters updated: masscart=0.695930906473653, length=0.7744562737520164, gravity=9.8\n",
      "Environment updated at episode 400. New masscart: 0.695930906473653, New length: 0.7744562737520164\n",
      "updateReward Function: def dynamicRewardFunction(observation, action):\n",
      "    try:\n",
      "        x, _, angle, _ = observation\n",
      "        cart_mass = 0.695930906473653\n",
      "        pole_length = 0.7744562737520164\n",
      "        \n",
      "        # Calculate reward based on cart position and pole angle\n",
      "        position_reward = 1.0 - abs(x) / 2.4  # Normalize position\n",
      "        angle_reward = 1.0 - abs(angle) / 0.209  # Normalize angle (12 degrees)\n",
      "        \n",
      "        # Adjust reward based on new cart mass and pole length\n",
      "        mass_factor = 1.0 / cart_mass\n",
      "        length_factor = pole_length\n",
      "        \n",
      "        # Combine rewards\n",
      "        reward = (position_reward + angle_reward) * mass_factor * length_factor\n",
      "        \n",
      "        # Ensure reward is non-negative\n",
      "        return max(reward, 0.0)\n",
      "    except Exception:\n",
      "        # Return default value if any error occurs\n",
      "        return 0.0\n",
      "Reward function updated dynamically from LLM.\n",
      "Updated reward function from LLM after environment change.\n",
      "Environment parameters updated: masscart=1.955469966864743, length=0.6331856854864333, gravity=9.8\n",
      "Environment updated at episode 500. New masscart: 1.955469966864743, New length: 0.6331856854864333\n",
      "updateReward Function: def dynamicRewardFunction(observation, action):\n",
      "    try:\n",
      "        x, _, angle, _ = observation\n",
      "        \n",
      "        # Calculate reward based on cart position and pole angle\n",
      "        position_reward = 1.0 - abs(x) / 2.4  # Normalize position reward\n",
      "        angle_reward = 1.0 - abs(angle) / 0.209  # Normalize angle reward (12 degrees ≈ 0.209 radians)\n",
      "        \n",
      "        # Adjust rewards based on new cart mass and pole length\n",
      "        mass_factor = 1.955469966864743 / 1.0  # Ratio of new mass to standard mass\n",
      "        length_factor = 0.6331856854864333 / 0.5  # Ratio of new length to standard length\n",
      "        \n",
      "        # Combine rewards with adjustments\n",
      "        reward = (position_reward * mass_factor + angle_reward * length_factor) / 2\n",
      "        \n",
      "        # Ensure reward is non-negative\n",
      "        return max(reward, 0)\n",
      "    except Exception:\n",
      "        # Return default value if any error occurs\n",
      "        return 0\n",
      "Reward function updated dynamically from LLM.\n",
      "Updated reward function from LLM after environment change.\n",
      "Environment parameters updated: masscart=0.6873851604047905, length=1.7435715310844422, gravity=9.8\n",
      "Environment updated at episode 600. New masscart: 0.6873851604047905, New length: 1.7435715310844422\n",
      "updateReward Function: def dynamicRewardFunction(observation, action):\n",
      "    try:\n",
      "        x, _, angle, _ = observation\n",
      "        cart_mass = 0.6873851604047905\n",
      "        pole_length = 1.7435715310844422\n",
      "\n",
      "        # Calculate reward based on cart position and pole angle\n",
      "        position_reward = 1.0 - abs(x) / 2.4  # Normalize position\n",
      "        angle_reward = 1.0 - abs(angle) / 0.209  # Normalize angle (12 degrees)\n",
      "\n",
      "        # Adjust rewards based on new cart mass and pole length\n",
      "        mass_factor = 1 / cart_mass\n",
      "        length_factor = pole_length / 0.5  # Assuming default length is 0.5\n",
      "\n",
      "        # Combine rewards with adjustments\n",
      "        reward = (position_reward + angle_reward) * mass_factor * length_factor\n",
      "\n",
      "        # Ensure reward is non-negative\n",
      "        return max(reward, 0)\n",
      "    except Exception:\n",
      "        return 0\n",
      "Reward function updated dynamically from LLM.\n",
      "Updated reward function from LLM after environment change.\n",
      "Environment parameters updated: masscart=0.798243801099485, length=1.016815125328416, gravity=9.8\n",
      "Environment updated at episode 700. New masscart: 0.798243801099485, New length: 1.016815125328416\n",
      "updateReward Function: def dynamicRewardFunction(observation, action):\n",
      "    try:\n",
      "        x, _, angle, _ = observation\n",
      "        cart_mass = 0.798243801099485\n",
      "        pole_length = 1.016815125328416\n",
      "        \n",
      "        # Calculate reward based on cart position and pole angle\n",
      "        position_reward = 1.0 - abs(x) / 2.4  # Normalize position within track\n",
      "        angle_reward = 1.0 - abs(angle) / 0.209  # Normalize angle (12 degrees in radians)\n",
      "        \n",
      "        # Adjust rewards based on new cart mass and pole length\n",
      "        mass_factor = 1.0 / cart_mass\n",
      "        length_factor = pole_length\n",
      "        \n",
      "        # Combine rewards with adjustments\n",
      "        reward = (position_reward + angle_reward) * mass_factor * length_factor\n",
      "        \n",
      "        return max(reward, 0)  # Ensure non-negative reward\n",
      "    except Exception:\n",
      "        return 0  # Default reward in case of any error\n",
      "Reward function updated dynamically from LLM.\n",
      "Updated reward function from LLM after environment change.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment parameters updated: masscart=1.9017122065631602, length=1.6051920234066666, gravity=9.8\n",
      "Environment updated at episode 800. New masscart: 1.9017122065631602, New length: 1.6051920234066666\n",
      "updateReward Function: def dynamicRewardFunction(observation, action):\n",
      "    try:\n",
      "        x, _, angle, _ = observation\n",
      "        cart_mass = 1.9017122065631602\n",
      "        pole_length = 1.6051920234066666\n",
      "\n",
      "        # Calculate reward based on cart position and pole angle\n",
      "        position_reward = 1.0 - abs(x) / 2.4  # Normalize position reward\n",
      "        angle_reward = 1.0 - abs(angle) / 0.209  # Normalize angle reward (12 degrees in radians)\n",
      "\n",
      "        # Adjust rewards based on new cart mass and pole length\n",
      "        mass_factor = 1.0 / cart_mass\n",
      "        length_factor = pole_length / 0.5  # Compare to standard pole length\n",
      "\n",
      "        # Combine rewards with adjustments\n",
      "        reward = (position_reward + angle_reward) * mass_factor * length_factor\n",
      "\n",
      "        return max(reward, 0)  # Ensure non-negative reward\n",
      "    except Exception:\n",
      "        return 0  # Return default value if any error occurs\n",
      "Reward function updated dynamically from LLM.\n",
      "Updated reward function from LLM after environment change.\n",
      "Environment parameters updated: masscart=1.2590818179520145, length=0.9957589402314226, gravity=9.8\n",
      "Environment updated at episode 900. New masscart: 1.2590818179520145, New length: 0.9957589402314226\n",
      "updateReward Function: def dynamicRewardFunction(observation, action):\n",
      "    try:\n",
      "        x, _, angle, _ = observation\n",
      "        cart_mass = 1.2590818179520145\n",
      "        pole_length = 0.9957589402314226\n",
      "        \n",
      "        # Calculate base reward\n",
      "        position_reward = 1.0 - abs(x) / 2.4  # Normalize x position\n",
      "        angle_reward = 1.0 - abs(angle) / 0.209  # Normalize angle (12 degrees in radians)\n",
      "        \n",
      "        # Adjust reward based on new cart mass and pole length\n",
      "        mass_factor = 1.0 / cart_mass\n",
      "        length_factor = pole_length\n",
      "        \n",
      "        # Combine rewards\n",
      "        reward = (position_reward + angle_reward) * mass_factor * length_factor\n",
      "        \n",
      "        # Ensure reward is non-negative\n",
      "        reward = max(reward, 0)\n",
      "        \n",
      "        return float(reward)\n",
      "    except Exception:\n",
      "        return 0.0\n",
      "Reward function updated dynamically from LLM.\n",
      "Updated reward function from LLM after environment change.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m updateInterval \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m  \u001b[38;5;66;03m# Change environment every 100 episodes\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Train the agent and collect rewards with dynamic environment updates\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m rewards \u001b[38;5;241m=\u001b[39m \u001b[43mtrainAgentWithDynamicUpdates\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustomEnv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdateInterval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Create a plot figure for multiple subplots\u001b[39;00m\n\u001b[1;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n",
      "Cell \u001b[0;32mIn[3], line 19\u001b[0m, in \u001b[0;36mtrainAgentWithDynamicUpdates\u001b[0;34m(agent, env, episodes, updateInterval)\u001b[0m\n\u001b[1;32m     15\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# Agent chooses action and steps in environment\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchooseAction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     next_observation, reward, done, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     21\u001b[0m     agent\u001b[38;5;241m.\u001b[39mremember(observation, action, reward, next_observation, done)\n",
      "File \u001b[0;32m~/BachelorsThesis/Using-LLMs-to-Generate-Reward-Functions-from-Natural-Language-in-RL-Environments/cartPoleShared.py:196\u001b[0m, in \u001b[0;36mDQLearningAgent.chooseAction\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    195\u001b[0m     qValues \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(stateTensor)\n\u001b[0;32m--> 196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqValues\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Number of episodes to train\n",
    "episodes = 1000\n",
    "updateInterval = 100  # Change environment every 100 episodes\n",
    "\n",
    "# Train the agent and collect rewards with dynamic environment updates\n",
    "rewards = trainAgentWithDynamicUpdates(agent, customEnv, episodes, updateInterval)\n",
    "\n",
    "# Create a plot figure for multiple subplots\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 1. Total Reward Comparison\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(np.arange(episodes), rewards, label=\"Dynamic Environment with LLM Reward Function\")\n",
    "for i in range(0, episodes, updateInterval):\n",
    "    plt.axvline(x=i, color='r', linestyle='--', linewidth=0.8)  # Add vertical lines for environment changes\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"Total Reward vs Episodes with Environment Changes\")\n",
    "plt.legend()\n",
    "\n",
    "# 2. Running Average of Rewards\n",
    "def movingAverageAndStd(data, windowSize=100):\n",
    "    average = np.convolve(data, np.ones(windowSize) / windowSize, mode='valid')\n",
    "    std = [np.std(data[i:i + windowSize]) for i in range(len(data) - windowSize + 1)]\n",
    "    return average, std\n",
    "\n",
    "avg_rewards, std_rewards = movingAverageAndStd(rewards, windowSize=100)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(np.arange(len(avg_rewards)), avg_rewards, label=\"Running Average of Reward\")\n",
    "plt.fill_between(np.arange(len(avg_rewards)), avg_rewards - std_rewards, avg_rewards + std_rewards, alpha=0.2, label=\"Reward Variance\")\n",
    "for i in range(0, episodes, updateInterval):\n",
    "    if i < len(avg_rewards):  # Ensure the vertical line is within the range of avg_rewards\n",
    "        plt.axvline(x=i, color='r', linestyle='--', linewidth=0.8)  # Add vertical lines for environment changes\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Average Reward\")\n",
    "plt.title(\"Running Average of Rewards with Environment Changes\")\n",
    "plt.legend()\n",
    "\n",
    "# 3. Episode Duration (length of each episode)\n",
    "def episodeDuration(agent, env, episodes=500):\n",
    "    durations = []\n",
    "    for episode in range(episodes):\n",
    "        observation = env.reset()[0]\n",
    "        done = False\n",
    "        duration = 0\n",
    "\n",
    "        while not done:\n",
    "            action = agent.chooseAction(observation)  # Updated to use DQL agent's policy\n",
    "            observation, _, done, _, _ = env.step(action)\n",
    "            duration += 1\n",
    "        durations.append(duration)\n",
    "\n",
    "    return durations\n",
    "\n",
    "durations = episodeDuration(agent, customEnv, episodes)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(np.arange(episodes), durations, label=\"Episode Duration\")\n",
    "for i in range(0, episodes, updateInterval):\n",
    "    plt.axvline(x=i, color='r', linestyle='--', linewidth=0.8)  # Add vertical lines for environment changes\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Episode Duration (Steps)\")\n",
    "plt.title(\"Episode Duration over Episodes\")\n",
    "plt.legend()\n",
    "\n",
    "# 4. Action Distribution\n",
    "def getActionDistribution(agent, env, episodes):\n",
    "    actions = []\n",
    "    for episode in range(episodes):\n",
    "        observation = env.reset()[0]\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.chooseAction(observation)  # Updated to use DQL agent's policy\n",
    "            actions.append(action)\n",
    "            observation, _, done, _, _ = env.step(action)\n",
    "\n",
    "    return actions\n",
    "\n",
    "actions = getActionDistribution(agent, customEnv, episodes)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.hist(actions, bins=2, alpha=0.7, label=\"Action Distribution\")\n",
    "plt.xlabel(\"Actions (0=Left, 1=Right)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Action Distribution across Episodes\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Reward Distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(rewards, bins=20, alpha=0.7)\n",
    "plt.title(\"Reward Distribution over Episodes\")\n",
    "plt.xlabel(\"Reward\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6. Reward Heatmap for Environmental Changes\n",
    "# Track environmental parameters like mass and length at each update interval\n",
    "mass_values = []\n",
    "length_values = []\n",
    "\n",
    "for episode in range(0, episodes, updateInterval):\n",
    "    mass_values.append(np.random.uniform(0.5, 2.0))\n",
    "    length_values.append(np.random.uniform(0.5, 2.0))\n",
    "\n",
    "# Create a DataFrame for heatmap\n",
    "data = pd.DataFrame({'Episode': range(0, episodes, updateInterval), \n",
    "                     'MassCart': mass_values, \n",
    "                     'PoleLength': length_values, \n",
    "                     'Reward': rewards[::updateInterval]})\n",
    "\n",
    "# Pivot the DataFrame to create a heatmap\n",
    "pivot_data = data.pivot(index='MassCart', columns='PoleLength', values='Reward')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(pivot_data, cmap=\"YlGnBu\", annot=True)\n",
    "plt.xlabel('Pole Length')\n",
    "plt.ylabel('Mass of Cart')\n",
    "plt.show()\n",
    "\n",
    "# 7. Success Rate Calculation\n",
    "success_threshold = 200  # Define the threshold for success\n",
    "success_rate = sum([1 if r >= success_threshold else 0 for r in rewards]) / episodes\n",
    "print(f\"Success Rate: {success_rate * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Visualize the top 20 runs from the simulation\n",
    "def visualizeDynamicRuns(env, rewards, episodes=1000, topRuns=20):\n",
    "    topRunIndices = np.argsort(rewards)[-topRuns:]\n",
    "\n",
    "    # Initialize a new environment to visualize the runs\n",
    "    visualizationEnv = gym.make('CartPole-v1', render_mode=\"human\")\n",
    "    agent = DQLearningAgent(visualizationEnv, stateSize=4, actionSize=2, device=device)\n",
    "\n",
    "    for idx in topRunIndices:\n",
    "        print(f\"Visualizing Run {idx + 1}: Total Reward = {rewards[idx]}\")\n",
    "\n",
    "        observation, _ = visualizationEnv.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Convert observation to tensor for the DQ-Learning agent\n",
    "            state = torch.tensor(observation, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "            # Choose an action using the DQ-Learning agent's model\n",
    "            with torch.no_grad():\n",
    "                qValues = agent.model(state)\n",
    "            action = torch.argmax(qValues).item()\n",
    "\n",
    "            nextObservation, reward, done, _, _ = visualizationEnv.step(action)\n",
    "            observation = nextObservation\n",
    "\n",
    "            # Render environment to visualize the run\n",
    "            visualizationEnv.render()\n",
    "\n",
    "            time.sleep(0.01)\n",
    "\n",
    "        print(\"End of Run\\n\")\n",
    "\n",
    "    # Close the visualization environment\n",
    "    visualizationEnv.close()\n",
    "\n",
    "visualizeDynamicRuns(customEnv, rewards, episodes=1000, topRuns=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Has not been updated to DQ-Learning yet\n",
    "\n",
    "\n",
    "def manualEnvironmentChange(env, episode):\n",
    "    if episode % 50 == 0:  # Ask every 50 episodes\n",
    "        userInput = input(f\"Do you want to change the environment in episode {episode}? (y/n): \")\n",
    "        if userInput.lower() == 'y':\n",
    "            masscart = float(input(\"Enter new mass for the cart: \"))\n",
    "            length = float(input(\"Enter new length for the pole: \"))\n",
    "            gravity = float(input(\"Enter new gravity: \"))\n",
    "            env.setEnvironmentParameters(masscart=masscart, length=length, gravity=gravity)\n",
    "\n",
    "# Train the agent with manual environment changes\n",
    "def trainAgentWithManualChanges(agent, env, episodes=1000):\n",
    "    rewards = []\n",
    "    for episode in range(episodes):\n",
    "        # Reset environment for each episode\n",
    "        observation = env.reset()[0]\n",
    "        state = agent.discretize(observation)\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Agent chooses action and steps in environment\n",
    "            action = agent.choose_action(state)\n",
    "            next_observation, reward, done, _, _ = env.step(action)\n",
    "            next_state = agent.discretize(next_observation)\n",
    "            agent.update(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        \n",
    "        rewards.append(total_reward)\n",
    "        \n",
    "        # Manual environment change prompt every 50 episodes\n",
    "        manualEnvironmentChange(env, episode)\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "# Call the manual version for exploratory testing\n",
    "# rewards_manual = trainAgentWithManualChanges(agent, customEnv, episodes=100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
