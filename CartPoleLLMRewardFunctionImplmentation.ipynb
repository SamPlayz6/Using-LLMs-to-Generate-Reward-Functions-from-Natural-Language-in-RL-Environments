{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **State of the art Reward Functions for Cart Pole**\n",
    "https://github.com/openai/gym/wiki/Leaderboard\n",
    "\n",
    "\n",
    "Continuous Angle-Based Reward\n",
    "Provides a continuous reward based on the angle of the pole.\n",
    "```\n",
    "def decide(self, observation):\n",
    "        position, velocity, angle, angle_velocity = observation\n",
    "        action = int(3. * angle + angle_velocity > 0.)\n",
    "        return action\n",
    "```\n",
    "\n",
    "Position-Angle Balanced Reward\n",
    "Balances rewards between pole angle and cart position.\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "Velocity-Aware Reward\n",
    "Incorporates cart and pole velocities.\n",
    "```\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Claude's Reward Functions**\n",
    "\n",
    "```python\n",
    "def LLMOutput(observation, action):\n",
    "    x, _, angle, _ = observation\n",
    "    reward = 1.0 - abs(angle) - 0.5 * abs(x)\n",
    "    return max(0, reward)\n",
    "```\n",
    "\n",
    "1. Input:\n",
    "   - The function takes two parameters: `observation` and `action` (although `action` is not used in this function).\n",
    "   - `observation` is assumed to be a tuple or list containing four elements, but only the first (x) and third (angle) are used.\n",
    "\n",
    "2. Unpacking the observation:\n",
    "   - `x, _, angle, _` = observation\n",
    "   - This line extracts the position (x) and angle from the observation. The underscores (_) indicate that we're ignoring the second and fourth elements of the observation.\n",
    "\n",
    "3. Calculating the reward:\n",
    "   - `reward = 1.0 - abs(angle) - 0.5 * abs(x)`\n",
    "   - The reward starts at 1.0 and is reduced based on two factors:\n",
    "     a. The absolute value of the angle: This penalizes the pole for leaning away from vertical.\n",
    "     b. Half the absolute value of the position: This penalizes the cart for being far from the center.\n",
    "\n",
    "4. The logic behind this reward calculation:\n",
    "   - The goal is to keep the pole upright (angle close to 0) and the cart near the center (x close to 0).\n",
    "   - A larger angle or distance from center results in a lower reward.\n",
    "   - The angle has a stronger impact on the reward than the position (full weight vs. half weight).\n",
    "\n",
    "5. Returning the reward:\n",
    "   - `return max(0, reward)`\n",
    "   - This ensures the reward is never negative. If the calculated reward is negative, 0 is returned instead.\n",
    "\n",
    "In summary, this function calculates a reward based on how well the pole is balanced (angle) and how centered the cart is (x). The reward is highest (close to 1) when the pole is perfectly upright and the cart is in the center. It decreases as the pole leans more or the cart moves farther from the center, with the angle having a stronger influence on the reward than the position.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "```python\n",
    "def LLMOutput(observation, action):\n",
    "    x, _, angle, _ = observation\n",
    "    reward = 1.0 - abs(angle) - 0.5 * abs(x)\n",
    "    return int(max(0, reward * 100))\n",
    "```\n",
    "\n",
    "1. Function Input:\n",
    "   - The function takes two parameters: `observation` and `action`.\n",
    "   - `observation` is likely a tuple or list containing information about the current state of the cart-pole system.\n",
    "\n",
    "2. Unpacking the Observation:\n",
    "   - `x, _, angle, _ = observation`\n",
    "   - This line unpacks the `observation` into separate variables.\n",
    "   - `x` represents the position of the cart.\n",
    "   - `angle` represents the angle of the pole.\n",
    "   - The `_` is used to ignore other values in the observation that are not needed for this calculation.\n",
    "\n",
    "3. Reward Calculation:\n",
    "   - `reward = 1.0 - abs(angle) - 0.5 * abs(x)`\n",
    "   - This line calculates a reward based on the current state.\n",
    "   - The reward starts at 1.0 and is reduced based on two factors:\n",
    "     a. The absolute value of the angle: A larger angle (more tilt) reduces the reward more.\n",
    "     b. Half of the absolute value of the position: Being further from the center (x=0) also reduces the reward, but less severely than the angle.\n",
    "\n",
    "4. Final Output:\n",
    "   - `return int(max(0, reward * 100))`\n",
    "   - The reward is multiplied by 100 to scale it up.\n",
    "   - The `max` function ensures the result is never negative.\n",
    "   - The `int` function converts the result to an integer.\n",
    "\n",
    "Logic behind the reward:\n",
    "- The goal is to keep the pole as upright as possible (angle close to 0) and the cart near the center (x close to 0).\n",
    "- The reward is highest (close to 100) when both the angle and position are close to 0.\n",
    "- As the pole tilts more or the cart moves further from the center, the reward decreases.\n",
    "- The angle has a more significant impact on the reward than the position, reflecting that keeping the pole upright is the primary goal.\n",
    "\n",
    "This reward function encourages actions that keep the pole balanced and the cart centered, which aligns with the objective of the cart-pole balancing task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "class CustomCartPoleEnv(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "        self.reward_function = self.angle_based_reward\n",
    "\n",
    "    def step(self, action):\n",
    "        observation, _, terminated, truncated, info = self.env.step(action)\n",
    "        reward = self.reward_function(observation, action)\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def angle_based_reward(self, observation, action):\n",
    "        _, _, angle, _ = observation\n",
    "        return np.cos(angle)  # Higher reward when angle is closer to 0 (vertical)\n",
    "    \n",
    "    def LLMRewardFunction(self, function_string):\n",
    "        local_namespace = {}\n",
    "        # Execute the function string in the local namespace\n",
    "        exec(function_string, globals(), local_namespace)\n",
    "        \n",
    "        # Find the function in the local namespace\n",
    "        new_function = None\n",
    "        for item in local_namespace.values():\n",
    "            if callable(item):\n",
    "                new_function = item\n",
    "                break\n",
    "        \n",
    "        if new_function is None:\n",
    "            raise ValueError(\"No function found in the provided string\")\n",
    "        \n",
    "        # Set the new function as the reward function\n",
    "        self.set_reward_function(new_function)\n",
    "\n",
    "    def set_reward_function(self, reward_function):\n",
    "        self.reward_function = reward_function\n",
    "\n",
    "def simple_angle_policy(observation):\n",
    "    _, _, angle, _ = observation\n",
    "    return 1 if angle > 0 else 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def dynamicRewardFunction(observation, action):\n",
      "    x, _, angle, _ = observation\n",
      "    reward = 1.0 - abs(angle) * 0.5 - abs(x) * 0.1\n",
      "    if abs(angle) > 0.2 or abs(x) > 2.4:\n",
      "        reward -= 10\n",
      "    return int(reward * 10)\n",
      "\n",
      " ---------------------------------------------------- \n",
      "\n",
      "Certainly! Let's break down the logic of this dynamic reward function:\n",
      "\n",
      "1. Input:\n",
      "   - `observation`: A tuple containing the current state of the environment. It includes:\n",
      "     - `x`: The position of the cart\n",
      "     - `angle`: The angle of the pole (in radians)\n",
      "   - `action`: The action taken (not used in this function)\n",
      "\n",
      "2. Reward Calculation:\n",
      "\n",
      "   ```python\n",
      "   reward = 1.0 - abs(angle) * 0.5 - abs(x) * 0.1\n",
      "   ```\n",
      "\n",
      "   - The reward starts at 1.0 (maximum reward)\n",
      "   - It then subtracts penalties based on:\n",
      "     - The absolute angle of the pole: `abs(angle) * 0.5`\n",
      "       - This encourages keeping the pole upright\n",
      "       - The larger the angle, the larger the penalty\n",
      "     - The absolute position of the cart: `abs(x) * 0.1`\n",
      "       - This encourages keeping the cart near the center\n",
      "       - The further from the center, the larger the penalty\n",
      "   - The angle has a higher weight (0.5) compared to the position (0.1), indicating that keeping the pole upright is more important than centering the cart\n",
      "\n",
      "3. Additional Penalty:\n",
      "\n",
      "   ```python\n",
      "   if abs(angle) > 0.2 or abs(x) > 2.4:\n",
      "       reward -= 10\n",
      "   ```\n",
      "\n",
      "   - If the angle exceeds 0.2 radians (about 11.5 degrees) or the cart position exceeds 2.4 units from the center, a large penalty of 10 is applied\n",
      "   - This discourages actions that lead to extreme angles or positions, which could result in failure\n",
      "\n",
      "4. Final Reward:\n",
      "\n",
      "   ```python\n",
      "   return int(reward * 10)\n",
      "   ```\n",
      "\n",
      "   - The final reward is multiplied by 10 and converted to an integer\n",
      "   - This scales the reward and removes decimal places, potentially making it easier for the learning algorithm to work with\n",
      "\n",
      "The logic behind this function is to provide a continuous reward signal that encourages:\n",
      "1. Keeping the pole as upright as possible (minimizing the angle)\n",
      "2. Keeping the cart near the center of the track (minimizing the position)\n",
      "3. Avoiding extreme situations that could lead to failure\n",
      "\n",
      "By providing a continuous reward instead of just a binary success/failure signal, the function gives the learning algorithm more detailed feedback on its performance, allowing it to learn more efficiently.\n",
      "\n",
      " ---------------------------------------------------- \n",
      " Reward Function Name:  dynamicRewardFunction\n"
     ]
    }
   ],
   "source": [
    "import anthropic\n",
    "\n",
    "# Create and wrap the environment\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "env = CustomCartPoleEnv(env)\n",
    "\n",
    "#API Query\n",
    "client = anthropic.Anthropic(\n",
    "    # defaults to os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "    api_key=\"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\",\n",
    ")\n",
    "generatedRewardFunction = client.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=1024,\n",
    "    messages=[\n",
    "        {\"role\": \"user\"\n",
    "\n",
    "         , \"content\": \"\"\"You are a python code outputter. I want your output to only be python code and just be one function, which is an integer. No other text with it the output. \n",
    "         This function will be a reward function, named dynamicRewardFunction(), for a RL environment that follows the description below. \n",
    "         The inputs are observation and action in that order, the input observation can be broken down as follows: x, _, angle,_ = observation :\n",
    "         \n",
    "         This environment is a pole balanced on a cart that can move from left to right, \n",
    "         the idea is to keep the pole as upright as possible by moving the cart either left or right, \n",
    "         the information you have available to you is the position(x) and the angle(angle).\"\"\"}\n",
    "    ]\n",
    ")\n",
    "print(generatedRewardFunction.content[0].text)\n",
    "print(\"\\n ---------------------------------------------------- \\n\")\n",
    "\n",
    "rewardFunctionExplanation = client.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=1024,\n",
    "    messages=[\n",
    "        {\"role\": \"user\"\n",
    "\n",
    "         , \"content\": f\"\"\"I need you to explain the logic behind this following function:\n",
    "        {generatedRewardFunction.content[0].text} \n",
    "\n",
    "        This environment is a pole balanced on a cart that can move from left to right, \n",
    "         the idea is to keep the pole as upright as possible by moving the cart either left or right, \n",
    "         the information you have available to you is the position(x) and the angle(angle).\n",
    "         \"\"\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(rewardFunctionExplanation.content[0].text)\n",
    "\n",
    "# generatedRewardFunction = \"\"\"def LLMOutput(observation, action):\n",
    "#    x, _, angle, _ = observation\n",
    "#    reward = 1.0 - abs(angle) - 0.5 * abs(x)\n",
    "#    if abs(angle) > 0.2 or abs(x) > 2.4:\n",
    "#        reward -= 10\n",
    "#    return reward\n",
    "# \"\"\"#API call to Claude\n",
    "\n",
    "# Set initial policy and reward function\n",
    "env.LLMRewardFunction(generatedRewardFunction.content[0].text)\n",
    "print(\"\\n ---------------------------------------------------- \\n Reward Function Name: \", env.reward_function.__name__)\n",
    "current_policy = simple_angle_policy  # Set the policy \n",
    "\n",
    "\n",
    "# Main loop\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(1000):\n",
    "    action = current_policy(observation)\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation with LLM Generated Policy\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gymenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
