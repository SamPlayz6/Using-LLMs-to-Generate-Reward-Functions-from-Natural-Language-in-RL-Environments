{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (2.1.2)\n",
      "Requirement already satisfied: matplotlib in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (3.9.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from gym) (3.1.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from matplotlib) (2.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/sd37/.conda/envs/thesis/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gym numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline SoTA reward function\n",
    "def SoTARewardFunction(observation, action):\n",
    "    x, _, theta, _ = observation\n",
    "    \n",
    "    if abs(theta) > np.deg2rad(15) or abs(x) > 2.4:\n",
    "        return 0\n",
    "    \n",
    "    return 1\n",
    "\n",
    "# Example LLM-generated reward functions\n",
    "def LLMRewardFunction1API(observation, action):\n",
    "    x, _, angle, _ = observation\n",
    "    reward = 1.0 - abs(angle) - 0.5 * abs(x)\n",
    "    return max(0, reward)\n",
    "\n",
    "def LLMRewardFunction2API(observation, action):\n",
    "    x, _, angle, _ = observation\n",
    "    reward = 1.0\n",
    "    \n",
    "    if abs(x) > 2.4 or abs(angle) > 0.2:\n",
    "        reward = -10.0\n",
    "    else:\n",
    "        reward += 10.0 / (1.0 + abs(angle))\n",
    "        reward += 5.0 / (1.0 + abs(x))\n",
    "    \n",
    "    return int(reward)\n",
    "\n",
    "\n",
    "def LLMRewardFunction1(observation, action):\n",
    "    # Unpack state variables\n",
    "    x, x_dot, angle, angle_dot = observation\n",
    "    \n",
    "    # Define constants\n",
    "    x_threshold = 2.4  # Cart position limit\n",
    "    angle_threshold = 12 * np.pi / 180  # Angle limit (12 degrees)\n",
    "    \n",
    "    # Check if episode is done\n",
    "    done = bool(\n",
    "        abs(x) > x_threshold or\n",
    "        abs(angle) > angle_threshold\n",
    "    )\n",
    "    \n",
    "    if not done:\n",
    "        # Reward for pole angle (closer to vertical is better)\n",
    "        angle_reward = 1 - abs(angle) / angle_threshold\n",
    "        \n",
    "        # Reward for cart position (closer to center is better)\n",
    "        position_reward = 1 - abs(x) / x_threshold\n",
    "        \n",
    "        # Reward for keeping the pole stable\n",
    "        stability_reward = 1 / (1 + abs(angle_dot))\n",
    "        \n",
    "        # Combine rewards\n",
    "        reward = angle_reward + position_reward + stability_reward\n",
    "    else:\n",
    "        # Penalty for ending episode\n",
    "        reward = -10\n",
    "    \n",
    "    return reward\n",
    "\n",
    "\n",
    "def LLMRewardFunction2(observation, action):\n",
    "    # Unpack state variables\n",
    "    x, x_dot, angle, angle_dot = observation\n",
    "    \n",
    "    # System parameters (now hardcoded as they can't be passed in)\n",
    "    m = 0.1  # pendulum mass\n",
    "    M = 1.0  # cart mass\n",
    "    l = 0.5  # pendulum length\n",
    "    g = 9.8  # gravity\n",
    "    \n",
    "    # Calculate potential energy\n",
    "    PE = m * g * l * (1 - np.cos(angle))\n",
    "    \n",
    "    # Calculate kinetic energy\n",
    "    KE_cart = 0.5 * M * x_dot**2\n",
    "    KE_pendulum = 0.5 * m * ((x_dot**2) + (l * angle_dot**2) + \n",
    "                             (2 * x_dot * l * angle_dot * np.cos(angle)))\n",
    "    KE_total = KE_cart + KE_pendulum\n",
    "    \n",
    "    # Calculate total energy\n",
    "    E_total = PE + KE_total\n",
    "    \n",
    "    # Define reward components\n",
    "    energy_reward = -E_total  # Minimize total energy\n",
    "    stability_reward = -abs(angle)  # Minimize angle deviation\n",
    "    \n",
    "    # Combine rewards (adjusted weights since we can't measure control effort now)\n",
    "    reward = 0.7 * energy_reward + 0.3 * stability_reward\n",
    "    \n",
    "    # Check if episode is done\n",
    "    x_threshold = 2.4\n",
    "    angle_threshold = np.pi / 2\n",
    "    done = bool(\n",
    "        abs(x) > x_threshold or\n",
    "        abs(angle) > angle_threshold\n",
    "    )\n",
    "    \n",
    "    if done:\n",
    "        reward -= 100  # Large penalty for ending episode\n",
    "    \n",
    "    return reward\n",
    "\n",
    "\n",
    "#---\n",
    "\n",
    "# Train the agent using Q-learning\n",
    "def train(env, agent, reward_model, episodes=500):\n",
    "    rewards = []\n",
    "    for episode in range(episodes):\n",
    "        observation = env.reset()[0]\n",
    "        state = agent.discretize(observation)\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.choose_action(state)\n",
    "            next_observation, reward, done, _, _ = env.step(action)\n",
    "            adjusted_reward = reward_model(next_observation, reward)\n",
    "            next_state = agent.discretize(next_observation)\n",
    "            agent.update(state, action, adjusted_reward, next_state)\n",
    "            state = next_state\n",
    "            total_reward += adjusted_reward\n",
    "        rewards.append(total_reward)\n",
    "    return rewards\n",
    "\n",
    "\n",
    "\n",
    "# episodes = 10000\n",
    "# baselineRewards = train(env, agent, SoTARewardFunction, episodes)\n",
    "# LLM1Rewards = train(env, agent, LLMRewardFunction1, episodes)\n",
    "# LLM2Rewards = train(env, agent, LLMRewardFunction2, episodes)\n",
    "\n",
    "# # Plot results\n",
    "# plt.plot(np.arange(episodes), LLM2Rewards, label=\"2nd LLM-Generated Reward Model\")\n",
    "# plt.plot(np.arange(episodes), baselineRewards, label=\"Custom Baseline Reward Model\")\n",
    "# plt.plot(np.arange(episodes), LLM1Rewards, label=\"1st LLM-Generated Reward Model\")\n",
    "\n",
    "# plt.xlabel(\"Episodes\")\n",
    "# plt.ylabel(\"Total Reward\")\n",
    "\n",
    "# plt.title(\"Reward Comparison over Time\")\n",
    "\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes ~ 60 second to run\n",
    "from cartPoleShared import QLearningAgent\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "agent = QLearningAgent(env)\n",
    "\n",
    "\n",
    "#This is temporary until I fix the error with the update function not being defined inthe calss\n",
    "def update(self, state, action, reward, next_state):\n",
    "    best_next_action = np.argmax(self.q_table[next_state])\n",
    "    td_target = reward + self.discount_factor * self.q_table[next_state][best_next_action]\n",
    "    td_error = td_target - self.q_table[state][action]\n",
    "    self.q_table[state][action] += self.learning_rate * td_error\n",
    "\n",
    "QLearningAgent.update = update\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def movingAverageAndStd(data, windowSize=100):\n",
    "    average = np.convolve(data, np.ones(windowSize) / windowSize, mode='valid')\n",
    "    std = [np.std(data[i:i+windowSize]) for i in range(len(data) - windowSize + 1)]\n",
    "    return average, std\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# List of reward functions to abstract repeated calls\n",
    "rewardFunctions = [\n",
    "    (\"Baseline Reward Model\", SoTARewardFunction),\n",
    "    (\"LLM-Generated Reward Model 1 from API\", LLMRewardFunction1API),\n",
    "    (\"LLM-Generated Reward Model 2 from API\", LLMRewardFunction2API),\n",
    "    (\"LLM-Generated Reward Model 1 - Finetuned\", LLMRewardFunction1),\n",
    "    (\"LLM-Generated Reward Model 2 - Finetuned\", LLMRewardFunction2)\n",
    "\n",
    "]\n",
    "\n",
    "# Store results in a dictionary\n",
    "rewardsDict = {}\n",
    "\n",
    "# Run experiments for each reward function\n",
    "episodes = 1000000\n",
    "for label, reward_fn in rewardFunctions:\n",
    "    rewardsDict[label] = train(env, agent, reward_fn, episodes)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "\n",
    "#Total Reward Comparison\n",
    "plt.subplot(2, 2, 1)\n",
    "for label, rewards in rewardsDict.items():\n",
    "    plt.plot(np.arange(episodes), rewards, label=label)\n",
    "\n",
    "\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "# 2.Running Average of Rewards\n",
    "\n",
    "# avgBaseline, stdBaseline = movingAverageAndStd(baselineRewards, windowSize=100)\n",
    "# avgLLM1, stdLLM1 = movingAverageAndStd(LLM1Rewards, windowSize=100)\n",
    "# avgLLM2, stdLLM2 = movingAverageAndStd(LLM2Rewards, windowSize=100)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "\n",
    "for label, rewards in rewardsDict.items():\n",
    "    avg, std = movingAverageAndStd(rewards, windowSize=100)\n",
    "    plt.plot(np.arange(len(avg)), avg, label=label)\n",
    "    plt.fill_between(np.arange(len(avg)), avg - std, avg + std, alpha=0.2, label=f\"{label} Variance\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Average Reward\")\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Episode Duration\n",
    "def episodeDuration(env, rewardModel, episodes=500):\n",
    "    durations = []\n",
    "    for episode in range(episodes):\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        duration = 0\n",
    "\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            observation, reward, done, _, _ = env.step(action)\n",
    "            duration += 1  \n",
    "        durations.append(duration)\n",
    "\n",
    "    return durations\n",
    "\n",
    "# Get the episode durations\n",
    "durationsDict = {label: episodeDuration(env, reward_fn, episodes) for label, reward_fn in rewardFunctions}\n",
    "\n",
    "\n",
    "# Plot Episode Duration\n",
    "plt.subplot(2, 2, 3)\n",
    "for label, durations in durationsDict.items():\n",
    "    plt.plot(np.arange(episodes), durations, label=label)\n",
    "\n",
    "\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Episode Duration(Steps)\")\n",
    "plt.legend()\n",
    "\n",
    "# 4.action distribution\n",
    "# def getActionDistributions(env, rewardModels, episodes):\n",
    "#     actionDistributions = {}\n",
    "#     for rewardModelName, rewardModel in rewardModels:\n",
    "#         actions = []\n",
    "#         for episode in range(episodes):\n",
    "#             observation = env.reset()[0]\n",
    "#             done = False\n",
    "\n",
    "#             while not done:\n",
    "#                 action = env.action_space.sample() \n",
    "#                 actions.append(action)\n",
    "#                 observation, _, done, _, _ = env.step(action)\n",
    "\n",
    "#         actionDistributions[rewardModelName] = actions \n",
    "\n",
    "#     return actionDistributions\n",
    "\n",
    "\n",
    "\n",
    "# actionDistributions = getActionDistributions(env, rewardFunctions, episodes)\n",
    "\n",
    "# # Plot action distributions\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# for rewardModelName, actions in actionDistributions.items():\n",
    "#     plt.hist(actions, bins=2, alpha=0.7, label=f\"{rewardModelName} Actions\")\n",
    "\n",
    "# plt.xlabel(\"Actions (0=Left, 1=Right)\")\n",
    "# plt.ylabel(\"Frequency\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# 5. Reward Distribution\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot for Baseline Rewards\n",
    "for i, (label, rewards) in enumerate(rewardsDict.items(), 1):\n",
    "    plt.subplot(1, len(rewardsDict.items()), i)\n",
    "    plt.hist(rewards, bins=20, alpha=0.7)\n",
    "    plt.title(label)\n",
    "    plt.xlabel(\"Reward\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 6. Success Rate Calc\n",
    "# successThreshold = 200\n",
    "\n",
    "# baselineSuccessRate = sum([1 if r >= successThreshold else 0 for r in baselineRewards]) / episodes\n",
    "# llmSuccessRate1 = sum([1 if r >= successThreshold else 0 for r in LLM1Rewards]) / episodes\n",
    "# llmSuccessRate2 = sum([1 if r >= successThreshold else 0 for r in LLM2Rewards]) / episodes\n",
    "\n",
    "# print(f\"Baseline Success Rate: {baselineSuccessRate * 100:.2f}%\")\n",
    "# print(f\"LLM-Generated Success Rate 1: {llmSuccessRate1 * 100:.2f}%\")\n",
    "# print(f\"LLM-Generated Success Rate 2: {llmSuccessRate2 * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def visualizeSuccessfulRuns(env, rewards_dict, durations_dict):\n",
    "    \n",
    "    # longest_runs_indices = np.argsort(durations)[-20:]  \n",
    "\n",
    "    env = gym.make('CartPole-v1', render_mode=\"human\")\n",
    "    agent = QLearningAgent(env)\n",
    "\n",
    "    for rewardModelName, rewards in rewards_dict.items():\n",
    "        durations = durations_dict[rewardModelName]\n",
    "        longest_runs_indices = np.argsort(durations)[-20:] # Get the indices of the top 20 longest runs\n",
    "\n",
    "        for idx in longest_runs_indices:\n",
    "            # print(f\"Duration: {durations[idx]}, Reward: {rewards[idx]}\")\n",
    "\n",
    "\n",
    "\n",
    "            observation, _ = env.reset()\n",
    "            state = agent.discretize(observation)\n",
    "            done = False\n",
    "\n",
    "\n",
    "            # print(f\"{rewardModelName} - Run {idx+1}\")\n",
    "\n",
    "            while not done:\n",
    "                action = agent.choose_action(state)\n",
    "                next_observation, reward, done, _, _ = env.step(action)\n",
    "                next_state = agent.discretize(next_observation)\n",
    "                state = next_state\n",
    "\n",
    "                # Render environment\n",
    "                env.render()\n",
    "\n",
    "                time.sleep(0.01)\n",
    "\n",
    "        print(f\"Next Model: {rewardModelName}\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "visualizeSuccessfulRuns(env, rewardsDict, durationsDict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maybe run many instances of reward models generated by Claude. Compare, with SoTA as baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Can also add an explainability metric**\n",
    "\n",
    "\n",
    "Prompt Engineering:\n",
    "Keep a log of the interaction with Claude. To see the developemnt. (See how these perform, run compute experiements)\n",
    "\n",
    "\n",
    "Eventually:(These can be developed by Claude)\n",
    "Adaptive learning rate \n",
    "Adaptive Reward Function - (For example change the weight of the pole, add friction. How does the reward function adapt?)\n",
    "\n",
    "\n",
    "\n",
    "-- Meeting 17/10/24 --\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Adaptive Learning Rate Work:\n",
    "\n",
    "Gradient of the reward function -> Informs the learning rate\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "YOU NEED TO:\n",
    "\n",
    "- Have a look at the work that already exists here. (e.g. Have a look at A2C with reward functions)\n",
    "\n",
    "- Run long simulations. Explain what is happenig in simulations (Why are there peaks, what is happening with the variance in the model? - Use Claudes Explanations)\n",
    "\n",
    "- Alter all code to run with Deep Q-Learning instead of Q-Learning\n",
    "\n",
    "\n",
    "Compare and contrast work already done. Look for space where I can contribute. What can I do that is different. Also whata type of work is already being done?:\n",
    "https://github.com/360ZMEM/LLMRsearcher-code\n",
    "https://360zmem.github.io/LLMRsearcher/\n",
    "\n",
    "https://github.com/schashni/Eureka \n",
    "\n",
    "\n",
    "**NO MORE PROGRAMMING UNTIL I HAVE A STRONG IDEA OF THE DOMAIN OF THE FIELD*\n",
    "\n",
    "\n",
    "\n",
    "-- \n",
    "\n",
    "You can create a virtual environment on the Lab PC to run your simulations.\n",
    "Use PuTTY & WinSCP\n",
    "\n",
    "Need to change to Deep Q-Learning or other learning method that utilizes GPUs.\n",
    "Also can implement parallel simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
