{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **State of the art Reward Functions for Cart Pole**\n",
    "https://github.com/openai/gym/wiki/Leaderboard\n",
    "\n",
    "\n",
    "Continuous Angle-Based Reward\n",
    "Provides a continuous reward based on the angle of the pole.\n",
    "```\n",
    "def decide(self, observation):\n",
    "        position, velocity, angle, angle_velocity = observation\n",
    "        action = int(3. * angle + angle_velocity > 0.)\n",
    "        return action\n",
    "```\n",
    "\n",
    "Position-Angle Balanced Reward\n",
    "Balances rewards between pole angle and cart position.\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "Velocity-Aware Reward\n",
    "Incorporates cart and pole velocities.\n",
    "```\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Claude's Reward Functions**\n",
    "\n",
    "```python\n",
    "def LLMOutput(observation, action):\n",
    "    x, _, angle, _ = observation\n",
    "    reward = 1.0 - abs(angle) - 0.5 * abs(x)\n",
    "    return max(0, reward)\n",
    "```\n",
    "\n",
    "1. Input:\n",
    "   - The function takes two parameters: `observation` and `action` (although `action` is not used in this function).\n",
    "   - `observation` is assumed to be a tuple or list containing four elements, but only the first (x) and third (angle) are used.\n",
    "\n",
    "2. Unpacking the observation:\n",
    "   - `x, _, angle, _` = observation\n",
    "   - This line extracts the position (x) and angle from the observation. The underscores (_) indicate that we're ignoring the second and fourth elements of the observation.\n",
    "\n",
    "3. Calculating the reward:\n",
    "   - `reward = 1.0 - abs(angle) - 0.5 * abs(x)`\n",
    "   - The reward starts at 1.0 and is reduced based on two factors:\n",
    "     a. The absolute value of the angle: This penalizes the pole for leaning away from vertical.\n",
    "     b. Half the absolute value of the position: This penalizes the cart for being far from the center.\n",
    "\n",
    "4. The logic behind this reward calculation:\n",
    "   - The goal is to keep the pole upright (angle close to 0) and the cart near the center (x close to 0).\n",
    "   - A larger angle or distance from center results in a lower reward.\n",
    "   - The angle has a stronger impact on the reward than the position (full weight vs. half weight).\n",
    "\n",
    "5. Returning the reward:\n",
    "   - `return max(0, reward)`\n",
    "   - This ensures the reward is never negative. If the calculated reward is negative, 0 is returned instead.\n",
    "\n",
    "In summary, this function calculates a reward based on how well the pole is balanced (angle) and how centered the cart is (x). The reward is highest (close to 1) when the pole is perfectly upright and the cart is in the center. It decreases as the pole leans more or the cart moves farther from the center, with the angle having a stronger influence on the reward than the position.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "def dynamicRewardFunction(observation, action):\n",
    "    x, _, angle, _ = observation\n",
    "    reward = 1.0\n",
    "    \n",
    "    if abs(x) > 2.4 or abs(angle) > 0.2:\n",
    "        reward = -10.0\n",
    "    else:\n",
    "        reward += 10.0 / (1.0 + abs(angle))\n",
    "        reward += 5.0 / (1.0 + abs(x))\n",
    "    \n",
    "    return int(reward)\n",
    "```\n",
    "\n",
    "Certainly! Let's break down the logic of this dynamic reward function:\n",
    "\n",
    "1. Function Input:\n",
    "   - The function takes two parameters: `observation` and `action`.\n",
    "   - `observation` is unpacked into four variables, but only `x` and `angle` are used.\n",
    "   - `x` represents the cart's position.\n",
    "   - `angle` represents the pole's angle from vertical.\n",
    "\n",
    "2. Initial Reward:\n",
    "   - The reward starts at 1.0.\n",
    "...\n",
    "This reward function encourages the learning algorithm to find a balance between keeping the pole upright and keeping the cart centered, with a stronger emphasis on pole verticality.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def dynamicRewardFunction(observation, action):\n",
      "    x, _, angle, _ = observation\n",
      "    reward = 1.0\n",
      "    if abs(x) > 2.4 or abs(angle) > 0.2:\n",
      "        reward = -10.0\n",
      "    else:\n",
      "        reward += (1.0 - abs(angle)) * 10\n",
      "        reward -= abs(x) * 0.5\n",
      "    return int(reward)\n",
      "\n",
      " ---------------------------------------------------- \n",
      "\n",
      "Certainly! Let's break down the logic behind this dynamic reward function:\n",
      "\n",
      "1. Input:\n",
      "   - `observation`: A tuple containing four values (x, _, angle, _)\n",
      "   - `action`: The action taken (not used in this function)\n",
      "\n",
      "2. Unpacking the observation:\n",
      "   `x, _, angle, _ = observation`\n",
      "   - `x`: The position of the cart\n",
      "   - `angle`: The angle of the pole (in radians)\n",
      "   - The underscores (_) represent unused values in the observation\n",
      "\n",
      "3. Base reward calculation:\n",
      "   `reward = 1.0 - abs(angle) - 0.5 * abs(x)`\n",
      "   - Starts with a base reward of 1.0\n",
      "   - Subtracts the absolute value of the angle (penalizes for tilting)\n",
      "   - Subtracts half the absolute value of the x position (penalizes for moving away from the center)\n",
      "\n",
      "4. Penalty for extreme states:\n",
      "   ```\n",
      "   if abs(angle) > 0.2 or abs(x) > 2.4:\n",
      "       reward -= 10\n",
      "   ```\n",
      "   - If the absolute angle is greater than 0.2 radians (about 11.5 degrees) OR\n",
      "   - If the absolute x position is greater than 2.4 units\n",
      "   - Then subtract 10 from the reward (heavy penalty for these extreme states)\n",
      "\n",
      "5. Return the calculated reward\n",
      "\n",
      "Logic explanation:\n",
      "- The function encourages keeping the pole as upright as possible (minimizing angle) and the cart as centered as possible (minimizing x).\n",
      "- The base reward decreases linearly with the angle and position offsets.\n",
      "- There's a higher weight on the angle (1.0) compared to the position (0.5), prioritizing keeping the pole upright over centering the cart.\n",
      "- The function heavily penalizes extreme states where the pole is tilting too much or the cart is too far from the center, as these states are close to failure.\n",
      "\n",
      "This reward structure guides the learning algorithm to find a policy that balances the pole while keeping the cart near the center, with a strong incentive to avoid extreme states that could lead to failure.\n",
      "\n",
      " ---------------------------------------------------- \n",
      " Reward Function Name:  dynamicRewardFunction\n",
      "Episode 1/1000, Total Reward: 94\n",
      "Episode 2/1000, Total Reward: 196\n",
      "Episode 3/1000, Total Reward: 76\n",
      "Episode 4/1000, Total Reward: 67\n",
      "Episode 5/1000, Total Reward: 76\n",
      "Episode 6/1000, Total Reward: 247\n",
      "Episode 7/1000, Total Reward: 77\n",
      "Episode 8/1000, Total Reward: 403\n",
      "Episode 9/1000, Total Reward: 87\n",
      "Episode 10/1000, Total Reward: 125\n",
      "Episode 11/1000, Total Reward: 451\n",
      "Episode 12/1000, Total Reward: 275\n",
      "Episode 13/1000, Total Reward: 107\n",
      "Episode 14/1000, Total Reward: 157\n",
      "Episode 15/1000, Total Reward: 379\n",
      "Episode 16/1000, Total Reward: 328\n",
      "Episode 17/1000, Total Reward: 94\n",
      "Episode 18/1000, Total Reward: 234\n",
      "Episode 19/1000, Total Reward: 225\n",
      "Episode 20/1000, Total Reward: 293\n",
      "Episode 21/1000, Total Reward: 177\n",
      "Episode 22/1000, Total Reward: 77\n",
      "Episode 23/1000, Total Reward: 242\n",
      "Episode 24/1000, Total Reward: 48\n",
      "Episode 25/1000, Total Reward: 67\n",
      "Episode 26/1000, Total Reward: 304\n",
      "Episode 27/1000, Total Reward: 117\n",
      "Episode 28/1000, Total Reward: 94\n",
      "Episode 29/1000, Total Reward: 76\n",
      "Episode 30/1000, Total Reward: 156\n",
      "Episode 31/1000, Total Reward: 97\n",
      "Episode 32/1000, Total Reward: 66\n",
      "Episode 33/1000, Total Reward: 48\n",
      "Episode 34/1000, Total Reward: 154\n",
      "Episode 35/1000, Total Reward: 247\n",
      "Episode 36/1000, Total Reward: 195\n",
      "Episode 37/1000, Total Reward: 107\n",
      "Episode 38/1000, Total Reward: 278\n",
      "Episode 39/1000, Total Reward: 115\n",
      "Episode 40/1000, Total Reward: 77\n",
      "Episode 41/1000, Total Reward: 95\n",
      "Episode 42/1000, Total Reward: 155\n",
      "Episode 43/1000, Total Reward: 141\n",
      "Episode 44/1000, Total Reward: 154\n",
      "Episode 45/1000, Total Reward: 286\n",
      "Episode 46/1000, Total Reward: 78\n",
      "Episode 47/1000, Total Reward: 186\n",
      "Episode 48/1000, Total Reward: 115\n",
      "Episode 49/1000, Total Reward: 314\n",
      "Episode 50/1000, Total Reward: 176\n",
      "Episode 51/1000, Total Reward: 87\n",
      "Episode 52/1000, Total Reward: 87\n",
      "Episode 53/1000, Total Reward: 165\n",
      "Episode 54/1000, Total Reward: 78\n",
      "Episode 55/1000, Total Reward: 166\n",
      "Episode 56/1000, Total Reward: 325\n",
      "Episode 57/1000, Total Reward: 156\n",
      "Episode 58/1000, Total Reward: 317\n",
      "Episode 59/1000, Total Reward: 160\n",
      "Episode 60/1000, Total Reward: 277\n",
      "Episode 61/1000, Total Reward: 126\n",
      "Episode 62/1000, Total Reward: 284\n",
      "Episode 63/1000, Total Reward: 176\n",
      "Episode 64/1000, Total Reward: 95\n",
      "Episode 65/1000, Total Reward: 124\n",
      "Episode 66/1000, Total Reward: 128\n",
      "Episode 67/1000, Total Reward: 116\n",
      "Episode 68/1000, Total Reward: 197\n",
      "Episode 69/1000, Total Reward: 237\n",
      "Episode 70/1000, Total Reward: 107\n",
      "Episode 71/1000, Total Reward: 247\n",
      "Episode 72/1000, Total Reward: 116\n",
      "Episode 73/1000, Total Reward: 178\n",
      "Episode 74/1000, Total Reward: 147\n",
      "Episode 75/1000, Total Reward: 105\n",
      "Episode 76/1000, Total Reward: 342\n",
      "Episode 77/1000, Total Reward: 108\n",
      "Episode 78/1000, Total Reward: 354\n",
      "Episode 79/1000, Total Reward: 166\n",
      "Episode 80/1000, Total Reward: 107\n",
      "Episode 81/1000, Total Reward: 107\n",
      "Episode 82/1000, Total Reward: 115\n",
      "Episode 83/1000, Total Reward: 97\n",
      "Episode 84/1000, Total Reward: 115\n",
      "Episode 85/1000, Total Reward: 164\n",
      "Episode 86/1000, Total Reward: 117\n",
      "Episode 87/1000, Total Reward: 235\n",
      "Episode 88/1000, Total Reward: 87\n",
      "Episode 89/1000, Total Reward: 144\n",
      "Episode 90/1000, Total Reward: 338\n",
      "Episode 91/1000, Total Reward: 104\n",
      "Episode 92/1000, Total Reward: 98\n",
      "Episode 93/1000, Total Reward: 106\n",
      "Episode 94/1000, Total Reward: 285\n",
      "Episode 95/1000, Total Reward: 125\n",
      "Episode 96/1000, Total Reward: 97\n",
      "Episode 97/1000, Total Reward: 176\n",
      "Episode 98/1000, Total Reward: 118\n",
      "Episode 99/1000, Total Reward: 177\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 133\u001b[0m\n\u001b[1;32m    130\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mchooseAction(state)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# Perform the action in the environment\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m nextObservation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m nextState \u001b[38;5;241m=\u001b[39m nextObservation\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Remember the experience in the replay memory\u001b[39;00m\n",
      "File \u001b[0;32m~/BachelorsThesis/Using-LLMs-to-Generate-Reward-Functions-from-Natural-Language-in-RL-Environments/cartPoleShared.py:81\u001b[0m, in \u001b[0;36mCustomCartPoleEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m---> 81\u001b[0m     observation, _, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;66;03m# Ensure rewardFunction is callable, otherwise fallback to default\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewardFunction \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewardFunction):\n",
      "File \u001b[0;32m~/.conda/envs/thesis/lib/python3.10/site-packages/gymnasium/wrappers/common.py:125\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[1;32m    114\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/.conda/envs/thesis/lib/python3.10/site-packages/gymnasium/wrappers/common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/thesis/lib/python3.10/site-packages/gymnasium/core.py:322\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    320\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    321\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/thesis/lib/python3.10/site-packages/gymnasium/wrappers/common.py:285\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/thesis/lib/python3.10/site-packages/gymnasium/envs/classic_control/cartpole.py:223\u001b[0m, in \u001b[0;36mCartPoleEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    220\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sutton_barto_reward \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 223\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m# truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32), reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, {}\n",
      "File \u001b[0;32m~/.conda/envs/thesis/lib/python3.10/site-packages/gymnasium/envs/classic_control/cartpole.py:337\u001b[0m, in \u001b[0;36mCartPoleEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    336\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[0;32m--> 337\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import anthropic\n",
    "from cartPoleShared import device\n",
    "from cartPoleShared import *\n",
    "\n",
    "\n",
    "\n",
    "#Dynmaically adding the LLMGeneratedRewardFunction Function\n",
    "\n",
    "def LLMRewardFunction(self, functionString):\n",
    "    localNamespace = {}\n",
    "    exec(functionString, globals(), localNamespace)\n",
    "    \n",
    "    new_function = None\n",
    "    for item in localNamespace.values():\n",
    "        if callable(item):\n",
    "            new_function = item\n",
    "            break\n",
    "    \n",
    "    if new_function is None:\n",
    "        raise ValueError(\"Invalid Function\")\n",
    "    \n",
    "    # Set the new function as the reward function\n",
    "    self.setRewardFunction(new_function)\n",
    "\n",
    "    \n",
    "def setRewardFunction(self, rewardFunction):\n",
    "    self.rewardFunction = rewardFunction\n",
    "\n",
    "#Added to CustomCartPoleEnv\n",
    "CustomCartPoleEnv.LLMRewardFunction = LLMRewardFunction\n",
    "CustomCartPoleEnv.setRewardFunction = setRewardFunction\n",
    "\n",
    "\n",
    "#Initilizating Class\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "env = CustomCartPoleEnv(env)\n",
    "\n",
    "# Determine stateSize and actionSize from the environment\n",
    "stateSize = env.observation_space.shape[0]\n",
    "actionSize = env.action_space.n\n",
    "\n",
    "# Initialize the DQLearningAgent with the required parameters\n",
    "agent = DQLearningAgent(env, stateSize=stateSize, actionSize=actionSize, device=device)\n",
    "\n",
    "\n",
    "#API Query\n",
    "apiKey=\"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\"\n",
    "modelName = \"claude-3-5-sonnet-20240620\"\n",
    "\n",
    "\n",
    "rewardFunctionMessage = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"You are a python code outputter. I want your output to only be python code and just be one function, which is an integer. No other text with it the output. \n",
    "        This function will be a reward function, named dynamicRewardFunction(), for a RL environment that follows the description below. \n",
    "        The inputs are observation and action in that order, the input observation can be broken down as follows: x, _, angle,_ = observation :\n",
    "        \n",
    "        This environment is a pole balanced on a cart that can move from left to right, \n",
    "        the idea is to keep the pole as upright as possible by moving the cart either left or right, \n",
    "        the information you have available to you is the position(x) and the angle(angle).\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "explanationMessage = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"I need you to explain the logic behind this following function:\n",
    "        def dynamicRewardFunction(observation, action):\n",
    "            x, _, angle, _ = observation\n",
    "            reward = 1.0 - abs(angle) - 0.5 * abs(x)\n",
    "            if abs(angle) > 0.2 or abs(x) > 2.4:\n",
    "                reward -= 10\n",
    "            return reward\n",
    "\n",
    "        This environment is a pole balanced on a cart that can move from left to right, \n",
    "        the idea is to keep the pole as upright as possible by moving the cart either left or right.\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Query the API for the reward function using a custom message\n",
    "generatedRewardFunction = queryAnthropicApi(apiKey, modelName, rewardFunctionMessage)\n",
    "# print(type(generatedRewardFunction))\n",
    "\n",
    "print(generatedRewardFunction)\n",
    "print(\"\\n ---------------------------------------------------- \\n\")\n",
    "\n",
    "# Get the explanation for the generated function using another custom message\n",
    "rewardExplanation = queryAnthropicExplanation(apiKey, modelName, explanationMessage)\n",
    "# print(type(generatedRewardFunction))\n",
    "# print(generatedRewardFunction)\n",
    "\n",
    "print(rewardExplanation)\n",
    "\n",
    "\n",
    "#Logging API Output\n",
    "\n",
    "logClaudeCall(rewardFunctionMessage, explanationMessage, generatedRewardFunction, rewardExplanation)\n",
    "\n",
    "# generatedRewardFunction = \"\"\"def LLMOutput(observation, action):\n",
    "#    x, _, angle, _ = observation\n",
    "#    reward = 1.0 - abs(angle) - 0.5 * abs(x)\n",
    "#    if abs(angle) > 0.2 or abs(x) > 2.4:\n",
    "#        reward -= 10\n",
    "#    return reward\n",
    "# \"\"\"#API call to Claude\n",
    "\n",
    "\n",
    "\n",
    "# Set initial policy and reward function\n",
    "env.LLMRewardFunction(generatedRewardFunction)\n",
    "\n",
    "print(\"\\n ---------------------------------------------------- \\n Reward Function Name: \", env.rewardFunction.__name__)\n",
    "\n",
    "\n",
    "\n",
    "# Main training loop\n",
    "numEpisodes = 1000\n",
    "batchSize = 32\n",
    "\n",
    "for episode in range(numEpisodes):\n",
    "    observation, info = env.reset(seed=42)\n",
    "    state = observation\n",
    "\n",
    "    done = False\n",
    "    totalReward = 0\n",
    "\n",
    "    while not done:\n",
    "        # Choose an action using the agent's policy\n",
    "        action = agent.chooseAction(state)\n",
    "\n",
    "        # Perform the action in the environment\n",
    "        nextObservation, reward, terminated, truncated, info = env.step(action)\n",
    "        nextState = nextObservation\n",
    "\n",
    "        # Remember the experience in the replay memory\n",
    "        agent.remember(state, action, reward, nextState, terminated or truncated)\n",
    "\n",
    "        # Update the state\n",
    "        state = nextState\n",
    "\n",
    "        # Update the total reward for this episode\n",
    "        totalReward += reward\n",
    "\n",
    "        # Check if the episode is done\n",
    "        done = terminated or truncated\n",
    "\n",
    "    # Train the agent using replay memory after each episode\n",
    "    agent.replay(batchSize=batchSize)\n",
    "\n",
    "    # Print progress\n",
    "    print(f\"Episode {episode + 1}/{numEpisodes}, Total Reward: {totalReward}\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alterations\n",
    "# - I could alter the prompt\n",
    "# - I could use other LLMs/Finetune\n",
    "\n",
    "\n",
    "# Could also implement a Policy that isnt learned using Q-Learning (Maybe A2C)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
