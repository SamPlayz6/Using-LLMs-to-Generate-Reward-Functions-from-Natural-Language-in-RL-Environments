{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **State of the art Reward Functions for Cart Pole**\n",
    "https://github.com/openai/gym/wiki/Leaderboard\n",
    "\n",
    "\n",
    "Continuous Angle-Based Reward\n",
    "Provides a continuous reward based on the angle of the pole.\n",
    "```\n",
    "def decide(self, observation):\n",
    "        position, velocity, angle, angle_velocity = observation\n",
    "        action = int(3. * angle + angle_velocity > 0.)\n",
    "        return action\n",
    "```\n",
    "\n",
    "Position-Angle Balanced Reward\n",
    "Balances rewards between pole angle and cart position.\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "Velocity-Aware Reward\n",
    "Incorporates cart and pole velocities.\n",
    "```\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Claude's Reward Functions**\n",
    "\n",
    "```python\n",
    "def LLMOutput(observation, action):\n",
    "    x, _, angle, _ = observation\n",
    "    reward = 1.0 - abs(angle) - 0.5 * abs(x)\n",
    "    return max(0, reward)\n",
    "```\n",
    "\n",
    "1. Input:\n",
    "   - The function takes two parameters: `observation` and `action` (although `action` is not used in this function).\n",
    "   - `observation` is assumed to be a tuple or list containing four elements, but only the first (x) and third (angle) are used.\n",
    "\n",
    "2. Unpacking the observation:\n",
    "   - `x, _, angle, _` = observation\n",
    "   - This line extracts the position (x) and angle from the observation. The underscores (_) indicate that we're ignoring the second and fourth elements of the observation.\n",
    "\n",
    "3. Calculating the reward:\n",
    "   - `reward = 1.0 - abs(angle) - 0.5 * abs(x)`\n",
    "   - The reward starts at 1.0 and is reduced based on two factors:\n",
    "     a. The absolute value of the angle: This penalizes the pole for leaning away from vertical.\n",
    "     b. Half the absolute value of the position: This penalizes the cart for being far from the center.\n",
    "\n",
    "4. The logic behind this reward calculation:\n",
    "   - The goal is to keep the pole upright (angle close to 0) and the cart near the center (x close to 0).\n",
    "   - A larger angle or distance from center results in a lower reward.\n",
    "   - The angle has a stronger impact on the reward than the position (full weight vs. half weight).\n",
    "\n",
    "5. Returning the reward:\n",
    "   - `return max(0, reward)`\n",
    "   - This ensures the reward is never negative. If the calculated reward is negative, 0 is returned instead.\n",
    "\n",
    "In summary, this function calculates a reward based on how well the pole is balanced (angle) and how centered the cart is (x). The reward is highest (close to 1) when the pole is perfectly upright and the cart is in the center. It decreases as the pole leans more or the cart moves farther from the center, with the angle having a stronger influence on the reward than the position.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "def dynamicRewardFunction(observation, action):\n",
    "    x, _, angle, _ = observation\n",
    "    reward = 1.0\n",
    "    \n",
    "    if abs(x) > 2.4 or abs(angle) > 0.2:\n",
    "        reward = -10.0\n",
    "    else:\n",
    "        reward += 10.0 / (1.0 + abs(angle))\n",
    "        reward += 5.0 / (1.0 + abs(x))\n",
    "    \n",
    "    return int(reward)\n",
    "```\n",
    "\n",
    "Certainly! Let's break down the logic of this dynamic reward function:\n",
    "\n",
    "1. Function Input:\n",
    "   - The function takes two parameters: `observation` and `action`.\n",
    "   - `observation` is unpacked into four variables, but only `x` and `angle` are used.\n",
    "   - `x` represents the cart's position.\n",
    "   - `angle` represents the pole's angle from vertical.\n",
    "\n",
    "2. Initial Reward:\n",
    "   - The reward starts at 1.0.\n",
    "...\n",
    "This reward function encourages the learning algorithm to find a balance between keeping the pole upright and keeping the cart centered, with a stronger emphasis on pole verticality.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "class CustomCartPoleEnv(gym.Wrapper):\n",
    "    def __init__(self, env, n_bins=10):\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "        self.rewardFunction = self.angleBasedReward\n",
    "        \n",
    "        # Define the number of discrete bins for each dimension of the observation space\n",
    "        self.n_bins = n_bins\n",
    "        self.bins = [\n",
    "            np.linspace(-4.8, 4.8, self.n_bins),  # Position\n",
    "            np.linspace(-5, 5, self.n_bins),      # Velocity\n",
    "            np.linspace(-0.418, 0.418, self.n_bins),  # Angle\n",
    "            np.linspace(-5, 5, self.n_bins)       # Angular velocity\n",
    "        ]\n",
    "        \n",
    "        # Initialize Q-table with zeros (discretized state space)\n",
    "        self.q_table = np.zeros((self.n_bins, self.n_bins, self.n_bins, self.n_bins, env.action_space.n))\n",
    "\n",
    "\n",
    "        self.learning_rate = 0.1\n",
    "        self.discount_factor = 0.99\n",
    "        self.epsilon = 0.1\n",
    "    \n",
    "    def discretize(self, observation):\n",
    "        state = []\n",
    "        for i, obs in enumerate(observation):\n",
    "            state.append(np.digitize(obs, self.bins[i]) - 1)\n",
    "        return tuple(state)\n",
    "    \n",
    "\n",
    "    def step(self, action):\n",
    "        observation, _, terminated, truncated, info = self.env.step(action)\n",
    "        reward = self.rewardFunction(observation, action)\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "#---\n",
    "\n",
    "\n",
    "\n",
    "    def angleBasedReward(self, observation, action):\n",
    "        _, _, angle, _ = observation\n",
    "        return np.cos(angle)\n",
    "\n",
    "    def LLMRewardFunction(self, functionString):\n",
    "        localNamespace = {}\n",
    "        exec(functionString, globals(), localNamespace)\n",
    "        \n",
    "        new_function = None\n",
    "        for item in localNamespace.values():\n",
    "            if callable(item):\n",
    "                new_function = item\n",
    "                break\n",
    "        \n",
    "        if new_function is None:\n",
    "            raise ValueError(\"Invalid Function\")\n",
    "        \n",
    "        # Set the new function as the reward function\n",
    "        self.setRewardFunction(new_function)\n",
    "\n",
    "\n",
    "#---\n",
    "\n",
    "\n",
    "\n",
    "    def setRewardFunction(self, rewardFunction):\n",
    "        self.rewardFunction = rewardFunction\n",
    "\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.random() < self.epsilon:  # Exploration\n",
    "            return self.env.action_space.sample()\n",
    "        else:  # Exploitation\n",
    "            return np.argmax(self.q_table[state])\n",
    "\n",
    "\n",
    "    def update_q_table(self, state, action, reward, next_state):\n",
    "        best_next_action = np.argmax(self.q_table[next_state])\n",
    "        td_target = reward + self.discount_factor * self.q_table[next_state][best_next_action]\n",
    "        td_error = td_target - self.q_table[state][action]\n",
    "        self.q_table[state][action] += self.learning_rate * td_error\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(env, episodes=500):\n",
    "    rewards = []\n",
    "    for episode in range(episodes):\n",
    "        observation = env.reset()[0]\n",
    "        state = env.discretize(observation)\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = env.choose_action(state)\n",
    "            next_observation, reward, done, _, _ = env.step(action)\n",
    "            next_state = env.discretize(next_observation)\n",
    "            env.update_q_table(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        \n",
    "        rewards.append(total_reward)\n",
    "    \n",
    "    return rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def dynamicRewardFunction(observation, action):\n",
      "    x, _, angle, _ = observation\n",
      "    reward = 1.0 - abs(angle) - 0.5 * abs(x)\n",
      "    return int(max(0, reward * 100))\n",
      "\n",
      " ---------------------------------------------------- \n",
      "\n",
      "Certainly! Let's break down the logic behind this dynamic reward function:\n",
      "\n",
      "1. Input:\n",
      "   - `observation`: A tuple containing state information, where:\n",
      "     - `x`: The position of the cart (negative values are left, positive are right)\n",
      "     - `angle`: The angle of the pole (0 is upright, positive is leaning right, negative is leaning left)\n",
      "   - `action`: The action taken (not used in this function)\n",
      "\n",
      "2. Reward Calculation:\n",
      "   `reward = 1.0 - abs(angle) - 0.5 * abs(x)`\n",
      "\n",
      "   This formula is designed to encourage two main behaviors:\n",
      "   a) Keeping the pole upright: `1.0 - abs(angle)`\n",
      "      - The more vertical the pole (closer to 0 angle), the higher this part of the reward.\n",
      "      - As the pole leans more (increasing |angle|), this part of the reward decreases.\n",
      "\n",
      "   b) Keeping the cart centered: `0.5 * abs(x)`\n",
      "      - The closer the cart is to the center (x = 0), the smaller this deduction.\n",
      "      - As the cart moves away from the center (increasing |x|), more is subtracted from the reward.\n",
      "      - The factor 0.5 makes this less important than keeping the pole upright.\n",
      "\n",
      "3. Reward Scaling and Bounding:\n",
      "   `return int(max(0, reward * 100))`\n",
      "\n",
      "   - The reward is multiplied by 100 to scale it up.\n",
      "   - The `max(0, ...)` ensures the reward is never negative.\n",
      "   - The `int(...)` converts the result to an integer.\n",
      "\n",
      "Logic behind this approach:\n",
      "- The agent receives the highest reward when the pole is perfectly upright (angle = 0) and the cart is at the center (x = 0).\n",
      "- As the pole leans more or the cart moves away from the center, the reward decreases.\n",
      "- Keeping the pole upright is prioritized over keeping the cart centered (note the 0.5 factor for x).\n",
      "- The reward is always non-negative and integer-valued, which can be beneficial for certain learning algorithms.\n",
      "\n",
      "This reward function encourages the agent to learn a policy that balances the pole while trying to keep the cart near the center, with a stronger emphasis on pole balance.\n",
      "\n",
      " ---------------------------------------------------- \n",
      " Reward Function Name:  dynamicRewardFunction\n"
     ]
    }
   ],
   "source": [
    "import anthropic\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "env = CustomCartPoleEnv(env)\n",
    "\n",
    "#API Query\n",
    "client = anthropic.Anthropic(\n",
    "    # defaults to os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "    api_key=\"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\",\n",
    ")\n",
    "generatedRewardFunction = client.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=1024,\n",
    "    messages=[\n",
    "        {\"role\": \"user\"\n",
    "\n",
    "         , \"content\": \"\"\"You are a python code outputter. I want your output to only be python code and just be one function, which is an integer. No other text with it the output. \n",
    "         This function will be a reward function, named dynamicRewardFunction(), for a RL environment that follows the description below. \n",
    "         The inputs are observation and action in that order, the input observation can be broken down as follows: x, _, angle,_ = observation :\n",
    "         \n",
    "         This environment is a pole balanced on a cart that can move from left to right, \n",
    "         the idea is to keep the pole as upright as possible by moving the cart either left or right, \n",
    "         the information you have available to you is the position(x) and the angle(angle).\"\"\"}\n",
    "    ]\n",
    ")\n",
    "print(generatedRewardFunction.content[0].text)\n",
    "print(\"\\n ---------------------------------------------------- \\n\")\n",
    "\n",
    "rewardFunctionExplanation = client.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=1024,\n",
    "    messages=[\n",
    "        {\"role\": \"user\"\n",
    "\n",
    "         , \"content\": f\"\"\"I need you to explain the logic behind this following function:\n",
    "        {generatedRewardFunction.content[0].text} \n",
    "\n",
    "        This environment is a pole balanced on a cart that can move from left to right, \n",
    "         the idea is to keep the pole as upright as possible by moving the cart either left or right, \n",
    "         the information you have available to you is the position(x) and the angle(angle).\n",
    "         \"\"\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(rewardFunctionExplanation.content[0].text)\n",
    "\n",
    "# generatedRewardFunction = \"\"\"def LLMOutput(observation, action):\n",
    "#    x, _, angle, _ = observation\n",
    "#    reward = 1.0 - abs(angle) - 0.5 * abs(x)\n",
    "#    if abs(angle) > 0.2 or abs(x) > 2.4:\n",
    "#        reward -= 10\n",
    "#    return reward\n",
    "# \"\"\"#API call to Claude\n",
    "\n",
    "\n",
    "\n",
    "# Set initial policy and reward function\n",
    "env.LLMRewardFunction(generatedRewardFunction.content[0].text)\n",
    "print(\"\\n ---------------------------------------------------- \\n Reward Function Name: \", env.rewardFunction.__name__)\n",
    "\n",
    "\n",
    "\n",
    "#Main loop\n",
    "observation, info = env.reset(seed=42)\n",
    "state = env.discretize(observation)\n",
    "\n",
    "for _ in range(1000):\n",
    "    action = env.choose_action(state)  #Q-learning policy\n",
    "    next_observation, reward, terminated, truncated, info = env.step(action)\n",
    "    next_state = env.discretize(next_observation)  # Discretize the next state\n",
    "\n",
    "\n",
    "    env.update_q_table(state, action, reward, next_state)\n",
    "    \n",
    "    state = next_state \n",
    "\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "        state = env.discretize(observation) \n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alterations\n",
    "# - I could alter the prompt\n",
    "# - I could use other LLMs/Finetune\n",
    "\n",
    "\n",
    "# Could also implement a Policy that isnt learned using Q-Learning (Maybe A2C)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gymenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
