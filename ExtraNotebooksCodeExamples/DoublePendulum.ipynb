{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a Python function that implements a reward function for a double pendulum reinforcement learning environment based on your description:\n",
      "\n",
      "```python\n",
      "def dynamicRewardFunction(observation, action):\n",
      "    theta1, theta2, theta1_dot, theta2_dot = observation\n",
      "    \n",
      "    # Constants for reward scaling\n",
      "    UPRIGHT_REWARD = 1.0\n",
      "    VELOCITY_PENALTY = 0.1\n",
      "    \n",
      "    # Calculate reward for being upright\n",
      "    upright_reward = UPRIGHT_REWARD * (np.cos(theta1) + np.cos(theta2))\n",
      "    \n",
      "    # Calculate penalty for angular velocities\n",
      "    velocity_penalty = VELOCITY_PENALTY * (theta1_dot**2 + theta2_dot**2)\n",
      "    \n",
      "    # Combine rewards and penalties\n",
      "    total_reward = upright_reward - velocity_penalty\n",
      "    \n",
      "    return total_reward\n",
      "```\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 1) (<string>, line 1)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\samdd\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3577\u001b[0m in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[0;32mIn[1], line 46\u001b[0m\n    env.LLMRewardFunction(generatedRewardFunction)\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32mc:\\Users\\samdd\\Desktop\\College\\4th Year\\FYProject\\SimpleCartPoleImplementation\\cartPoleShared.py:53\u001b[1;36m in \u001b[1;35mLLMRewardFunction\u001b[1;36m\n\u001b[1;33m    exec(functionString, globals(), localNamespace)\u001b[1;36m\n",
      "\u001b[1;36m  File \u001b[1;32m<string>:1\u001b[1;36m\u001b[0m\n\u001b[1;33m    Here's a Python function that implements a reward function for a double pendulum reinforcement learning environment based on your description:\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated string literal (detected at line 1)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#Try to implement some code with multilink cartpole (Can build to a full body)\n",
    "from cartPoleShared import * # Might be a problem is I import all\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "class TwoLinkEnvWrapper(CustomCartPoleEnv):\n",
    "    # This seemed to be a fairly standard reward function - Not sure which one is best to use as a baseline\n",
    "    def defaultRewardFunction(self, observation, action):\n",
    "        theta1, theta2, theta1_dot, theta2_dot = observation\n",
    "        reward = -(abs(theta1) + abs(theta2) + abs(theta1_dot) + abs(theta2_dot))\n",
    "        return reward\n",
    "    \n",
    "\n",
    "\n",
    "#API Query\n",
    "api_key=\"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\"\n",
    "model_name = \"claude-3-5-sonnet-20240620\"\n",
    "\n",
    "reward_function_message = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"You are a python code outputter. I want your output to only be python code and just be one function. \n",
    "        This function will be a reward function, named dynamicRewardFunction(), for a RL environment that follows the description below. \n",
    "        The inputs are observation and action in that order. The observation can be broken down as follows: \n",
    "        theta1, theta2, theta1_dot, theta2_dot = observation.\n",
    "\n",
    "        This environment is a double-pendulum (two-link) system. The goal is to balance the pendulums upright with minimal angular velocity. \n",
    "        Please generate a reward function that rewards keeping the pendulums upright and penalizes large angular velocities.\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Call the API to generate the reward function\n",
    "generatedRewardFunction = queryAnthropicApi(api_key, model_name, reward_function_message)\n",
    "print(generatedRewardFunction)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set up the two-link environment\n",
    "env = gym.make('Acrobot-v1') \n",
    "env = TwoLinkEnvWrapper(env)\n",
    "agent = QLearningAgent(env)\n",
    "\n",
    "\n",
    "env.LLMRewardFunction(generatedRewardFunction)\n",
    "\n",
    "\n",
    "episodes = 1000\n",
    "rewards = train(agent, env, episodes)\n",
    "\n",
    "\n",
    "plt.plot(np.arange(episodes), rewards)\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"Reward over Time with LLM-Generated Reward Function\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gymenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
