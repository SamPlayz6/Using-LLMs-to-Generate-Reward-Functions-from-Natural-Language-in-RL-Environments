{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f453cd8-341c-4e4f-8a51-e27edfd00383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up CartPole for Open Day demonstration...\n",
      "Environment parameters updated: masscart=1.0, length=2.5, gravity=9.8\n",
      "Starting with pole length: 2.5m\n",
      "Loading pre-trained model from /home/sd37/BachelorsThesis/Using-LLMs-to-Generate-Reward-Functions-from-Natural-Language-in-RL-Environments/ExtraNotebooksCodeExamples/Final Report Graph Generations/cartpole_pretrained_model.pt\n",
      "Pre-trained model loaded successfully!\n",
      "\n",
      "ðŸŽ® DEMO STARTED! Pole length will change every 2 minutes\n",
      "Next change scheduled at: 09:13:56\n",
      "Press Ctrl+C to stop the demonstration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1861546/385806378.py:64: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  agent.model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode completed: #1, Steps: 88, Length: 2.5m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sd37/BachelorsThesis/Using-LLMs-to-Generate-Reward-Functions-from-Natural-Language-in-RL-Environments/RLEnvironment/training/agent.py:83: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  states = torch.tensor([t[0] for t in minibatch], dtype=torch.float32).to(self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode completed: #2, Steps: 92, Length: 2.5m\n",
      "Episode completed: #3, Steps: 64, Length: 2.5m\n",
      "Episode completed: #4, Steps: 48, Length: 2.5m\n",
      "Episode completed: #5, Steps: 39, Length: 2.5m\n",
      "Episode completed: #6, Steps: 87, Length: 2.5m\n",
      "Episode completed: #7, Steps: 57, Length: 2.5m\n",
      "Episode completed: #8, Steps: 39, Length: 2.5m\n",
      "Episode completed: #9, Steps: 35, Length: 2.5m\n",
      "Episode completed: #10, Steps: 27, Length: 2.5m\n",
      "Episode completed: #11, Steps: 27, Length: 2.5m\n",
      "Episode completed: #12, Steps: 23, Length: 2.5m\n",
      "Episode completed: #13, Steps: 23, Length: 2.5m\n",
      "Episode completed: #14, Steps: 10, Length: 2.5m\n",
      "Episode completed: #15, Steps: 20, Length: 2.5m\n",
      "Episode completed: #16, Steps: 9, Length: 2.5m\n",
      "Episode completed: #17, Steps: 10, Length: 2.5m\n",
      "Episode completed: #18, Steps: 10, Length: 2.5m\n",
      "Episode completed: #19, Steps: 10, Length: 2.5m\n",
      "Episode completed: #20, Steps: 10, Length: 2.5m\n",
      "Episode completed: #21, Steps: 13, Length: 2.5m\n",
      "Episode completed: #22, Steps: 11, Length: 2.5m\n",
      "Episode completed: #23, Steps: 10, Length: 2.5m\n",
      "Episode completed: #24, Steps: 10, Length: 2.5m\n",
      "Episode completed: #25, Steps: 10, Length: 2.5m\n",
      "Episode completed: #26, Steps: 10, Length: 2.5m\n",
      "Episode completed: #27, Steps: 10, Length: 2.5m\n",
      "Episode completed: #28, Steps: 10, Length: 2.5m\n",
      "Episode completed: #29, Steps: 12, Length: 2.5m\n",
      "Episode completed: #30, Steps: 13, Length: 2.5m\n",
      "Episode completed: #31, Steps: 19, Length: 2.5m\n",
      "Episode completed: #32, Steps: 18, Length: 2.5m\n",
      "Episode completed: #33, Steps: 28, Length: 2.5m\n",
      "Episode completed: #34, Steps: 30, Length: 2.5m\n",
      "Episode completed: #35, Steps: 36, Length: 2.5m\n",
      "Episode completed: #36, Steps: 33, Length: 2.5m\n",
      "Episode completed: #37, Steps: 71, Length: 2.5m\n",
      "Episode completed: #38, Steps: 46, Length: 2.5m\n",
      "Episode completed: #39, Steps: 64, Length: 2.5m\n",
      "Episode completed: #40, Steps: 71, Length: 2.5m\n",
      "Episode completed: #41, Steps: 67, Length: 2.5m\n",
      "Episode completed: #42, Steps: 72, Length: 2.5m\n",
      "Episode completed: #43, Steps: 76, Length: 2.5m\n",
      "Episode completed: #44, Steps: 73, Length: 2.5m\n",
      "Episode completed: #45, Steps: 73, Length: 2.5m\n",
      "Episode completed: #46, Steps: 72, Length: 2.5m\n",
      "Episode completed: #47, Steps: 73, Length: 2.5m\n",
      "Episode completed: #48, Steps: 74, Length: 2.5m\n",
      "Episode completed: #49, Steps: 71, Length: 2.5m\n",
      "Episode completed: #50, Steps: 71, Length: 2.5m\n",
      "Episode completed: #51, Steps: 74, Length: 2.5m\n",
      "Episode completed: #52, Steps: 73, Length: 2.5m\n",
      "Episode completed: #53, Steps: 66, Length: 2.5m\n",
      "Episode completed: #54, Steps: 68, Length: 2.5m\n",
      "Episode completed: #55, Steps: 64, Length: 2.5m\n",
      "Episode completed: #56, Steps: 70, Length: 2.5m\n",
      "Episode completed: #57, Steps: 69, Length: 2.5m\n",
      "Episode completed: #58, Steps: 65, Length: 2.5m\n",
      "Episode completed: #59, Steps: 67, Length: 2.5m\n",
      "Episode completed: #60, Steps: 70, Length: 2.5m\n",
      "Episode completed: #61, Steps: 61, Length: 2.5m\n",
      "Episode completed: #62, Steps: 63, Length: 2.5m\n",
      "Episode completed: #63, Steps: 64, Length: 2.5m\n",
      "Episode completed: #64, Steps: 58, Length: 2.5m\n",
      "Episode completed: #65, Steps: 55, Length: 2.5m\n",
      "Episode completed: #66, Steps: 59, Length: 2.5m\n",
      "Episode completed: #67, Steps: 61, Length: 2.5m\n",
      "Episode completed: #68, Steps: 62, Length: 2.5m\n",
      "Episode completed: #69, Steps: 68, Length: 2.5m\n",
      "Episode completed: #70, Steps: 66, Length: 2.5m\n",
      "Episode completed: #71, Steps: 53, Length: 2.5m\n",
      "Episode completed: #72, Steps: 58, Length: 2.5m\n",
      "Episode completed: #73, Steps: 64, Length: 2.5m\n",
      "Episode completed: #74, Steps: 57, Length: 2.5m\n",
      "Episode completed: #75, Steps: 54, Length: 2.5m\n",
      "Episode completed: #76, Steps: 66, Length: 2.5m\n",
      "Episode completed: #77, Steps: 62, Length: 2.5m\n",
      "Episode completed: #78, Steps: 62, Length: 2.5m\n",
      "Episode completed: #79, Steps: 70, Length: 2.5m\n",
      "Episode completed: #80, Steps: 59, Length: 2.5m\n",
      "Episode completed: #81, Steps: 57, Length: 2.5m\n",
      "Episode completed: #82, Steps: 72, Length: 2.5m\n",
      "Episode completed: #83, Steps: 75, Length: 2.5m\n",
      "Episode completed: #84, Steps: 63, Length: 2.5m\n",
      "Episode completed: #85, Steps: 59, Length: 2.5m\n",
      "Episode completed: #86, Steps: 62, Length: 2.5m\n",
      "Episode completed: #87, Steps: 58, Length: 2.5m\n",
      "Episode completed: #88, Steps: 68, Length: 2.5m\n",
      "Episode completed: #89, Steps: 74, Length: 2.5m\n",
      "Episode completed: #90, Steps: 65, Length: 2.5m\n",
      "Episode completed: #91, Steps: 76, Length: 2.5m\n",
      "Episode completed: #92, Steps: 74, Length: 2.5m\n",
      "Episode completed: #93, Steps: 72, Length: 2.5m\n",
      "Episode completed: #94, Steps: 68, Length: 2.5m\n",
      "Episode completed: #95, Steps: 81, Length: 2.5m\n",
      "Episode completed: #96, Steps: 63, Length: 2.5m\n",
      "Episode completed: #97, Steps: 65, Length: 2.5m\n",
      "Episode completed: #98, Steps: 64, Length: 2.5m\n",
      "Episode completed: #99, Steps: 78, Length: 2.5m\n",
      "Episode completed: #100, Steps: 77, Length: 2.5m\n",
      "Episode completed: #101, Steps: 72, Length: 2.5m\n",
      "Episode completed: #102, Steps: 83, Length: 2.5m\n",
      "Episode completed: #103, Steps: 79, Length: 2.5m\n",
      "Episode completed: #104, Steps: 69, Length: 2.5m\n",
      "Episode completed: #105, Steps: 71, Length: 2.5m\n",
      "Episode completed: #106, Steps: 102, Length: 2.5m\n",
      "Episode completed: #107, Steps: 79, Length: 2.5m\n",
      "\n",
      "ðŸš¨ ENVIRONMENT CHANGE ðŸš¨\n",
      "Changing pole length from 2.5m to 3.0m\n",
      "Environment parameters updated: masscart=1.0, length=3.0, gravity=9.8\n",
      "Next change scheduled at: 09:15:56\n",
      "ðŸ§  Updating reward function to adapt to new length...\n",
      "\n",
      "Generating new stability reward function...\n",
      "Update count: 0/3\n",
      "\n",
      "Proposed Function:\n",
      "Here's a modified version of the stability reward function with detailed inline comments:\n",
      "\n",
      "```python\n",
      "def reward_function(observation, action):\n",
      "    # Extract relevant state variables from observation\n",
      "    cart_position = observation[0]\n",
      "    pole_angle = observation[2]\n",
      "    \n",
      "    # Primary reward component: Pole angle stability\n",
      "    # Use cosine to create a smooth, symmetric reward centered at 0 (upright position)\n",
      "    # Scaled to range 0 to 2, then shifted to range -1 to 1\n",
      "    angle_stability = 2 * (math.cos(pole_angle) - 0.5)\n",
      "    \n",
      "    # Secondary penalty: Cart position\n",
      "    # Penalize cart for moving too far from center (0)\n",
      "    # Use a quadratic penalty to increase cost as cart moves further out\n",
      "    position_penalty = -0.1 * cart_position**2\n",
      "    \n",
      "    # Combine components with appropriate weights\n",
      "    # Angle stability has highest positive weight (2.0) as the primary objective\n",
      "    # Position penalty has a smaller negative weight (-0.1) as a secondary consideration\n",
      "    total_reward = 2.0 * angle_stability + position_penalty\n",
      "    \n",
      "    # Ensure the reward is positive when the pole is close to upright, even if the cart is off-center\n",
      "    total_reward = max(total_reward, 0.1)\n",
      "    \n",
      "    return total_reward\n",
      "```\n",
      "\n",
      "This function makes small improvements while adhering to the given constraints and priorities:\n",
      "\n",
      "1. It uses only the `observation` and `action` inputs as required.\n",
      "2. The pole angle stability remains the primary reward component with the highest positive weight (2.0).\n",
      "3. The cart position is a secondary penalty term with a smaller weight (-0.1).\n",
      "4. The function is kept simple with only two components.\n",
      "5. The weights are within the suggested ranges.\n",
      "6. The reward is guaranteed to be positive when the pole is upright, even if the cart is off-center, thanks to the `max` function at the end.\n",
      "7. The function focuses on stability aspects, using cosine for a smooth angle reward and a quadratic penalty for position.\n",
      "\n",
      "These changes should provide a stable reward signal that prioritizes keeping the pole upright while also encouraging the cart to stay near the center.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sd37/.conda/envs/thesis/lib/python3.10/site-packages/numpy/_core/fromnumeric.py:3904: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/sd37/.conda/envs/thesis/lib/python3.10/site-packages/numpy/_core/_methods.py:147: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating new efficiency reward function...\n",
      "Update count: 0/3\n",
      "\n",
      "Proposed Function:\n",
      "Here's a modified version of the reward function with detailed inline comments, focusing on efficiency aspects while adhering to the given constraints and requirements:\n",
      "\n",
      "```python\n",
      "def reward_function(observation, action):\n",
      "    # Extract state variables from observation\n",
      "    cart_position = observation[0]\n",
      "    cart_velocity = observation[1]\n",
      "    pole_angle = observation[2]\n",
      "    pole_angular_velocity = observation[3]\n",
      "\n",
      "    # Primary reward component: Pole angle stability (highest positive weight)\n",
      "    angle_stability_reward = 2.0 * (1 - abs(pole_angle))  # Weight: 2.0, decreases as angle deviates from 0\n",
      "\n",
      "    # Secondary penalty: Cart position (smaller negative weight)\n",
      "    position_penalty = -0.2 * abs(cart_position)  # Weight: -0.2, increases as cart moves from center\n",
      "\n",
      "    # Secondary penalty: Cart velocity (smaller negative weight)\n",
      "    velocity_penalty = -0.1 * abs(cart_velocity)  # Weight: -0.1, increases with cart speed\n",
      "\n",
      "    # Combine rewards and penalties\n",
      "    total_reward = angle_stability_reward + position_penalty + velocity_penalty\n",
      "\n",
      "    # Ensure the reward is positive when the pole is upright, even if the cart is off-center\n",
      "    total_reward = max(total_reward, 0.1)\n",
      "\n",
      "    return total_reward\n",
      "```\n",
      "\n",
      "This function makes small, incremental improvements while focusing on efficiency aspects:\n",
      "\n",
      "1. It maintains the critical priority structure with pole angle stability as the primary reward component with the highest positive weight (2.0).\n",
      "\n",
      "2. Cart position and velocity are used as secondary penalty terms with smaller weights (-0.2 and -0.1 respectively).\n",
      "\n",
      "3. The sum of penalty weights (-0.3) does not exceed 50% of the stability reward weight (2.0).\n",
      "\n",
      "4. The function signature and use of observation and action variables comply with the given constraints.\n",
      "\n",
      "5. The function is kept simple with only 3 components: angle stability, position penalty, and velocity penalty.\n",
      "\n",
      "6. Scaling factors are moderate and within the suggested ranges.\n",
      "\n",
      "7. The function ensures a positive reward when the pole is upright, even if the cart is off-center, by using the max() function to set a minimum reward of 0.1.\n",
      "\n",
      "8. Detailed inline comments explain each component and its purpose.\n",
      "\n",
      "This revised function should provide a balance between pole stability and cart efficiency, encouraging the agent to keep the pole upright while minimizing cart movement and velocity.\n",
      "Episode completed: #108, Steps: 80, Length: 3.0m\n",
      "Episode completed: #109, Steps: 90, Length: 3.0m\n",
      "Episode completed: #110, Steps: 100, Length: 3.0m\n",
      "Episode completed: #111, Steps: 91, Length: 3.0m\n",
      "Episode completed: #112, Steps: 91, Length: 3.0m\n",
      "Episode completed: #113, Steps: 80, Length: 3.0m\n",
      "Episode completed: #114, Steps: 147, Length: 3.0m\n",
      "Episode completed: #115, Steps: 93, Length: 3.0m\n",
      "Episode completed: #116, Steps: 194, Length: 3.0m\n",
      "Episode completed: #117, Steps: 172, Length: 3.0m\n",
      "Episode completed: #118, Steps: 122, Length: 3.0m\n",
      "Episode completed: #119, Steps: 123, Length: 3.0m\n",
      "Episode completed: #120, Steps: 125, Length: 3.0m\n",
      "Episode completed: #121, Steps: 110, Length: 3.0m\n",
      "Episode completed: #122, Steps: 39, Length: 3.0m\n",
      "Episode completed: #123, Steps: 35, Length: 3.0m\n",
      "Episode completed: #124, Steps: 33, Length: 3.0m\n",
      "Episode completed: #125, Steps: 34, Length: 3.0m\n",
      "Episode completed: #126, Steps: 23, Length: 3.0m\n",
      "Episode completed: #127, Steps: 27, Length: 3.0m\n",
      "Episode completed: #128, Steps: 32, Length: 3.0m\n",
      "Episode completed: #129, Steps: 24, Length: 3.0m\n",
      "Episode completed: #130, Steps: 26, Length: 3.0m\n",
      "Episode completed: #131, Steps: 26, Length: 3.0m\n",
      "Episode completed: #132, Steps: 32, Length: 3.0m\n",
      "Episode completed: #133, Steps: 29, Length: 3.0m\n",
      "Episode completed: #134, Steps: 27, Length: 3.0m\n",
      "Episode completed: #135, Steps: 35, Length: 3.0m\n",
      "Episode completed: #136, Steps: 33, Length: 3.0m\n",
      "Episode completed: #137, Steps: 35, Length: 3.0m\n",
      "Episode completed: #138, Steps: 64, Length: 3.0m\n",
      "Episode completed: #139, Steps: 74, Length: 3.0m\n",
      "Episode completed: #140, Steps: 114, Length: 3.0m\n",
      "Episode completed: #141, Steps: 110, Length: 3.0m\n",
      "Episode completed: #142, Steps: 126, Length: 3.0m\n",
      "Episode completed: #143, Steps: 138, Length: 3.0m\n",
      "Episode completed: #144, Steps: 120, Length: 3.0m\n",
      "Episode completed: #145, Steps: 129, Length: 3.0m\n",
      "Episode completed: #146, Steps: 158, Length: 3.0m\n",
      "Episode completed: #147, Steps: 145, Length: 3.0m\n",
      "Episode completed: #148, Steps: 151, Length: 3.0m\n",
      "Episode completed: #149, Steps: 180, Length: 3.0m\n",
      "Episode completed: #150, Steps: 146, Length: 3.0m\n",
      "Episode completed: #151, Steps: 136, Length: 3.0m\n",
      "Episode completed: #152, Steps: 163, Length: 3.0m\n",
      "Episode completed: #153, Steps: 151, Length: 3.0m\n",
      "Episode completed: #154, Steps: 154, Length: 3.0m\n",
      "Episode completed: #155, Steps: 128, Length: 3.0m\n",
      "\n",
      "ðŸš¨ ENVIRONMENT CHANGE ðŸš¨\n",
      "Changing pole length from 3.0m to 0.5m\n",
      "Environment parameters updated: masscart=1.0, length=0.5, gravity=9.8\n",
      "Next change scheduled at: 09:17:56\n",
      "ðŸ§  Updating reward function to adapt to new length...\n",
      "\n",
      "Generating new stability reward function...\n",
      "Update count: 1/3\n",
      "\n",
      "Proposed Function:\n",
      "Here's a slightly improved version of the reward function with detailed inline comments:\n",
      "\n",
      "```python\n",
      "def reward_function(observation, action):\n",
      "    # Extract relevant state variables from observation\n",
      "    cart_position = observation[0]\n",
      "    cart_velocity = observation[1]\n",
      "    pole_angle = observation[2]\n",
      "    \n",
      "    # Primary reward component: Pole angle stability\n",
      "    # Use cosine to create a smooth, symmetric reward centered at 0 (upright position)\n",
      "    # Scaled to range 0 to 2, then shifted to range -1 to 1\n",
      "    angle_stability = 2 * (math.cos(pole_angle) - 0.5)\n",
      "    \n",
      "    # Secondary penalty: Cart position\n",
      "    # Penalize cart for moving too far from center (0)\n",
      "    # Use a quadratic penalty to increase cost as cart moves further out\n",
      "    position_penalty = -0.2 * cart_position**2\n",
      "    \n",
      "    # Additional minor penalty: Cart velocity\n",
      "    # Slightly penalize high velocities to encourage smoother motion\n",
      "    velocity_penalty = -0.1 * cart_velocity**2\n",
      "    \n",
      "    # Combine components with appropriate weights\n",
      "    # Angle stability has highest positive weight (2.5) as the primary objective\n",
      "    # Position and velocity penalties have smaller negative weights as secondary considerations\n",
      "    # Total penalty weight (-0.3) is less than 50% of stability weight (2.5)\n",
      "    total_reward = 2.5 * angle_stability + position_penalty + velocity_penalty\n",
      "    \n",
      "    # Ensure the reward is positive when the pole is close to upright, even if the cart is off-center\n",
      "    total_reward = max(total_reward, 0.1)\n",
      "    \n",
      "    return total_reward\n",
      "```\n",
      "\n",
      "This function maintains the core structure and priorities of the original while making small improvements:\n",
      "\n",
      "1. Increased the weight of the angle stability reward from 2.0 to 2.5 to further emphasize pole balance.\n",
      "2. Slightly increased the position penalty weight from -0.1 to -0.2 to encourage keeping the cart more centered.\n",
      "3. Added a small velocity penalty (-0.1 * cart_velocity**2) to discourage excessive cart movement.\n",
      "4. The sum of penalty weights (-0.3) remains less than 50% of the stability reward weight (2.5).\n",
      "5. Maintained the positive reward floor of 0.1 for near-upright positions.\n",
      "\n",
      "These changes should provide a small improvement in stability while adhering to the given constraints and guidelines.\n",
      "\n",
      "Generating new efficiency reward function...\n",
      "Update count: 1/3\n",
      "\n",
      "Proposed Function:\n",
      "Here's a slightly modified version of the reward function with detailed inline comments, focusing on efficiency aspects while making small improvements:\n",
      "\n",
      "```python\n",
      "def reward_function(observation, action):\n",
      "    # Extract state variables from observation\n",
      "    cart_position, cart_velocity, pole_angle, pole_angular_velocity = observation\n",
      "\n",
      "    # Primary reward component: Pole angle stability (highest positive weight)\n",
      "    # Using 2.0 as weight, which is within the recommended range of 1.5-3.0\n",
      "    # Efficiency: Using 1 - abs() instead of cos() for a simpler calculation\n",
      "    angle_stability_reward = 2.0 * (1 - abs(pole_angle))\n",
      "\n",
      "    # Secondary penalty: Cart position (smaller negative weight)\n",
      "    # Using -0.2 as weight, which is within the recommended range of -0.1 to -0.5\n",
      "    # Efficiency: Directly multiplying by -0.2 instead of using a separate variable\n",
      "    position_penalty = -0.2 * abs(cart_position)\n",
      "\n",
      "    # Secondary penalty: Cart velocity (smaller negative weight)\n",
      "    # Using -0.1 as weight, which is within the recommended range of -0.1 to -0.3\n",
      "    # Efficiency: Directly multiplying by -0.1 instead of using a separate variable\n",
      "    velocity_penalty = -0.1 * abs(cart_velocity)\n",
      "\n",
      "    # Combine rewards and penalties\n",
      "    # Efficiency: Calculating total_reward in one line\n",
      "    total_reward = angle_stability_reward + position_penalty + velocity_penalty\n",
      "\n",
      "    # Ensure the reward is positive when the pole is upright, even if the cart is off-center\n",
      "    # Efficiency: Using max() function for a concise implementation\n",
      "    return max(total_reward, 0.1)\n",
      "```\n",
      "\n",
      "This modified function maintains the core structure and weights of the original function while making small efficiency improvements. The changes include:\n",
      "\n",
      "1. Unpacking the observation tuple directly in the function parameters for cleaner code.\n",
      "2. Removing unnecessary comments to reduce clutter.\n",
      "3. Combining the reward calculation and return statement for conciseness.\n",
      "4. Ensuring all weights are within the recommended ranges.\n",
      "5. Maintaining the positive reward for an upright pole, even with an off-center cart.\n",
      "\n",
      "The function adheres to all specified constraints and requirements, using only the observation and action inputs as specified.\n",
      "Episode completed: #156, Steps: 141, Length: 0.5m\n",
      "Episode completed: #157, Steps: 148, Length: 0.5m\n",
      "Episode completed: #158, Steps: 147, Length: 0.5m\n",
      "Episode completed: #159, Steps: 122, Length: 0.5m\n",
      "Episode completed: #160, Steps: 108, Length: 0.5m\n",
      "Episode completed: #161, Steps: 106, Length: 0.5m\n",
      "Episode completed: #162, Steps: 110, Length: 0.5m\n",
      "Episode completed: #163, Steps: 97, Length: 0.5m\n",
      "Episode completed: #164, Steps: 113, Length: 0.5m\n",
      "Episode completed: #165, Steps: 144, Length: 0.5m\n",
      "Episode completed: #166, Steps: 127, Length: 0.5m\n",
      "Episode completed: #167, Steps: 115, Length: 0.5m\n",
      "Episode completed: #168, Steps: 99, Length: 0.5m\n",
      "Episode completed: #169, Steps: 120, Length: 0.5m\n",
      "Episode completed: #170, Steps: 97, Length: 0.5m\n",
      "Episode completed: #171, Steps: 107, Length: 0.5m\n",
      "Episode completed: #172, Steps: 93, Length: 0.5m\n",
      "Episode completed: #173, Steps: 71, Length: 0.5m\n",
      "Episode completed: #174, Steps: 87, Length: 0.5m\n",
      "Episode completed: #175, Steps: 93, Length: 0.5m\n",
      "Episode completed: #176, Steps: 68, Length: 0.5m\n",
      "Episode completed: #177, Steps: 61, Length: 0.5m\n",
      "Episode completed: #178, Steps: 69, Length: 0.5m\n",
      "Episode completed: #179, Steps: 71, Length: 0.5m\n",
      "Episode completed: #180, Steps: 70, Length: 0.5m\n",
      "Episode completed: #181, Steps: 77, Length: 0.5m\n",
      "Episode completed: #182, Steps: 91, Length: 0.5m\n",
      "Episode completed: #183, Steps: 87, Length: 0.5m\n",
      "Episode completed: #184, Steps: 107, Length: 0.5m\n",
      "Episode completed: #185, Steps: 103, Length: 0.5m\n",
      "Episode completed: #186, Steps: 135, Length: 0.5m\n",
      "Episode completed: #187, Steps: 119, Length: 0.5m\n",
      "Episode completed: #188, Steps: 140, Length: 0.5m\n",
      "Episode completed: #189, Steps: 176, Length: 0.5m\n",
      "Episode completed: #190, Steps: 150, Length: 0.5m\n",
      "Episode completed: #191, Steps: 161, Length: 0.5m\n",
      "Episode completed: #192, Steps: 154, Length: 0.5m\n",
      "Episode completed: #193, Steps: 231, Length: 0.5m\n",
      "Episode completed: #194, Steps: 130, Length: 0.5m\n",
      "\n",
      "ðŸš¨ ENVIRONMENT CHANGE ðŸš¨\n",
      "Changing pole length from 0.5m to 3.0m\n",
      "Environment parameters updated: masscart=1.0, length=3.0, gravity=9.8\n",
      "Next change scheduled at: 09:19:56\n",
      "ðŸ§  Updating reward function to adapt to new length...\n",
      "\n",
      "Generating new stability reward function...\n",
      "Update count: 2/3\n",
      "\n",
      "Proposed Function:\n",
      "Here's a slightly modified version of the reward function with small improvements and detailed inline comments:\n",
      "\n",
      "```python\n",
      "def reward_function(observation, action):\n",
      "    # Extract relevant state variables from observation\n",
      "    cart_position = observation[0]\n",
      "    cart_velocity = observation[1]\n",
      "    pole_angle = observation[2]\n",
      "    pole_angular_velocity = observation[3]  # Added for potential future use\n",
      "    \n",
      "    # Primary reward component: Pole angle stability\n",
      "    # Use cosine to create a smooth, symmetric reward centered at 0 (upright position)\n",
      "    # Scaled to range 0 to 2, then shifted to range -1 to 1\n",
      "    angle_stability = 2 * (math.cos(pole_angle) - 0.5)\n",
      "    \n",
      "    # Secondary penalty: Cart position\n",
      "    # Penalize cart for moving too far from center (0)\n",
      "    # Use a quadratic penalty to increase cost as cart moves further out\n",
      "    # Reduced weight slightly to -0.15 to decrease its impact\n",
      "    position_penalty = -0.15 * cart_position**2\n",
      "    \n",
      "    # Additional minor penalty: Cart velocity\n",
      "    # Slightly penalize high velocities to encourage smoother motion\n",
      "    # Keep the same weight as it's already small\n",
      "    velocity_penalty = -0.1 * cart_velocity**2\n",
      "    \n",
      "    # Combine components with appropriate weights\n",
      "    # Angle stability has highest positive weight (2.7) as the primary objective\n",
      "    # Slightly increased from 2.5 to emphasize stability even more\n",
      "    # Position and velocity penalties have smaller negative weights as secondary considerations\n",
      "    # Total penalty weight (-0.25) is still less than 50% of stability weight (2.7)\n",
      "    total_reward = 2.7 * angle_stability + position_penalty + velocity_penalty\n",
      "    \n",
      "    # Ensure the reward is positive when the pole is close to upright, even if the cart is off-center\n",
      "    # Increased the minimum reward slightly to 0.15 to provide more positive feedback\n",
      "    total_reward = max(total_reward, 0.15)\n",
      "    \n",
      "    return total_reward\n",
      "```\n",
      "\n",
      "This modified version makes the following small improvements:\n",
      "\n",
      "1. Slightly increased the weight of the angle stability component from 2.5 to 2.7 to further emphasize the importance of keeping the pole upright.\n",
      "2. Reduced the weight of the position penalty from -0.2 to -0.15 to slightly decrease its impact on the overall reward.\n",
      "3. Increased the minimum reward from 0.1 to 0.15 to provide more positive feedback when the pole is close to upright.\n",
      "4. Added pole_angular_velocity to the extracted variables for potential future use, although it's not currently used in the reward calculation.\n",
      "\n",
      "These changes maintain the overall structure and priorities of the original function while making small adjustments to potentially improve performance. The function still adheres to all the specified constraints and requirements.\n",
      "\n",
      "Generating new efficiency reward function...\n",
      "Update count: 2/3\n",
      "\n",
      "Proposed Function:\n",
      "Here's a slightly improved version of the reward function with a focus on efficiency and detailed inline comments:\n",
      "\n",
      "```python\n",
      "def reward_function(observation, action):\n",
      "    # Extract state variables from observation\n",
      "    cart_position, cart_velocity, pole_angle, pole_angular_velocity = observation\n",
      "\n",
      "    # Primary reward component: Pole angle stability (highest positive weight)\n",
      "    # Weight: 2.0 (within recommended range 1.5-3.0)\n",
      "    # Efficiency: Using 1 - abs() for faster computation than cos()\n",
      "    angle_stability_reward = 2.0 * (1 - abs(pole_angle))\n",
      "\n",
      "    # Secondary penalty: Cart position (smaller negative weight)\n",
      "    # Weight: -0.2 (within recommended range -0.1 to -0.5)\n",
      "    # Efficiency: Combining position and velocity penalties in one calculation\n",
      "    combined_penalty = -0.2 * abs(cart_position) - 0.1 * abs(cart_velocity)\n",
      "\n",
      "    # Combine rewards and penalties\n",
      "    # Efficiency: Calculating total_reward in one line, reducing variable usage\n",
      "    total_reward = angle_stability_reward + combined_penalty\n",
      "\n",
      "    # Ensure the reward is positive when the pole is upright, even if the cart is off-center\n",
      "    # Efficiency: Using max() function for a concise implementation\n",
      "    # Slight adjustment: Increased minimum reward to 0.2 for better learning signal\n",
      "    return max(total_reward, 0.2)\n",
      "```\n",
      "\n",
      "This version makes the following small improvements:\n",
      "1. Combines the position and velocity penalties into a single calculation, reducing the number of operations.\n",
      "2. Removes the separate `velocity_penalty` variable, streamlining the code.\n",
      "3. Slightly increases the minimum reward from 0.1 to 0.2, potentially providing a better learning signal while still keeping the reward positive when the pole is upright.\n",
      "4. Maintains the use of only `observation` and `action` as inputs, adhering to the constraints.\n",
      "5. Keeps the core logic and weights the same, focusing on minor efficiency improvements.\n",
      "\n",
      "These changes aim to slightly optimize the function while maintaining its core functionality and adhering to all specified constraints and priorities.\n",
      "Episode completed: #195, Steps: 146, Length: 3.0m\n",
      "Episode completed: #196, Steps: 156, Length: 3.0m\n",
      "Episode completed: #197, Steps: 149, Length: 3.0m\n",
      "Episode completed: #198, Steps: 106, Length: 3.0m\n",
      "Episode completed: #199, Steps: 107, Length: 3.0m\n",
      "Episode completed: #200, Steps: 92, Length: 3.0m\n",
      "Episode completed: #201, Steps: 89, Length: 3.0m\n",
      "Episode completed: #202, Steps: 85, Length: 3.0m\n",
      "Episode completed: #203, Steps: 89, Length: 3.0m\n",
      "Episode completed: #204, Steps: 92, Length: 3.0m\n",
      "Episode completed: #205, Steps: 92, Length: 3.0m\n",
      "Episode completed: #206, Steps: 97, Length: 3.0m\n",
      "Episode completed: #207, Steps: 79, Length: 3.0m\n",
      "Episode completed: #208, Steps: 75, Length: 3.0m\n",
      "Episode completed: #209, Steps: 79, Length: 3.0m\n",
      "Episode completed: #210, Steps: 57, Length: 3.0m\n",
      "Episode completed: #211, Steps: 47, Length: 3.0m\n",
      "Episode completed: #212, Steps: 66, Length: 3.0m\n",
      "Episode completed: #213, Steps: 55, Length: 3.0m\n",
      "Episode completed: #214, Steps: 68, Length: 3.0m\n",
      "Episode completed: #215, Steps: 49, Length: 3.0m\n",
      "Episode completed: #216, Steps: 91, Length: 3.0m\n",
      "Episode completed: #217, Steps: 81, Length: 3.0m\n",
      "Episode completed: #218, Steps: 81, Length: 3.0m\n",
      "Episode completed: #219, Steps: 128, Length: 3.0m\n",
      "Episode completed: #220, Steps: 128, Length: 3.0m\n",
      "Episode completed: #221, Steps: 161, Length: 3.0m\n",
      "Episode completed: #222, Steps: 208, Length: 3.0m\n",
      "Episode completed: #223, Steps: 153, Length: 3.0m\n",
      "Episode completed: #224, Steps: 124, Length: 3.0m\n",
      "Episode completed: #225, Steps: 161, Length: 3.0m\n",
      "Episode completed: #226, Steps: 117, Length: 3.0m\n",
      "Episode completed: #227, Steps: 113, Length: 3.0m\n",
      "Episode completed: #228, Steps: 156, Length: 3.0m\n",
      "Episode completed: #229, Steps: 116, Length: 3.0m\n",
      "Episode completed: #230, Steps: 87, Length: 3.0m\n",
      "Episode completed: #231, Steps: 79, Length: 3.0m\n",
      "Episode completed: #232, Steps: 62, Length: 3.0m\n",
      "Episode completed: #233, Steps: 67, Length: 3.0m\n",
      "Episode completed: #234, Steps: 55, Length: 3.0m\n",
      "Episode completed: #235, Steps: 60, Length: 3.0m\n",
      "Episode completed: #236, Steps: 49, Length: 3.0m\n",
      "Episode completed: #237, Steps: 49, Length: 3.0m\n",
      "Episode completed: #238, Steps: 53, Length: 3.0m\n",
      "Episode completed: #239, Steps: 50, Length: 3.0m\n",
      "Episode completed: #240, Steps: 43, Length: 3.0m\n",
      "Episode completed: #241, Steps: 59, Length: 3.0m\n",
      "Episode completed: #242, Steps: 65, Length: 3.0m\n",
      "\n",
      "ðŸš¨ ENVIRONMENT CHANGE ðŸš¨\n",
      "Changing pole length from 3.0m to 0.5m\n",
      "Environment parameters updated: masscart=1.0, length=0.5, gravity=9.8\n",
      "Next change scheduled at: 09:21:56\n",
      "ðŸ§  Updating reward function to adapt to new length...\n",
      "\n",
      "Reached maximum updates (3) for component 1. Skipping update.\n",
      "\n",
      "Reached maximum updates (3) for component 2. Skipping update.\n",
      "Episode completed: #243, Steps: 68, Length: 0.5m\n",
      "Episode completed: #244, Steps: 71, Length: 0.5m\n",
      "Episode completed: #245, Steps: 45, Length: 0.5m\n",
      "Episode completed: #246, Steps: 55, Length: 0.5m\n",
      "Episode completed: #247, Steps: 60, Length: 0.5m\n",
      "Episode completed: #248, Steps: 50, Length: 0.5m\n",
      "Episode completed: #249, Steps: 45, Length: 0.5m\n",
      "Episode completed: #250, Steps: 61, Length: 0.5m\n",
      "Episode completed: #251, Steps: 45, Length: 0.5m\n",
      "Episode completed: #252, Steps: 54, Length: 0.5m\n",
      "Episode completed: #253, Steps: 46, Length: 0.5m\n",
      "Episode completed: #254, Steps: 51, Length: 0.5m\n",
      "Episode completed: #255, Steps: 49, Length: 0.5m\n",
      "Episode completed: #256, Steps: 50, Length: 0.5m\n",
      "Episode completed: #257, Steps: 39, Length: 0.5m\n",
      "Episode completed: #258, Steps: 44, Length: 0.5m\n",
      "Episode completed: #259, Steps: 36, Length: 0.5m\n",
      "Episode completed: #260, Steps: 45, Length: 0.5m\n",
      "Episode completed: #261, Steps: 47, Length: 0.5m\n",
      "Episode completed: #262, Steps: 47, Length: 0.5m\n",
      "Episode completed: #263, Steps: 40, Length: 0.5m\n",
      "Episode completed: #264, Steps: 49, Length: 0.5m\n",
      "Episode completed: #265, Steps: 61, Length: 0.5m\n",
      "Episode completed: #266, Steps: 53, Length: 0.5m\n",
      "Episode completed: #267, Steps: 58, Length: 0.5m\n",
      "Episode completed: #268, Steps: 52, Length: 0.5m\n",
      "Episode completed: #269, Steps: 58, Length: 0.5m\n",
      "Episode completed: #270, Steps: 50, Length: 0.5m\n",
      "Episode completed: #271, Steps: 60, Length: 0.5m\n",
      "Episode completed: #272, Steps: 61, Length: 0.5m\n",
      "Episode completed: #273, Steps: 50, Length: 0.5m\n",
      "Episode completed: #274, Steps: 56, Length: 0.5m\n",
      "Episode completed: #275, Steps: 53, Length: 0.5m\n",
      "Episode completed: #276, Steps: 74, Length: 0.5m\n",
      "Episode completed: #277, Steps: 68, Length: 0.5m\n",
      "Episode completed: #278, Steps: 82, Length: 0.5m\n",
      "Episode completed: #279, Steps: 109, Length: 0.5m\n",
      "Episode completed: #280, Steps: 62, Length: 0.5m\n",
      "Episode completed: #281, Steps: 67, Length: 0.5m\n",
      "Episode completed: #282, Steps: 79, Length: 0.5m\n",
      "Episode completed: #283, Steps: 67, Length: 0.5m\n",
      "Episode completed: #284, Steps: 78, Length: 0.5m\n",
      "Episode completed: #285, Steps: 86, Length: 0.5m\n",
      "Episode completed: #286, Steps: 108, Length: 0.5m\n",
      "Episode completed: #287, Steps: 180, Length: 0.5m\n",
      "Episode completed: #288, Steps: 185, Length: 0.5m\n",
      "Episode completed: #289, Steps: 89, Length: 0.5m\n",
      "Episode completed: #290, Steps: 70, Length: 0.5m\n",
      "Episode completed: #291, Steps: 78, Length: 0.5m\n",
      "Episode completed: #292, Steps: 202, Length: 0.5m\n",
      "Episode completed: #293, Steps: 68, Length: 0.5m\n",
      "Episode completed: #294, Steps: 51, Length: 0.5m\n",
      "Episode completed: #295, Steps: 62, Length: 0.5m\n",
      "Episode completed: #296, Steps: 46, Length: 0.5m\n",
      "Episode completed: #297, Steps: 55, Length: 0.5m\n",
      "Episode completed: #298, Steps: 41, Length: 0.5m\n",
      "Episode completed: #299, Steps: 61, Length: 0.5m\n",
      "Episode completed: #300, Steps: 45, Length: 0.5m\n",
      "Episode completed: #301, Steps: 49, Length: 0.5m\n",
      "Episode completed: #302, Steps: 50, Length: 0.5m\n",
      "Episode completed: #303, Steps: 57, Length: 0.5m\n",
      "Episode completed: #304, Steps: 55, Length: 0.5m\n",
      "Episode completed: #305, Steps: 50, Length: 0.5m\n",
      "Episode completed: #306, Steps: 53, Length: 0.5m\n",
      "Episode completed: #307, Steps: 41, Length: 0.5m\n",
      "Episode completed: #308, Steps: 35, Length: 0.5m\n",
      "Episode completed: #309, Steps: 34, Length: 0.5m\n",
      "Episode completed: #310, Steps: 40, Length: 0.5m\n",
      "Episode completed: #311, Steps: 42, Length: 0.5m\n",
      "Episode completed: #312, Steps: 34, Length: 0.5m\n",
      "Episode completed: #313, Steps: 37, Length: 0.5m\n",
      "Episode completed: #314, Steps: 45, Length: 0.5m\n",
      "Episode completed: #315, Steps: 50, Length: 0.5m\n",
      "Episode completed: #316, Steps: 46, Length: 0.5m\n",
      "Episode completed: #317, Steps: 61, Length: 0.5m\n",
      "Episode completed: #318, Steps: 66, Length: 0.5m\n",
      "Episode completed: #319, Steps: 73, Length: 0.5m\n",
      "Episode completed: #320, Steps: 106, Length: 0.5m\n",
      "Episode completed: #321, Steps: 69, Length: 0.5m\n",
      "Episode completed: #322, Steps: 100, Length: 0.5m\n",
      "Episode completed: #323, Steps: 60, Length: 0.5m\n",
      "Episode completed: #324, Steps: 69, Length: 0.5m\n",
      "Episode completed: #325, Steps: 60, Length: 0.5m\n",
      "Episode completed: #326, Steps: 68, Length: 0.5m\n",
      "Episode completed: #327, Steps: 54, Length: 0.5m\n",
      "Episode completed: #328, Steps: 55, Length: 0.5m\n",
      "Episode completed: #329, Steps: 76, Length: 0.5m\n",
      "Episode completed: #330, Steps: 58, Length: 0.5m\n",
      "Episode completed: #331, Steps: 78, Length: 0.5m\n",
      "Episode completed: #332, Steps: 47, Length: 0.5m\n",
      "Episode completed: #333, Steps: 40, Length: 0.5m\n",
      "Episode completed: #334, Steps: 52, Length: 0.5m\n",
      "Episode completed: #335, Steps: 68, Length: 0.5m\n",
      "Episode completed: #336, Steps: 65, Length: 0.5m\n",
      "\n",
      "ðŸš¨ ENVIRONMENT CHANGE ðŸš¨\n",
      "Changing pole length from 0.5m to 1.5m\n",
      "Environment parameters updated: masscart=1.0, length=1.5, gravity=9.8\n",
      "Next change scheduled at: 09:23:56\n",
      "ðŸ§  Updating reward function to adapt to new length...\n",
      "\n",
      "Reached maximum updates (3) for component 1. Skipping update.\n",
      "\n",
      "Reached maximum updates (3) for component 2. Skipping update.\n",
      "Episode completed: #337, Steps: 48, Length: 1.5m\n",
      "Episode completed: #338, Steps: 74, Length: 1.5m\n",
      "Episode completed: #339, Steps: 40, Length: 1.5m\n",
      "Episode completed: #340, Steps: 46, Length: 1.5m\n",
      "Episode completed: #341, Steps: 34, Length: 1.5m\n",
      "Episode completed: #342, Steps: 54, Length: 1.5m\n",
      "Episode completed: #343, Steps: 38, Length: 1.5m\n",
      "Episode completed: #344, Steps: 32, Length: 1.5m\n",
      "Episode completed: #345, Steps: 49, Length: 1.5m\n",
      "Episode completed: #346, Steps: 36, Length: 1.5m\n",
      "Episode completed: #347, Steps: 45, Length: 1.5m\n",
      "Episode completed: #348, Steps: 57, Length: 1.5m\n",
      "Episode completed: #349, Steps: 62, Length: 1.5m\n",
      "Episode completed: #350, Steps: 63, Length: 1.5m\n",
      "Episode completed: #351, Steps: 46, Length: 1.5m\n",
      "Episode completed: #352, Steps: 94, Length: 1.5m\n",
      "Episode completed: #353, Steps: 51, Length: 1.5m\n",
      "Episode completed: #354, Steps: 69, Length: 1.5m\n",
      "Episode completed: #355, Steps: 50, Length: 1.5m\n",
      "Episode completed: #356, Steps: 66, Length: 1.5m\n",
      "Episode completed: #357, Steps: 173, Length: 1.5m\n",
      "Episode completed: #358, Steps: 84, Length: 1.5m\n",
      "Episode completed: #359, Steps: 75, Length: 1.5m\n",
      "Episode completed: #360, Steps: 238, Length: 1.5m\n",
      "Episode completed: #361, Steps: 55, Length: 1.5m\n",
      "Episode completed: #362, Steps: 57, Length: 1.5m\n",
      "Episode completed: #363, Steps: 54, Length: 1.5m\n",
      "Episode completed: #364, Steps: 42, Length: 1.5m\n",
      "Episode completed: #365, Steps: 62, Length: 1.5m\n",
      "Episode completed: #366, Steps: 49, Length: 1.5m\n",
      "Episode completed: #367, Steps: 37, Length: 1.5m\n",
      "Episode completed: #368, Steps: 64, Length: 1.5m\n",
      "Episode completed: #369, Steps: 47, Length: 1.5m\n",
      "Episode completed: #370, Steps: 66, Length: 1.5m\n",
      "Episode completed: #371, Steps: 62, Length: 1.5m\n",
      "Episode completed: #372, Steps: 69, Length: 1.5m\n",
      "Episode completed: #373, Steps: 49, Length: 1.5m\n",
      "Episode completed: #374, Steps: 42, Length: 1.5m\n",
      "Episode completed: #375, Steps: 38, Length: 1.5m\n",
      "Episode completed: #376, Steps: 51, Length: 1.5m\n",
      "Episode completed: #377, Steps: 46, Length: 1.5m\n",
      "Episode completed: #378, Steps: 46, Length: 1.5m\n",
      "Episode completed: #379, Steps: 51, Length: 1.5m\n",
      "Episode completed: #380, Steps: 51, Length: 1.5m\n",
      "Episode completed: #381, Steps: 49, Length: 1.5m\n",
      "Episode completed: #382, Steps: 59, Length: 1.5m\n",
      "Episode completed: #383, Steps: 78, Length: 1.5m\n",
      "Episode completed: #384, Steps: 82, Length: 1.5m\n",
      "Episode completed: #385, Steps: 62, Length: 1.5m\n",
      "Episode completed: #386, Steps: 89, Length: 1.5m\n",
      "Episode completed: #387, Steps: 100, Length: 1.5m\n",
      "Episode completed: #388, Steps: 59, Length: 1.5m\n",
      "Episode completed: #389, Steps: 181, Length: 1.5m\n",
      "Episode completed: #390, Steps: 59, Length: 1.5m\n",
      "Episode completed: #391, Steps: 176, Length: 1.5m\n",
      "Episode completed: #392, Steps: 45, Length: 1.5m\n",
      "Episode completed: #393, Steps: 73, Length: 1.5m\n",
      "Episode completed: #394, Steps: 72, Length: 1.5m\n",
      "Episode completed: #395, Steps: 44, Length: 1.5m\n",
      "Episode completed: #396, Steps: 84, Length: 1.5m\n",
      "Episode completed: #397, Steps: 60, Length: 1.5m\n",
      "Episode completed: #398, Steps: 72, Length: 1.5m\n",
      "Episode completed: #399, Steps: 64, Length: 1.5m\n",
      "Episode completed: #400, Steps: 38, Length: 1.5m\n",
      "Episode completed: #401, Steps: 32, Length: 1.5m\n",
      "Episode completed: #402, Steps: 45, Length: 1.5m\n",
      "Episode completed: #403, Steps: 56, Length: 1.5m\n",
      "Episode completed: #404, Steps: 36, Length: 1.5m\n",
      "Episode completed: #405, Steps: 55, Length: 1.5m\n",
      "Episode completed: #406, Steps: 37, Length: 1.5m\n",
      "Episode completed: #407, Steps: 45, Length: 1.5m\n",
      "Episode completed: #408, Steps: 35, Length: 1.5m\n",
      "Episode completed: #409, Steps: 43, Length: 1.5m\n",
      "Episode completed: #410, Steps: 40, Length: 1.5m\n",
      "Episode completed: #411, Steps: 32, Length: 1.5m\n",
      "Episode completed: #412, Steps: 41, Length: 1.5m\n",
      "Episode completed: #413, Steps: 34, Length: 1.5m\n",
      "Episode completed: #414, Steps: 45, Length: 1.5m\n",
      "Episode completed: #415, Steps: 29, Length: 1.5m\n",
      "Episode completed: #416, Steps: 57, Length: 1.5m\n",
      "Episode completed: #417, Steps: 33, Length: 1.5m\n",
      "Episode completed: #418, Steps: 45, Length: 1.5m\n",
      "Episode completed: #419, Steps: 34, Length: 1.5m\n",
      "Episode completed: #420, Steps: 35, Length: 1.5m\n",
      "Episode completed: #421, Steps: 64, Length: 1.5m\n",
      "Episode completed: #422, Steps: 49, Length: 1.5m\n",
      "Episode completed: #423, Steps: 46, Length: 1.5m\n",
      "Episode completed: #424, Steps: 48, Length: 1.5m\n",
      "Episode completed: #425, Steps: 48, Length: 1.5m\n",
      "Episode completed: #426, Steps: 112, Length: 1.5m\n",
      "Episode completed: #427, Steps: 296, Length: 1.5m\n",
      "Episode completed: #428, Steps: 74, Length: 1.5m\n",
      "Episode completed: #429, Steps: 63, Length: 1.5m\n",
      "Episode completed: #430, Steps: 48, Length: 1.5m\n",
      "\n",
      "ðŸš¨ ENVIRONMENT CHANGE ðŸš¨\n",
      "Changing pole length from 1.5m to 3.0m\n",
      "Environment parameters updated: masscart=1.0, length=3.0, gravity=9.8\n",
      "Next change scheduled at: 09:25:56\n",
      "ðŸ§  Updating reward function to adapt to new length...\n",
      "\n",
      "Reached maximum updates (3) for component 1. Skipping update.\n",
      "\n",
      "Reached maximum updates (3) for component 2. Skipping update.\n",
      "Episode completed: #431, Steps: 44, Length: 3.0m\n",
      "Episode completed: #432, Steps: 80, Length: 3.0m\n",
      "Episode completed: #433, Steps: 56, Length: 3.0m\n",
      "Episode completed: #434, Steps: 111, Length: 3.0m\n",
      "Episode completed: #435, Steps: 36, Length: 3.0m\n",
      "Episode completed: #436, Steps: 59, Length: 3.0m\n",
      "Episode completed: #437, Steps: 40, Length: 3.0m\n",
      "Episode completed: #438, Steps: 57, Length: 3.0m\n",
      "Episode completed: #439, Steps: 44, Length: 3.0m\n",
      "Episode completed: #440, Steps: 51, Length: 3.0m\n",
      "Episode completed: #441, Steps: 46, Length: 3.0m\n",
      "Episode completed: #442, Steps: 46, Length: 3.0m\n",
      "Episode completed: #443, Steps: 63, Length: 3.0m\n",
      "Episode completed: #444, Steps: 64, Length: 3.0m\n",
      "Episode completed: #445, Steps: 62, Length: 3.0m\n",
      "Episode completed: #446, Steps: 61, Length: 3.0m\n",
      "Episode completed: #447, Steps: 37, Length: 3.0m\n",
      "Episode completed: #448, Steps: 70, Length: 3.0m\n",
      "Episode completed: #449, Steps: 57, Length: 3.0m\n",
      "Episode completed: #450, Steps: 36, Length: 3.0m\n",
      "Episode completed: #451, Steps: 33, Length: 3.0m\n",
      "Episode completed: #452, Steps: 39, Length: 3.0m\n",
      "Episode completed: #453, Steps: 35, Length: 3.0m\n",
      "Episode completed: #454, Steps: 29, Length: 3.0m\n",
      "Episode completed: #455, Steps: 33, Length: 3.0m\n",
      "Episode completed: #456, Steps: 41, Length: 3.0m\n",
      "Episode completed: #457, Steps: 39, Length: 3.0m\n",
      "Episode completed: #458, Steps: 36, Length: 3.0m\n",
      "Episode completed: #459, Steps: 59, Length: 3.0m\n",
      "Episode completed: #460, Steps: 38, Length: 3.0m\n",
      "Episode completed: #461, Steps: 43, Length: 3.0m\n",
      "Episode completed: #462, Steps: 43, Length: 3.0m\n",
      "Episode completed: #463, Steps: 33, Length: 3.0m\n",
      "Episode completed: #464, Steps: 40, Length: 3.0m\n",
      "Episode completed: #465, Steps: 56, Length: 3.0m\n",
      "Episode completed: #466, Steps: 66, Length: 3.0m\n",
      "Episode completed: #467, Steps: 35, Length: 3.0m\n",
      "Episode completed: #468, Steps: 57, Length: 3.0m\n",
      "Episode completed: #469, Steps: 86, Length: 3.0m\n",
      "Episode completed: #470, Steps: 37, Length: 3.0m\n",
      "Episode completed: #471, Steps: 45, Length: 3.0m\n",
      "Episode completed: #472, Steps: 42, Length: 3.0m\n",
      "Episode completed: #473, Steps: 65, Length: 3.0m\n",
      "Episode completed: #474, Steps: 34, Length: 3.0m\n",
      "Episode completed: #475, Steps: 56, Length: 3.0m\n",
      "Episode completed: #476, Steps: 44, Length: 3.0m\n",
      "Episode completed: #477, Steps: 50, Length: 3.0m\n",
      "Episode completed: #478, Steps: 77, Length: 3.0m\n",
      "Episode completed: #479, Steps: 118, Length: 3.0m\n",
      "Episode completed: #480, Steps: 35, Length: 3.0m\n",
      "Episode completed: #481, Steps: 38, Length: 3.0m\n",
      "Episode completed: #482, Steps: 52, Length: 3.0m\n",
      "Episode completed: #483, Steps: 48, Length: 3.0m\n",
      "Episode completed: #484, Steps: 60, Length: 3.0m\n",
      "Episode completed: #485, Steps: 61, Length: 3.0m\n",
      "Episode completed: #486, Steps: 58, Length: 3.0m\n",
      "Episode completed: #487, Steps: 41, Length: 3.0m\n",
      "Episode completed: #488, Steps: 50, Length: 3.0m\n",
      "Episode completed: #489, Steps: 46, Length: 3.0m\n",
      "Episode completed: #490, Steps: 39, Length: 3.0m\n",
      "Episode completed: #491, Steps: 52, Length: 3.0m\n",
      "Episode completed: #492, Steps: 66, Length: 3.0m\n",
      "Episode completed: #493, Steps: 55, Length: 3.0m\n",
      "Episode completed: #494, Steps: 41, Length: 3.0m\n",
      "Episode completed: #495, Steps: 42, Length: 3.0m\n",
      "Episode completed: #496, Steps: 44, Length: 3.0m\n",
      "Episode completed: #497, Steps: 69, Length: 3.0m\n",
      "Episode completed: #498, Steps: 52, Length: 3.0m\n",
      "Episode completed: #499, Steps: 85, Length: 3.0m\n",
      "Episode completed: #500, Steps: 52, Length: 3.0m\n",
      "Episode completed: #501, Steps: 49, Length: 3.0m\n",
      "Episode completed: #502, Steps: 57, Length: 3.0m\n",
      "Episode completed: #503, Steps: 35, Length: 3.0m\n",
      "Episode completed: #504, Steps: 53, Length: 3.0m\n",
      "Episode completed: #505, Steps: 29, Length: 3.0m\n",
      "Episode completed: #506, Steps: 36, Length: 3.0m\n",
      "Episode completed: #507, Steps: 53, Length: 3.0m\n",
      "Episode completed: #508, Steps: 45, Length: 3.0m\n",
      "Episode completed: #509, Steps: 53, Length: 3.0m\n",
      "Episode completed: #510, Steps: 44, Length: 3.0m\n",
      "Episode completed: #511, Steps: 47, Length: 3.0m\n",
      "Episode completed: #512, Steps: 77, Length: 3.0m\n",
      "Episode completed: #513, Steps: 51, Length: 3.0m\n",
      "Episode completed: #514, Steps: 50, Length: 3.0m\n",
      "Episode completed: #515, Steps: 63, Length: 3.0m\n",
      "Episode completed: #516, Steps: 63, Length: 3.0m\n",
      "Episode completed: #517, Steps: 53, Length: 3.0m\n",
      "Episode completed: #518, Steps: 36, Length: 3.0m\n",
      "Episode completed: #519, Steps: 50, Length: 3.0m\n",
      "Episode completed: #520, Steps: 62, Length: 3.0m\n",
      "Episode completed: #521, Steps: 49, Length: 3.0m\n",
      "Episode completed: #522, Steps: 48, Length: 3.0m\n",
      "Episode completed: #523, Steps: 35, Length: 3.0m\n",
      "Episode completed: #524, Steps: 46, Length: 3.0m\n",
      "Episode completed: #525, Steps: 49, Length: 3.0m\n",
      "Episode completed: #526, Steps: 48, Length: 3.0m\n",
      "Episode completed: #527, Steps: 55, Length: 3.0m\n",
      "Episode completed: #528, Steps: 61, Length: 3.0m\n",
      "Episode completed: #529, Steps: 74, Length: 3.0m\n",
      "Episode completed: #530, Steps: 80, Length: 3.0m\n",
      "Episode completed: #531, Steps: 57, Length: 3.0m\n",
      "Episode completed: #532, Steps: 45, Length: 3.0m\n",
      "Episode completed: #533, Steps: 85, Length: 3.0m\n",
      "Episode completed: #534, Steps: 103, Length: 3.0m\n",
      "Episode completed: #535, Steps: 49, Length: 3.0m\n",
      "Episode completed: #536, Steps: 45, Length: 3.0m\n",
      "Episode completed: #537, Steps: 72, Length: 3.0m\n",
      "Episode completed: #538, Steps: 80, Length: 3.0m\n",
      "\n",
      "ðŸš¨ ENVIRONMENT CHANGE ðŸš¨\n",
      "Changing pole length from 3.0m to 0.5m\n",
      "Environment parameters updated: masscart=1.0, length=0.5, gravity=9.8\n",
      "Next change scheduled at: 09:27:56\n",
      "ðŸ§  Updating reward function to adapt to new length...\n",
      "\n",
      "Reached maximum updates (3) for component 1. Skipping update.\n",
      "\n",
      "Reached maximum updates (3) for component 2. Skipping update.\n",
      "Episode completed: #539, Steps: 48, Length: 0.5m\n",
      "Episode completed: #540, Steps: 42, Length: 0.5m\n",
      "Episode completed: #541, Steps: 48, Length: 0.5m\n",
      "Episode completed: #542, Steps: 39, Length: 0.5m\n",
      "Episode completed: #543, Steps: 42, Length: 0.5m\n",
      "Episode completed: #544, Steps: 50, Length: 0.5m\n",
      "Episode completed: #545, Steps: 48, Length: 0.5m\n",
      "Episode completed: #546, Steps: 92, Length: 0.5m\n",
      "Episode completed: #547, Steps: 42, Length: 0.5m\n",
      "Episode completed: #548, Steps: 47, Length: 0.5m\n",
      "Episode completed: #549, Steps: 49, Length: 0.5m\n",
      "Episode completed: #550, Steps: 48, Length: 0.5m\n",
      "Episode completed: #551, Steps: 43, Length: 0.5m\n",
      "Episode completed: #552, Steps: 80, Length: 0.5m\n",
      "Episode completed: #553, Steps: 49, Length: 0.5m\n",
      "Episode completed: #554, Steps: 65, Length: 0.5m\n",
      "Episode completed: #555, Steps: 59, Length: 0.5m\n",
      "Episode completed: #556, Steps: 35, Length: 0.5m\n",
      "Episode completed: #557, Steps: 63, Length: 0.5m\n",
      "Episode completed: #558, Steps: 55, Length: 0.5m\n",
      "Episode completed: #559, Steps: 42, Length: 0.5m\n",
      "Episode completed: #560, Steps: 32, Length: 0.5m\n",
      "Episode completed: #561, Steps: 34, Length: 0.5m\n",
      "Episode completed: #562, Steps: 40, Length: 0.5m\n",
      "Episode completed: #563, Steps: 32, Length: 0.5m\n",
      "Episode completed: #564, Steps: 41, Length: 0.5m\n",
      "Episode completed: #565, Steps: 39, Length: 0.5m\n",
      "Episode completed: #566, Steps: 42, Length: 0.5m\n",
      "Episode completed: #567, Steps: 31, Length: 0.5m\n",
      "Episode completed: #568, Steps: 39, Length: 0.5m\n",
      "Episode completed: #569, Steps: 42, Length: 0.5m\n",
      "Episode completed: #570, Steps: 46, Length: 0.5m\n",
      "Episode completed: #571, Steps: 32, Length: 0.5m\n",
      "Episode completed: #572, Steps: 42, Length: 0.5m\n",
      "Episode completed: #573, Steps: 40, Length: 0.5m\n",
      "Episode completed: #574, Steps: 35, Length: 0.5m\n",
      "Episode completed: #575, Steps: 41, Length: 0.5m\n",
      "Episode completed: #576, Steps: 42, Length: 0.5m\n",
      "Episode completed: #577, Steps: 46, Length: 0.5m\n",
      "Episode completed: #578, Steps: 46, Length: 0.5m\n",
      "Episode completed: #579, Steps: 54, Length: 0.5m\n",
      "Episode completed: #580, Steps: 57, Length: 0.5m\n",
      "Episode completed: #581, Steps: 45, Length: 0.5m\n",
      "Episode completed: #582, Steps: 37, Length: 0.5m\n",
      "Episode completed: #583, Steps: 43, Length: 0.5m\n",
      "Episode completed: #584, Steps: 41, Length: 0.5m\n",
      "Episode completed: #585, Steps: 77, Length: 0.5m\n",
      "Episode completed: #586, Steps: 143, Length: 0.5m\n",
      "Episode completed: #587, Steps: 49, Length: 0.5m\n",
      "Episode completed: #588, Steps: 37, Length: 0.5m\n",
      "Episode completed: #589, Steps: 164, Length: 0.5m\n",
      "Episode completed: #590, Steps: 37, Length: 0.5m\n",
      "Episode completed: #591, Steps: 45, Length: 0.5m\n",
      "Episode completed: #592, Steps: 69, Length: 0.5m\n",
      "Episode completed: #593, Steps: 132, Length: 0.5m\n",
      "Episode completed: #594, Steps: 43, Length: 0.5m\n",
      "Episode completed: #595, Steps: 68, Length: 0.5m\n",
      "Episode completed: #596, Steps: 47, Length: 0.5m\n",
      "Episode completed: #597, Steps: 38, Length: 0.5m\n",
      "Episode completed: #598, Steps: 66, Length: 0.5m\n",
      "Episode completed: #599, Steps: 44, Length: 0.5m\n",
      "Episode completed: #600, Steps: 145, Length: 0.5m\n",
      "Episode completed: #601, Steps: 49, Length: 0.5m\n",
      "Episode completed: #602, Steps: 49, Length: 0.5m\n",
      "Episode completed: #603, Steps: 32, Length: 0.5m\n",
      "Episode completed: #604, Steps: 65, Length: 0.5m\n",
      "Episode completed: #605, Steps: 41, Length: 0.5m\n",
      "Episode completed: #606, Steps: 34, Length: 0.5m\n",
      "Episode completed: #607, Steps: 66, Length: 0.5m\n",
      "Episode completed: #608, Steps: 38, Length: 0.5m\n",
      "Episode completed: #609, Steps: 54, Length: 0.5m\n",
      "Episode completed: #610, Steps: 44, Length: 0.5m\n",
      "Episode completed: #611, Steps: 38, Length: 0.5m\n",
      "Episode completed: #612, Steps: 42, Length: 0.5m\n",
      "Episode completed: #613, Steps: 36, Length: 0.5m\n",
      "Episode completed: #614, Steps: 44, Length: 0.5m\n",
      "Episode completed: #615, Steps: 32, Length: 0.5m\n",
      "Episode completed: #616, Steps: 38, Length: 0.5m\n",
      "Episode completed: #617, Steps: 36, Length: 0.5m\n",
      "Episode completed: #618, Steps: 59, Length: 0.5m\n",
      "Episode completed: #619, Steps: 32, Length: 0.5m\n",
      "Episode completed: #620, Steps: 31, Length: 0.5m\n",
      "Episode completed: #621, Steps: 51, Length: 0.5m\n",
      "Episode completed: #622, Steps: 49, Length: 0.5m\n",
      "Episode completed: #623, Steps: 33, Length: 0.5m\n",
      "Episode completed: #624, Steps: 39, Length: 0.5m\n",
      "Episode completed: #625, Steps: 41, Length: 0.5m\n",
      "Episode completed: #626, Steps: 42, Length: 0.5m\n",
      "Episode completed: #627, Steps: 31, Length: 0.5m\n",
      "Episode completed: #628, Steps: 51, Length: 0.5m\n",
      "Episode completed: #629, Steps: 35, Length: 0.5m\n",
      "Episode completed: #630, Steps: 46, Length: 0.5m\n",
      "Episode completed: #631, Steps: 65, Length: 0.5m\n",
      "Episode completed: #632, Steps: 77, Length: 0.5m\n",
      "Episode completed: #633, Steps: 38, Length: 0.5m\n",
      "Episode completed: #634, Steps: 140, Length: 0.5m\n",
      "Episode completed: #635, Steps: 45, Length: 0.5m\n",
      "Episode completed: #636, Steps: 73, Length: 0.5m\n",
      "Episode completed: #637, Steps: 71, Length: 0.5m\n",
      "Episode completed: #638, Steps: 43, Length: 0.5m\n",
      "Episode completed: #639, Steps: 114, Length: 0.5m\n",
      "Episode completed: #640, Steps: 45, Length: 0.5m\n",
      "Episode completed: #641, Steps: 134, Length: 0.5m\n",
      "Episode completed: #642, Steps: 41, Length: 0.5m\n",
      "Episode completed: #643, Steps: 39, Length: 0.5m\n",
      "Episode completed: #644, Steps: 99, Length: 0.5m\n",
      "Episode completed: #645, Steps: 37, Length: 0.5m\n",
      "Episode completed: #646, Steps: 35, Length: 0.5m\n",
      "Episode completed: #647, Steps: 52, Length: 0.5m\n",
      "Episode completed: #648, Steps: 57, Length: 0.5m\n",
      "Episode completed: #649, Steps: 37, Length: 0.5m\n",
      "\n",
      "ðŸš¨ ENVIRONMENT CHANGE ðŸš¨\n",
      "Changing pole length from 0.5m to 1.5m\n",
      "Environment parameters updated: masscart=1.0, length=1.5, gravity=9.8\n",
      "Next change scheduled at: 09:29:56\n",
      "ðŸ§  Updating reward function to adapt to new length...\n",
      "\n",
      "Reached maximum updates (3) for component 1. Skipping update.\n",
      "\n",
      "Reached maximum updates (3) for component 2. Skipping update.\n",
      "Episode completed: #650, Steps: 44, Length: 1.5m\n",
      "Episode completed: #651, Steps: 48, Length: 1.5m\n",
      "Episode completed: #652, Steps: 61, Length: 1.5m\n",
      "Episode completed: #653, Steps: 31, Length: 1.5m\n",
      "Episode completed: #654, Steps: 56, Length: 1.5m\n",
      "Episode completed: #655, Steps: 43, Length: 1.5m\n",
      "Episode completed: #656, Steps: 28, Length: 1.5m\n",
      "Episode completed: #657, Steps: 37, Length: 1.5m\n",
      "Episode completed: #658, Steps: 32, Length: 1.5m\n",
      "Episode completed: #659, Steps: 32, Length: 1.5m\n",
      "Episode completed: #660, Steps: 35, Length: 1.5m\n",
      "Episode completed: #661, Steps: 47, Length: 1.5m\n",
      "Episode completed: #662, Steps: 32, Length: 1.5m\n",
      "Episode completed: #663, Steps: 49, Length: 1.5m\n",
      "Episode completed: #664, Steps: 28, Length: 1.5m\n",
      "Episode completed: #665, Steps: 47, Length: 1.5m\n",
      "Episode completed: #666, Steps: 58, Length: 1.5m\n",
      "Episode completed: #667, Steps: 51, Length: 1.5m\n",
      "Episode completed: #668, Steps: 41, Length: 1.5m\n",
      "Episode completed: #669, Steps: 41, Length: 1.5m\n",
      "Episode completed: #670, Steps: 51, Length: 1.5m\n",
      "Episode completed: #671, Steps: 46, Length: 1.5m\n",
      "Episode completed: #672, Steps: 36, Length: 1.5m\n",
      "Episode completed: #673, Steps: 53, Length: 1.5m\n",
      "Episode completed: #674, Steps: 30, Length: 1.5m\n",
      "Episode completed: #675, Steps: 60, Length: 1.5m\n",
      "Episode completed: #676, Steps: 30, Length: 1.5m\n",
      "Episode completed: #677, Steps: 26, Length: 1.5m\n",
      "Episode completed: #678, Steps: 48, Length: 1.5m\n",
      "Episode completed: #679, Steps: 45, Length: 1.5m\n",
      "Episode completed: #680, Steps: 42, Length: 1.5m\n",
      "Episode completed: #681, Steps: 54, Length: 1.5m\n",
      "Episode completed: #682, Steps: 62, Length: 1.5m\n",
      "Episode completed: #683, Steps: 49, Length: 1.5m\n",
      "Episode completed: #684, Steps: 51, Length: 1.5m\n",
      "Episode completed: #685, Steps: 42, Length: 1.5m\n",
      "Episode completed: #686, Steps: 32, Length: 1.5m\n",
      "Episode completed: #687, Steps: 37, Length: 1.5m\n",
      "Episode completed: #688, Steps: 32, Length: 1.5m\n",
      "Episode completed: #689, Steps: 54, Length: 1.5m\n",
      "Episode completed: #690, Steps: 50, Length: 1.5m\n",
      "Episode completed: #691, Steps: 32, Length: 1.5m\n",
      "Episode completed: #692, Steps: 51, Length: 1.5m\n",
      "Episode completed: #693, Steps: 46, Length: 1.5m\n",
      "Episode completed: #694, Steps: 112, Length: 1.5m\n",
      "Episode completed: #695, Steps: 93, Length: 1.5m\n",
      "Episode completed: #696, Steps: 30, Length: 1.5m\n",
      "Episode completed: #697, Steps: 67, Length: 1.5m\n",
      "Episode completed: #698, Steps: 35, Length: 1.5m\n",
      "Episode completed: #699, Steps: 55, Length: 1.5m\n",
      "Episode completed: #700, Steps: 35, Length: 1.5m\n",
      "Episode completed: #701, Steps: 41, Length: 1.5m\n",
      "Episode completed: #702, Steps: 32, Length: 1.5m\n",
      "Episode completed: #703, Steps: 36, Length: 1.5m\n",
      "Episode completed: #704, Steps: 73, Length: 1.5m\n",
      "Episode completed: #705, Steps: 30, Length: 1.5m\n",
      "Episode completed: #706, Steps: 30, Length: 1.5m\n",
      "Episode completed: #707, Steps: 40, Length: 1.5m\n",
      "Episode completed: #708, Steps: 41, Length: 1.5m\n",
      "Episode completed: #709, Steps: 28, Length: 1.5m\n",
      "Episode completed: #710, Steps: 66, Length: 1.5m\n",
      "Episode completed: #711, Steps: 30, Length: 1.5m\n",
      "Episode completed: #712, Steps: 48, Length: 1.5m\n",
      "Episode completed: #713, Steps: 51, Length: 1.5m\n",
      "Episode completed: #714, Steps: 39, Length: 1.5m\n",
      "Episode completed: #715, Steps: 38, Length: 1.5m\n",
      "Episode completed: #716, Steps: 34, Length: 1.5m\n",
      "Episode completed: #717, Steps: 49, Length: 1.5m\n",
      "Episode completed: #718, Steps: 35, Length: 1.5m\n",
      "Episode completed: #719, Steps: 35, Length: 1.5m\n",
      "Episode completed: #720, Steps: 38, Length: 1.5m\n",
      "Episode completed: #721, Steps: 54, Length: 1.5m\n",
      "Episode completed: #722, Steps: 47, Length: 1.5m\n",
      "Episode completed: #723, Steps: 39, Length: 1.5m\n",
      "Episode completed: #724, Steps: 54, Length: 1.5m\n",
      "Episode completed: #725, Steps: 38, Length: 1.5m\n",
      "Episode completed: #726, Steps: 37, Length: 1.5m\n",
      "Episode completed: #727, Steps: 34, Length: 1.5m\n",
      "Episode completed: #728, Steps: 60, Length: 1.5m\n",
      "Episode completed: #729, Steps: 60, Length: 1.5m\n",
      "Episode completed: #730, Steps: 31, Length: 1.5m\n",
      "Episode completed: #731, Steps: 48, Length: 1.5m\n",
      "Episode completed: #732, Steps: 40, Length: 1.5m\n",
      "Episode completed: #733, Steps: 46, Length: 1.5m\n",
      "Episode completed: #734, Steps: 39, Length: 1.5m\n",
      "Episode completed: #735, Steps: 88, Length: 1.5m\n",
      "Episode completed: #736, Steps: 77, Length: 1.5m\n",
      "Episode completed: #737, Steps: 48, Length: 1.5m\n",
      "Episode completed: #738, Steps: 40, Length: 1.5m\n",
      "Episode completed: #739, Steps: 125, Length: 1.5m\n",
      "Episode completed: #740, Steps: 145, Length: 1.5m\n",
      "Episode completed: #741, Steps: 54, Length: 1.5m\n",
      "Episode completed: #742, Steps: 41, Length: 1.5m\n",
      "Episode completed: #743, Steps: 131, Length: 1.5m\n",
      "Episode completed: #744, Steps: 55, Length: 1.5m\n",
      "Episode completed: #745, Steps: 77, Length: 1.5m\n",
      "Episode completed: #746, Steps: 49, Length: 1.5m\n",
      "Episode completed: #747, Steps: 33, Length: 1.5m\n",
      "Episode completed: #748, Steps: 45, Length: 1.5m\n",
      "Episode completed: #749, Steps: 53, Length: 1.5m\n",
      "Episode completed: #750, Steps: 35, Length: 1.5m\n",
      "Episode completed: #751, Steps: 48, Length: 1.5m\n",
      "Episode completed: #752, Steps: 40, Length: 1.5m\n",
      "Episode completed: #753, Steps: 70, Length: 1.5m\n",
      "Episode completed: #754, Steps: 59, Length: 1.5m\n",
      "Episode completed: #755, Steps: 60, Length: 1.5m\n",
      "Episode completed: #756, Steps: 66, Length: 1.5m\n",
      "Episode completed: #757, Steps: 29, Length: 1.5m\n",
      "Episode completed: #758, Steps: 30, Length: 1.5m\n",
      "Episode completed: #759, Steps: 50, Length: 1.5m\n",
      "Episode completed: #760, Steps: 35, Length: 1.5m\n",
      "Episode completed: #761, Steps: 39, Length: 1.5m\n",
      "Episode completed: #762, Steps: 46, Length: 1.5m\n",
      "Episode completed: #763, Steps: 30, Length: 1.5m\n",
      "Episode completed: #764, Steps: 60, Length: 1.5m\n",
      "Episode completed: #765, Steps: 59, Length: 1.5m\n",
      "Episode completed: #766, Steps: 28, Length: 1.5m\n",
      "Episode completed: #767, Steps: 69, Length: 1.5m\n",
      "Episode completed: #768, Steps: 55, Length: 1.5m\n",
      "Episode completed: #769, Steps: 58, Length: 1.5m\n",
      "\n",
      "ðŸš¨ ENVIRONMENT CHANGE ðŸš¨\n",
      "Changing pole length from 1.5m to 0.5m\n",
      "Environment parameters updated: masscart=1.0, length=0.5, gravity=9.8\n",
      "Next change scheduled at: 09:31:56\n",
      "ðŸ§  Updating reward function to adapt to new length...\n",
      "\n",
      "Reached maximum updates (3) for component 1. Skipping update.\n",
      "\n",
      "Reached maximum updates (3) for component 2. Skipping update.\n",
      "Episode completed: #770, Steps: 37, Length: 0.5m\n",
      "Episode completed: #771, Steps: 52, Length: 0.5m\n",
      "Episode completed: #772, Steps: 54, Length: 0.5m\n",
      "Episode completed: #773, Steps: 42, Length: 0.5m\n",
      "Episode completed: #774, Steps: 36, Length: 0.5m\n",
      "Episode completed: #775, Steps: 59, Length: 0.5m\n",
      "Episode completed: #776, Steps: 84, Length: 0.5m\n",
      "Episode completed: #777, Steps: 38, Length: 0.5m\n",
      "Episode completed: #778, Steps: 43, Length: 0.5m\n",
      "Episode completed: #779, Steps: 58, Length: 0.5m\n",
      "Episode completed: #780, Steps: 43, Length: 0.5m\n",
      "Episode completed: #781, Steps: 47, Length: 0.5m\n",
      "Episode completed: #782, Steps: 77, Length: 0.5m\n",
      "Episode completed: #783, Steps: 38, Length: 0.5m\n",
      "Episode completed: #784, Steps: 43, Length: 0.5m\n",
      "Episode completed: #785, Steps: 45, Length: 0.5m\n",
      "Episode completed: #786, Steps: 33, Length: 0.5m\n",
      "Episode completed: #787, Steps: 77, Length: 0.5m\n",
      "Episode completed: #788, Steps: 43, Length: 0.5m\n",
      "Episode completed: #789, Steps: 32, Length: 0.5m\n",
      "Episode completed: #790, Steps: 49, Length: 0.5m\n",
      "Episode completed: #791, Steps: 122, Length: 0.5m\n",
      "Episode completed: #792, Steps: 41, Length: 0.5m\n",
      "Episode completed: #793, Steps: 33, Length: 0.5m\n",
      "Episode completed: #794, Steps: 51, Length: 0.5m\n",
      "Episode completed: #795, Steps: 36, Length: 0.5m\n",
      "Episode completed: #796, Steps: 40, Length: 0.5m\n",
      "Episode completed: #797, Steps: 30, Length: 0.5m\n",
      "Episode completed: #798, Steps: 36, Length: 0.5m\n",
      "Episode completed: #799, Steps: 33, Length: 0.5m\n",
      "Episode completed: #800, Steps: 52, Length: 0.5m\n",
      "Episode completed: #801, Steps: 47, Length: 0.5m\n",
      "Episode completed: #802, Steps: 38, Length: 0.5m\n",
      "Episode completed: #803, Steps: 30, Length: 0.5m\n",
      "Episode completed: #804, Steps: 31, Length: 0.5m\n",
      "Episode completed: #805, Steps: 92, Length: 0.5m\n",
      "Episode completed: #806, Steps: 54, Length: 0.5m\n",
      "Episode completed: #807, Steps: 70, Length: 0.5m\n",
      "Episode completed: #808, Steps: 38, Length: 0.5m\n",
      "Episode completed: #809, Steps: 67, Length: 0.5m\n",
      "Episode completed: #810, Steps: 31, Length: 0.5m\n",
      "Episode completed: #811, Steps: 78, Length: 0.5m\n",
      "Episode completed: #812, Steps: 37, Length: 0.5m\n",
      "Episode completed: #813, Steps: 38, Length: 0.5m\n",
      "Episode completed: #814, Steps: 43, Length: 0.5m\n",
      "Episode completed: #815, Steps: 34, Length: 0.5m\n",
      "Episode completed: #816, Steps: 38, Length: 0.5m\n",
      "Episode completed: #817, Steps: 29, Length: 0.5m\n",
      "\n",
      "Demonstration stopped by user\n",
      "Environment closed. Demo ended.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import torch  # Make sure this is imported\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "current_dir = os.getcwd()  \n",
    "project_root = str(Path(current_dir).parent.parent)\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# Import your custom components\n",
    "from RLEnvironment.env import CustomCartPoleEnv\n",
    "from AdaptiveRewardFunctionLearning.RewardGeneration.rewardCritic import RewardUpdateSystem\n",
    "from RLEnvironment.training.agent import DQLearningAgent\n",
    "from AdaptiveRewardFunctionLearning.RewardGeneration.rewardCodeGeneration import stabilityReward, efficiencyReward\n",
    "from AdaptiveRewardFunctionLearning.Prompts.prompts import apiKey, modelName\n",
    "\n",
    "def run_continuous_cartpole_demo(use_pretrained=True, save_model=True):\n",
    "    \"\"\"\n",
    "    Run a continuous CartPole demonstration where the pole length \n",
    "    randomly changes every 2 minutes of real-life time.\n",
    "    \n",
    "    Args:\n",
    "        use_pretrained: Whether to load a pre-trained model if available\n",
    "        save_model: Whether to save the model after pre-training\n",
    "    \"\"\"\n",
    "    print(\"Setting up CartPole for Open Day demonstration...\")\n",
    "    \n",
    "    # Initialize environment with human rendering\n",
    "    env = gym.make('CartPole-v1', render_mode='human')\n",
    "    env = CustomCartPoleEnv(env)\n",
    "    \n",
    "    # Set initial pole length\n",
    "    possible_lengths = [0.5, 3.0, 1.5, 0.5, 2.5]\n",
    "    current_length = random.choice(possible_lengths)\n",
    "    env.setEnvironmentParameters(length=current_length)\n",
    "    print(f\"Starting with pole length: {current_length}m\")\n",
    "    \n",
    "    # Set up adaptive reward function\n",
    "    env.setComponentReward(1, stabilityReward)  # Stability component\n",
    "    env.setComponentReward(2, efficiencyReward)  # Efficiency component\n",
    "    \n",
    "    # Initialize update system\n",
    "    update_system = RewardUpdateSystem(apiKey, modelName)\n",
    "    \n",
    "    # Create agent\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    agent = DQLearningAgent(env, state_size, action_size, \"cpu\", epsilon=0.01)\n",
    "    \n",
    "    # Define model save path\n",
    "    model_path = os.path.join(current_dir, \"cartpole_pretrained_model.pt\")\n",
    "    \n",
    "    # Check if we should use a pre-trained model\n",
    "    if use_pretrained and os.path.exists(model_path):\n",
    "        print(f\"Loading pre-trained model from {model_path}\")\n",
    "        try:\n",
    "            # Load the model state dict\n",
    "            agent.model.load_state_dict(torch.load(model_path))\n",
    "            # Also copy to target model\n",
    "            agent.targetModel.load_state_dict(agent.model.state_dict())\n",
    "            print(\"Pre-trained model loaded successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            print(\"Will pre-train a new model instead.\")\n",
    "            use_pretrained = False\n",
    "    else:\n",
    "        use_pretrained = False\n",
    "        \n",
    "    # Pre-train the agent only if not using pre-trained model\n",
    "    if not use_pretrained:\n",
    "        print(\"Pre-training agent...\")\n",
    "        for episode in range(5000):  # Increased episodes for better performance\n",
    "            state = env.reset()[0]\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = agent.chooseAction(state)\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                agent.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "            agent.replay(64)  # Larger batch for better learning\n",
    "            \n",
    "            # Print progress occasionally\n",
    "            if (episode + 1) % 50 == 0:\n",
    "                print(f\"Pre-training episode {episode + 1}/500, Reward: {episode_reward}\")\n",
    "                \n",
    "        # Save the trained model if requested\n",
    "        if save_model:\n",
    "            print(f\"Saving pre-trained model to {model_path}\")\n",
    "            torch.save(agent.model.state_dict(), model_path)\n",
    "            print(\"Model saved for future demonstrations!\")\n",
    "    \n",
    "    # Set up timing for pole length changes\n",
    "    change_interval_seconds = 120  # 2 minutes\n",
    "    last_change_time = datetime.now()\n",
    "    next_change_time = last_change_time + timedelta(seconds=change_interval_seconds)\n",
    "    \n",
    "    print(f\"\\nðŸŽ® DEMO STARTED! Pole length will change every 2 minutes\")\n",
    "    print(f\"Next change scheduled at: {next_change_time.strftime('%H:%M:%S')}\")\n",
    "    print(\"Press Ctrl+C to stop the demonstration\")\n",
    "    \n",
    "    # Reset environment to start demonstration\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    episode_steps = 0\n",
    "    episodes_completed = 0\n",
    "    \n",
    "    try:\n",
    "        # Main demonstration loop - runs until interrupted\n",
    "        while True:\n",
    "            # Rest of your code remains the same...\n",
    "            # Check if it's time to change the pole length\n",
    "            current_time = datetime.now()\n",
    "            \n",
    "            if current_time >= next_change_time:\n",
    "                # Choose a new length that's different from the current one\n",
    "                new_lengths = [l for l in possible_lengths if l != current_length]\n",
    "                new_length = random.choice(new_lengths)\n",
    "                \n",
    "                print(f\"\\nðŸš¨ ENVIRONMENT CHANGE ðŸš¨\")\n",
    "                print(f\"Changing pole length from {current_length}m to {new_length}m\")\n",
    "                env.setEnvironmentParameters(length=new_length)\n",
    "                current_length = new_length\n",
    "                \n",
    "                # Update timing\n",
    "                last_change_time = current_time\n",
    "                next_change_time = current_time + timedelta(seconds=change_interval_seconds)\n",
    "                print(f\"Next change scheduled at: {next_change_time.strftime('%H:%M:%S')}\")\n",
    "                \n",
    "                # Trigger reward function update after length change\n",
    "                print(\"ðŸ§  Updating reward function to adapt to new length...\")\n",
    "                for component in [1, 2]:\n",
    "                    update_system.targetComponent = component\n",
    "                    current_func = env.rewardComponents[f'rewardFunction{component}']\n",
    "                    new_function, updated = update_system.validateAndUpdate(current_func)\n",
    "                    if updated:\n",
    "                        env.setComponentReward(component, new_function)\n",
    "                \n",
    "                # Reset the environment for a clean start with new length\n",
    "                state = env.reset()[0]\n",
    "                done = False\n",
    "                episode_steps = 0\n",
    "            \n",
    "            # Choose action and step environment\n",
    "            action = agent.chooseAction(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            \n",
    "            # Track progress\n",
    "            episode_steps += 1\n",
    "            \n",
    "            # Train agent with experience\n",
    "            done = terminated or truncated\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Update state\n",
    "            state = next_state\n",
    "            \n",
    "            # Reset if done\n",
    "            if done:\n",
    "                episodes_completed += 1\n",
    "                print(f\"Episode completed: #{episodes_completed}, Steps: {episode_steps}, Length: {current_length}m\")\n",
    "                state = env.reset()[0]\n",
    "                done = False\n",
    "                episode_steps = 0\n",
    "                agent.replay(32)\n",
    "                \n",
    "            # Small sleep to prevent CPU overuse\n",
    "            time.sleep(0.001)\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nDemonstration stopped by user\")\n",
    "    finally:\n",
    "        env.close()\n",
    "        print(\"Environment closed. Demo ended.\")\n",
    "\n",
    "# Run the continuous demonstration\n",
    "if __name__ == \"__main__\":\n",
    "    # Set to True to use a pre-trained model if available\n",
    "    run_continuous_cartpole_demo(use_pretrained=True, save_model=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
